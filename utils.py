# -*- coding: utf-8 -*-
"""
å­¦æœ¯ç»¼è¿°ç”Ÿæˆç³»ç»Ÿå·¥å…·å‡½æ•°åº“

è„šæœ¬ç›®æ ‡: æä¾›å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå…±ç”¨çš„å·¥å…·å‡½æ•°
ä¸Šä¸‹æ–‡: ä»multi_agent.pyä¸­æå–çš„å¯å¤ç”¨å·¥å…·å‡½æ•°
è¾“å…¥: å„ç§æ•°æ®ç»“æ„å’Œå‚æ•°
æ‰§è¡Œæ­¥éª¤: æŒ‰éœ€è°ƒç”¨ä¸åŒçš„å·¥å…·å‡½æ•°
è¾“å‡º: å¤„ç†åçš„æ•°æ®ç»“æœ
"""

import re
import json
import logging
import os
from datetime import datetime
from typing import Dict, List, Optional
from llm_review_generator import EnhancedSimilarityCalculator
import hashlib
from dataclasses import dataclass
from typing import Dict, List, Optional

async def search_relevant_content(
    db, 
    similarity_calculator: EnhancedSimilarityCalculator,
    topic: str, 
    subtopics: List[str] = None,
    purpose: str = "æ£€ç´¢",
    llm_call_func = None  # æ–°å¢ï¼šLLMè°ƒç”¨å‡½æ•°ç”¨äºç¿»è¯‘
    ) -> Dict:
    """
    æœç´¢ç›¸å…³å†…å®¹å¹¶è®¡ç®—å¢å¼ºç›¸ä¼¼åº¦
    
    Args:
        db: æ•°æ®åº“å¯¹è±¡ (AcademicPaperDatabase)
        similarity_calculator: å¢å¼ºç›¸ä¼¼åº¦è®¡ç®—å™¨
        topic: ä¸»é¢˜
        subtopics: å­ä¸»é¢˜åˆ—è¡¨
        purpose: æœç´¢ç›®çš„ï¼Œç”¨äºæ—¥å¿—æ˜¾ç¤º
        llm_call_func: LLMè°ƒç”¨å‡½æ•°ï¼Œç”¨äºç¿»è¯‘å…³é”®è¯
        
    Returns:
        åŒ…å«åˆ†ç±»å†…å®¹å’Œç»Ÿè®¡ä¿¡æ¯çš„ä¸Šä¸‹æ–‡å­—å…¸
    """
    print(f"ğŸ” æ­£åœ¨æ£€ç´¢å…³äº'{topic}'çš„ç ”ç©¶èµ„æ–™ç”¨äº{purpose}...")
    
    search_queries = [topic]
    if subtopics:
        search_queries.extend(subtopics)
    
    # ğŸ†• ç¿»è¯‘æœç´¢æŸ¥è¯¢ä¸ºè‹±æ–‡
    if llm_call_func:
        print("ğŸ”¤ æ­£åœ¨ç¿»è¯‘æœç´¢æŸ¥è¯¢ä¸ºè‹±æ–‡...")
        english_queries = await translate_keywords_batch(search_queries, llm_call_func)
        # åŒæ—¶ä¿ç•™åŸå§‹æŸ¥è¯¢ä½œä¸ºå¤‡ç”¨
        final_queries = english_queries 
        print(f"ğŸ“ è‹±æ–‡æŸ¥è¯¢: {final_queries}")
    else:
        print("âš ï¸ æœªæä¾›LLMå‡½æ•°ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æŸ¥è¯¢ï¼ˆå¯èƒ½å½±å“æœç´¢æ•ˆæœï¼‰")
        final_queries = search_queries
    
    # ğŸ”§ä¿®å¤ï¼šenglish_topicåº”è¯¥æ˜¯å­—ç¬¦ä¸²ï¼Œä¸æ˜¯åˆ—è¡¨
    if llm_call_func:
        english_topic_list = await translate_keywords_batch([topic], llm_call_func)
        if english_topic_list:
            english_topic = english_topic_list[0]  # å–ç¿»è¯‘åçš„ä¸»é¢˜å­—ç¬¦ä¸²
        else:
            english_topic = topic  # ç¿»è¯‘å¤±è´¥æ—¶ä½¿ç”¨åŸå§‹ä¸»é¢˜
        print(f"ğŸ¯ ç›¸ä¼¼åº¦è®¡ç®—ä½¿ç”¨è‹±æ–‡ä¸»é¢˜: {english_topic}")
    else:
        # å¦‚æœæ²¡æœ‰LLMå‡½æ•°ï¼Œä½¿ç”¨åŸå§‹ä¸»é¢˜
        english_topic = topic
        print(f"ğŸ¯ ç›¸ä¼¼åº¦è®¡ç®—ä½¿ç”¨åŸå§‹ä¸»é¢˜: {english_topic}")
    
    all_results = []
    for query in final_queries:
        # æ£€ç´¢æ–‡æœ¬å†…å®¹
        text_results = db.search_content(
            query, content_type="texts", n_results=500
        )
        all_results.extend(text_results)
        
    # ä½¿ç”¨å¢å¼ºç›¸ä¼¼åº¦é‡æ–°æ’åº
    enhanced_results = []
    seen_ids = set()
    
    # æ·»åŠ è¿‡æ»¤ç»Ÿè®¡
    total_raw_results = len(all_results)
    duplicate_count = 0
    content_type_filtered_count = 0  # æ–°å¢ï¼šå†…å®¹ç±»å‹è¿‡æ»¤ç»Ÿè®¡
    short_content_count = 0
    low_similarity_count = 0
    
    for result in all_results:
        if result['id'] in seen_ids:
            duplicate_count += 1
            continue
        seen_ids.add(result['id'])
        
        # ğŸ†• æ™ºèƒ½å†…å®¹ç±»å‹è¿‡æ»¤ï¼šæ ¹æ®æœç´¢æ¥æºå†³å®šè¿‡æ»¤ç­–ç•¥
        content_type = result.get('metadata', {}).get('content_type', '')
        collection_name = result.get('collection', '')
        
        # åªè¿‡æ»¤æ¥è‡ªtexts collectionçš„å›¾ç‰‡å’Œè¡¨æ ¼æè¿°æ–‡æœ¬
        # æ¥è‡ªimages/tables collectionçš„å†…å®¹ä¿ç•™ï¼ˆè¿™äº›æ˜¯æˆ‘ä»¬ä¸“é—¨æœç´¢çš„å›¾ç‰‡/è¡¨æ ¼ï¼‰
        if content_type in ['image_text', 'table_text'] and collection_name == 'texts':
            content_type_filtered_count += 1
            continue  # åªè·³è¿‡å­˜å‚¨åœ¨texts collectionä¸­çš„å›¾ç‰‡å’Œè¡¨æ ¼æè¿°æ–‡æœ¬
        
        # ğŸ†• é•¿åº¦è¿‡æ»¤ï¼šåˆ é™¤å°‘äº200å­—ç¬¦çš„çŸ­å†…å®¹
        content = result.get("document", "")
        if len(content) < 200:
            short_content_count += 1
            continue
            
        # ğŸ†• æ¸…ç†å­¦æœ¯å¼•ç”¨ï¼Œæé«˜å†…å®¹è´¨é‡
        content = clean_academic_citations(content)
        result["document"] = content  # æ›´æ–°æ¸…ç†åçš„å†…å®¹
        
        # ğŸ†• ä½¿ç”¨è‹±æ–‡ä¸»é¢˜è®¡ç®—ç›¸ä¼¼åº¦
        enhanced_score = similarity_calculator.calculate_enhanced_similarity(result, english_topic)
        if enhanced_score >= 0.05:  # ç›¸å…³æ€§é˜ˆå€¼
            result['enhanced_similarity'] = enhanced_score
            enhanced_results.append(result)
        else:
            low_similarity_count += 1
    
    # æŒ‰å¢å¼ºç›¸ä¼¼åº¦æ’åº
    enhanced_results.sort(key=lambda x: x['enhanced_similarity'], reverse=True)
    
    # ğŸ†• è¯¦ç»†çš„è¿‡æ»¤ç»Ÿè®¡
    print(f"ğŸ“Š å‡½æ•°search_relevant_contentè¿‡æ»¤ç»Ÿè®¡:")
    print(f"  åŸå§‹ç»“æœ: {total_raw_results} æ¡ï¼Œå»é‡è¿‡æ»¤: -{duplicate_count} æ¡ï¼Œå†…å®¹ç±»å‹è¿‡æ»¤: -{content_type_filtered_count} æ¡ (æ¥è‡ªtextsçš„å›¾ç‰‡/è¡¨æ ¼æè¿°)ï¼Œé•¿åº¦è¿‡æ»¤: -{short_content_count} æ¡ï¼Œç›¸ä¼¼åº¦è¿‡æ»¤: -{low_similarity_count} æ¡")
    print(f"  æœ€ç»ˆç»“æœ: {len(enhanced_results)} æ¡")
    
    # æŒ‰å†…å®¹ç±»å‹åˆ†ç»„
    context = {
        "main_topic": topic,
        "subtopics": subtopics or [],
        "relevant_content": {
            "texts": [],
        },
        "source_papers": {},
        "statistics": {}
    }
    
    # åˆ†ç±»å­˜å‚¨å†…å®¹  
    for result in enhanced_results:  # ğŸ†• ç§»é™¤æ•°é‡é™åˆ¶ï¼Œè¿”å›æ‰€æœ‰ç›¸å…³ç»“æœ
        content_type = result['metadata']['content_type']
        paper_name = result['metadata']['paper_name']
        
        if paper_name not in context["source_papers"]:
            context["source_papers"][paper_name] = {
                "content_count": 0,
                "sections": []
            }
        
        context["source_papers"][paper_name]["content_count"] += 1
        
        content_item = {
            "content": result["document"],
            "paper": paper_name,
            "page": result["metadata"]["page_idx"],
            "relevance_score": result['enhanced_similarity'],
            "metadata": result["metadata"]
        }
        
        context["relevant_content"]["texts"].append(content_item)

    # ç»Ÿè®¡ä¿¡æ¯
    context["statistics"] = {
        "total_papers": len(context["source_papers"]),
        "total_texts": len(context["relevant_content"]["texts"])
    }
    
    print(f"ğŸ“Š å‡½æ•°search_relevant_contentæ£€ç´¢å®Œæˆ: {context['statistics']['total_papers']} ç¯‡è®ºæ–‡, "
          f"{context['statistics']['total_texts']} æ¡æ–‡æœ¬"
          )
    
    return context

async def search_section_specific_materials(
    section_info: Dict, 
    db, 
    similarity_calculator_class, 
    llm_call_func,
    chapter_title_english: str,  # ğŸ†• æ–°å¢ï¼šç¿»è¯‘åçš„ç« èŠ‚æ ‡é¢˜ï¼Œç”¨ä½œç›¸ä¼¼åº¦è®¡ç®—çš„ä¸»è¦ä¾æ®
    existing_materials: List[Dict] = None,  # æ–°å¢ï¼šå·²æœ‰çš„é€šç”¨ææ–™
    logger = None,  # æ–°å¢ï¼šæ—¥å¿—è®°å½•å™¨
    max_texts: int = 50,
    max_equations: int = 20,
    max_figures: int = 20,
    max_tables: int = 20
    ) -> Dict:
    """
    æ ¹æ®ç« èŠ‚å…³é”®è¯æœç´¢ç‰¹å®šçš„ç›¸å…³ææ–™ï¼Œä½¿ç”¨ç« èŠ‚æ ‡é¢˜ä½œä¸ºç›¸ä¼¼åº¦è®¡ç®—çš„ä¸»è¦ä¾æ®
    
    è„šæœ¬ç›®æ ‡: ä¸ºç‰¹å®šç« èŠ‚æœç´¢ç›¸å…³çš„ç ”ç©¶ææ–™
    ä¸Šä¸‹æ–‡: ä»multi_agent.pyä¸­æå–çš„ç« èŠ‚ç‰¹å®šææ–™æœç´¢é€»è¾‘
    è¾“å…¥: 
    - section_info: ç« èŠ‚ä¿¡æ¯
    - db: æ•°æ®åº“å¯¹è±¡
    - similarity_calculator_class: ç›¸ä¼¼åº¦è®¡ç®—å™¨ç±»
    - llm_call_func: LLMè°ƒç”¨å‡½æ•°
    - chapter_title_english: ç¿»è¯‘åçš„ç« èŠ‚æ ‡é¢˜ï¼Œç”¨ä½œç›¸ä¼¼åº¦è®¡ç®—çš„ä¸»è¦ä¾æ®
    - existing_materials: å·²æœ‰çš„é€šç”¨ææ–™ï¼ˆç”¨äºå»é‡ï¼‰
    - logger: æ—¥å¿—è®°å½•å™¨ï¼ˆå¯é€‰ï¼‰
    - max_texts: æ–‡æœ¬ææ–™æœ€å¤§æ•°é‡
    - max_equations: å…¬å¼ææ–™æœ€å¤§æ•°é‡
    - max_figures: å›¾è¡¨ææ–™æœ€å¤§æ•°é‡
    - max_tables: è¡¨æ ¼ææ–™æœ€å¤§æ•°é‡
    æ‰§è¡Œæ­¥éª¤:
    1. æå–ç« èŠ‚å…³é”®è¯å’Œæ ‡é¢˜
    2. æ„å»ºæœç´¢æŸ¥è¯¢å¹¶ç¿»è¯‘ä¸ºè‹±æ–‡
    3. ä½¿ç”¨æ•°æ®åº“æœç´¢å„ç±»å‹å†…å®¹
    4. ğŸ”§æ”¹è¿›ï¼šä½¿ç”¨ç« èŠ‚æ ‡é¢˜è®¡ç®—å¢å¼ºç›¸ä¼¼åº¦å¹¶å»é‡
    5. ä¸å·²æœ‰é€šç”¨ææ–™å»é‡ï¼ˆä»…å¯¹æ–‡æœ¬ç±»å‹ï¼‰
    6. æŒ‰å†…å®¹ç±»å‹åˆ†ç»„è¿”å›
    è¾“å‡º: æŒ‰ç±»å‹åˆ†ç±»çš„ææ–™å­—å…¸ï¼ˆå·²å»é™¤ä¸é€šç”¨ææ–™é‡å¤çš„å†…å®¹ï¼‰
    
    Args:
        section_info: åŒ…å«ç« èŠ‚ä¿¡æ¯çš„å­—å…¸
        db: æ•°æ®åº“å¯¹è±¡ï¼Œéœ€è¦æœ‰search_contentæ–¹æ³•
        similarity_calculator_class: ç›¸ä¼¼åº¦è®¡ç®—å™¨ç±»(EnhancedSimilarityCalculator)
        llm_call_func: LLMè°ƒç”¨å‡½æ•°ï¼Œç”¨äºç¿»è¯‘å…³é”®è¯
        chapter_title_english (str): ç¿»è¯‘åçš„ç« èŠ‚æ ‡é¢˜ï¼Œç”¨ä½œç›¸ä¼¼åº¦è®¡ç®—çš„ä¸»è¦ä¾æ®
        existing_materials (List[Dict], optional): å·²æœ‰çš„é€šç”¨ææ–™åˆ—è¡¨ï¼Œç”¨äºå»é‡. Defaults to None.
        logger (optional): æ—¥å¿—è®°å½•å™¨ï¼Œç”¨äºè®°å½•è¿‡ç¨‹ä¿¡æ¯. Defaults to None.
        max_texts (int, optional): æ–‡æœ¬ææ–™æœ€å¤§æ•°é‡. Defaults to 50.
        max_equations (int, optional): å…¬å¼ææ–™æœ€å¤§æ•°é‡. Defaults to 20.
        max_figures (int, optional): å›¾è¡¨ææ–™æœ€å¤§æ•°é‡. Defaults to 20.
        max_tables (int, optional): è¡¨æ ¼ææ–™æœ€å¤§æ•°é‡. Defaults to 20.
        
    Returns:
        æŒ‰å†…å®¹ç±»å‹åˆ†ç±»çš„ææ–™å­—å…¸ï¼ˆå·²å»é™¤ä¸é€šç”¨ææ–™é‡å¤çš„å†…å®¹ï¼‰
    """
    # è®¾ç½®æ—¥å¿—è®°å½•å™¨
    if logger is None:
        # å¦‚æœæ²¡æœ‰ä¼ å…¥loggerï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„æ§åˆ¶å°æ—¥å¿—å™¨
        import logging
        logger = logging.getLogger("section_materials_search")
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
    
    logger.info(f"ğŸ” ä¸ºç« èŠ‚ '{section_info.get('title', 'æœªå‘½åç« èŠ‚')}' æœç´¢ç‰¹å®šææ–™...")
    
    
    # è·å–ç« èŠ‚å…³é”®è¯
    keywords = section_info.get("keywords", [])
    
    # æ„å»ºæœç´¢æŸ¥è¯¢
    search_queries = []
    
    # æ·»åŠ å…³é”®è¯ä½œä¸ºä¸»è¦æœç´¢è¯
    if keywords:
        search_queries.extend(keywords)
    
    # ä½¿ç”¨æ‰¹é‡ç¿»è¯‘åŠŸèƒ½ï¼Œæé«˜æ•ˆç‡
    search_queries = await translate_keywords_batch(search_queries, llm_call_func)
    logger.info(f"ğŸ” ç« èŠ‚ç‰¹å®šæœç´¢æŸ¥è¯¢å…³é”®è¯: {search_queries}")
    
    # å­˜å‚¨æ‰€æœ‰æœç´¢ç»“æœ
    all_results = []
    seen_ids = set()
    
    # å¯¹æ¯ä¸ªæŸ¥è¯¢è¯è¿›è¡Œæœç´¢
    for query in search_queries:
        if not query.strip():
            continue
            
        # æœç´¢æ–‡æœ¬å†…å®¹
        text_results = db.search_content(
            query, content_type="texts", n_results=max_texts*10
        )
        all_results.extend(text_results)
        
        # æœç´¢å…¬å¼
        equation_results = db.search_content(
            query, content_type="equations", n_results=max_equations*10
        )
        all_results.extend(equation_results)
        
        # æœç´¢å›¾è¡¨
        figure_results = db.search_content(
            query, content_type="images", n_results=max_figures*10
        )
        all_results.extend(figure_results)
        
        # æœç´¢è¡¨æ ¼
        table_results = db.search_content(
            query, content_type="tables", n_results=max_tables*10
        )
        all_results.extend(table_results)
    

    # ğŸ“Š å¼€å§‹è¯¦ç»†çš„ç­›é€‰ç»Ÿè®¡
    total_raw_results = len(all_results)
    logger.info(f"ğŸ” åŸå§‹æœç´¢ç»“æœæ€»æ•°: {total_raw_results} æ¡")
    
    # å»é‡å¹¶è®¡ç®—å¢å¼ºç›¸ä¼¼åº¦
    enhanced_results = []
    duplicate_count = 0
    content_type_filtered_count = 0  # æ–°å¢ï¼šå†…å®¹ç±»å‹è¿‡æ»¤ç»Ÿè®¡
    short_content_count = 0
    low_similarity_count = 0
    
    for result in all_results:
        if result['id'] in seen_ids:
            duplicate_count += 1
            continue
        seen_ids.add(result['id'])
        
        # ğŸ†• æ™ºèƒ½å†…å®¹ç±»å‹è¿‡æ»¤ï¼šæ ¹æ®æœç´¢æ¥æºå†³å®šè¿‡æ»¤ç­–ç•¥
        content_type = result.get('metadata', {}).get('content_type', '')
        collection_name = result.get('collection', '')
        
        # åªè¿‡æ»¤æ¥è‡ªtexts collectionçš„å›¾ç‰‡å’Œè¡¨æ ¼æè¿°æ–‡æœ¬
        # æ¥è‡ªimages/tables collectionçš„å†…å®¹ä¿ç•™ï¼ˆè¿™äº›æ˜¯æˆ‘ä»¬ä¸“é—¨æœç´¢çš„å›¾ç‰‡/è¡¨æ ¼ï¼‰
        if content_type in ['image_text', 'table_text'] and collection_name == 'texts':
            content_type_filtered_count += 1
            continue  # åªè·³è¿‡å­˜å‚¨åœ¨texts collectionä¸­çš„å›¾ç‰‡å’Œè¡¨æ ¼æè¿°æ–‡æœ¬
        
        # ğŸ†• é•¿åº¦è¿‡æ»¤ï¼šåˆ é™¤å°‘äº200å­—ç¬¦çš„çŸ­å†…å®¹
        content = result.get("document", "")
        if len(content) < 200:
            short_content_count += 1
            continue
            
        # ğŸ†• æ¸…ç†å­¦æœ¯å¼•ç”¨ï¼Œæé«˜å†…å®¹è´¨é‡
        content = clean_academic_citations(content)
        result["document"] = content  # æ›´æ–°æ¸…ç†åçš„å†…å®¹
        
        # ğŸ”§æ”¹è¿›ï¼šä½¿ç”¨ç« èŠ‚æ ‡é¢˜ä½œä¸ºç›¸ä¼¼åº¦è®¡ç®—çš„ä¸»è¦ä¾æ®
        if similarity_calculator_class and chapter_title_english:
            # ä½¿ç”¨çœŸæ­£çš„ç« èŠ‚æ ‡é¢˜åˆ›å»ºç›¸ä¼¼åº¦è®¡ç®—å™¨
            english_keywords = search_queries[:6] if search_queries else []  # é™åˆ¶å…³é”®è¯æ•°é‡
            temp_calculator = similarity_calculator_class(chapter_title_english, english_keywords)
            enhanced_score = temp_calculator.calculate_enhanced_similarity(result, chapter_title_english)
            
            # ğŸ†• æ·»åŠ å…³é”®è¯åŒ¹é…åŠ åˆ†ï¼Œæé«˜ææ–™å¤šæ ·æ€§
            keyword_bonus = 0
            content_lower = content.lower()
            for keyword in search_queries:
                if keyword and keyword.lower() in content_lower:
                    keyword_bonus += 0.005  # è¾ƒå°çš„åŠ åˆ†ï¼Œä¿æŒç« èŠ‚æ ‡é¢˜ä¸ºä¸»å¯¼
            enhanced_score += keyword_bonus
        else:
            # å›é€€æ–¹æ¡ˆï¼šç®€å•å­—ç¬¦ä¸²åŒ¹é…
            enhanced_score = 0.01  # é»˜è®¤åˆ†æ•°
            content_lower = content.lower()
            # ä¼˜å…ˆåŒ¹é…ç« èŠ‚æ ‡é¢˜
            if chapter_title_english and chapter_title_english.lower() in content_lower:
                enhanced_score += 0.02
            # ç„¶ååŒ¹é…å…³é”®è¯
            for english_query in search_queries:
                if english_query and english_query.lower() in content_lower:
                    enhanced_score += 0.005
        
        if enhanced_score >= 0.05:  # ç« èŠ‚ç‰¹å®šçš„ç›¸å…³æ€§é˜ˆå€¼
            result['enhanced_similarity'] = enhanced_score
            enhanced_results.append(result)
        else:
            low_similarity_count += 1
    
    # ğŸ“Š è¾“å‡ºè¯¦ç»†çš„ç­›é€‰ç»Ÿè®¡
    logger.info(f"ğŸ“Š å‡½æ•°search_section_specific_materialsç« èŠ‚ç‰¹å®šææ–™ç­›é€‰ç»Ÿè®¡:")
    logger.info(f"  åŸå§‹ç»“æœ: {total_raw_results} æ¡ï¼Œå»é‡è¿‡æ»¤: -{duplicate_count} æ¡ (ä¿ç•™: {total_raw_results - duplicate_count} æ¡)ï¼Œå†…å®¹ç±»å‹è¿‡æ»¤: -{content_type_filtered_count} æ¡ï¼Œé•¿åº¦è¿‡æ»¤: -{short_content_count} æ¡ (ä¿ç•™: {total_raw_results - duplicate_count - content_type_filtered_count - short_content_count} æ¡)ï¼Œç›¸ä¼¼åº¦è¿‡æ»¤: -{low_similarity_count} æ¡ (é˜ˆå€¼ >= 0.05)ï¼Œæœ€ç»ˆé€šè¿‡: {len(enhanced_results)} æ¡ï¼Œæ€»è¿‡æ»¤ç‡: {((total_raw_results - len(enhanced_results)) / total_raw_results * 100):.1f}%" if total_raw_results > 0 else "  æ€»è¿‡æ»¤ç‡: 0%")
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    enhanced_results.sort(key=lambda x: x['enhanced_similarity'], reverse=True)
    
    # æŒ‰å†…å®¹ç±»å‹åˆ†ç»„
    section_materials = {
        "texts": [],
        "equations": [],
        "figures": [],
        "tables": []
    }
    
    # åˆ†ç±»å­˜å‚¨å†…å®¹ - å¢å¼ºç‰ˆæœ¬ï¼Œå¢åŠ è°ƒè¯•ä¿¡æ¯
    logger.info(f"ğŸ” å¼€å§‹åˆ†ç±» {len(enhanced_results)} æ¡æœç´¢ç»“æœ...")
    
    # ç”¨äºç»Ÿè®¡å„ç§å†…å®¹ç±»å‹
    type_counts_before_limit = {}
    
    for i, result in enumerate(enhanced_results):  
        content_type = result['metadata'].get('content_type', 'unknown')
        paper_name = result['metadata'].get('paper_name', 'æœªçŸ¥è®ºæ–‡')
        
        # ç»Ÿè®¡å†…å®¹ç±»å‹ (é™åˆ¶å‰)
        type_counts_before_limit[content_type] = type_counts_before_limit.get(content_type, 0) + 1
        
        content_item = {
            "content": result["document"],
            "paper": paper_name,
            "page": result["metadata"].get("page_idx", -1),
            "relevance_score": result['enhanced_similarity'],
            "metadata": result["metadata"],
            "source": "ç« èŠ‚ç‰¹å®šæœç´¢",
            "content_type": content_type  # ğŸ”§ æ·»åŠ ææ–™ç±»å‹å­—æ®µ
        }
        
        # æ”¹è¿›çš„å†…å®¹ç±»å‹åˆ¤æ–­é€»è¾‘
        if content_type in ["text", "texts"]:
            section_materials["texts"].append(content_item)
        elif content_type in ["equation", "equations"]:
            section_materials["equations"].append(content_item)
        elif content_type in ["image", "images", "figure", "figures", "image_text"]:
            # ğŸ†• åŒ…å« image_text ç±»å‹ï¼Œè¿™æ˜¯å›¾ç‰‡çš„æ–‡æœ¬æè¿°
            section_materials["figures"].append(content_item)
        elif content_type in ["table", "tables", "table_text", "table_image"]:
            # ğŸ†• åŒ…å« table_text å’Œ table_image ç±»å‹ï¼Œåˆ†åˆ«æ˜¯è¡¨æ ¼çš„æ–‡æœ¬å†…å®¹å’Œå›¾ç‰‡å½¢å¼
            section_materials["tables"].append(content_item)
        else:
            # é»˜è®¤å½’ç±»ä¸ºæ–‡æœ¬ï¼Œä½†è®°å½•è­¦å‘Šï¼Œå¹¶è¿›è¡Œå»é‡æ£€æŸ¥
            logger.warning(f"âš ï¸ æœªçŸ¥å†…å®¹ç±»å‹ '{content_type}'ï¼Œå½’ç±»ä¸ºæ–‡æœ¬ææ–™")
            text_content = content_item.get("content", "").strip()

    
    # ğŸ“Š è®°å½•é™åˆ¶å‰çš„æ•°é‡
    texts_before = len(section_materials["texts"])
    equations_before = len(section_materials["equations"])
    figures_before = len(section_materials["figures"])
    tables_before = len(section_materials["tables"])
    
    # æ–°å¢ï¼šé™åˆ¶æ¯ç§ç±»å‹ææ–™çš„æ•°é‡
    section_materials["texts"] = section_materials["texts"][:max_texts]
    section_materials["equations"] = section_materials["equations"][:max_equations]
    section_materials["figures"] = section_materials["figures"][:max_figures]
    section_materials["tables"] = section_materials["tables"][:max_tables]
    
    # ğŸ“Š æ˜¾ç¤ºè¯¦ç»†çš„å†…å®¹ç±»å‹åˆ†å¸ƒå’Œé™åˆ¶æ•ˆæœ
    logger.info(f"ğŸ“Š å†…å®¹ç±»å‹åŸå§‹åˆ†å¸ƒ: {type_counts_before_limit}")
    logger.info(f"ğŸ“Š å†…å®¹åˆ†ç±»å’Œæ•°é‡é™åˆ¶ç»Ÿè®¡:")
    logger.info(f"  æ–‡æœ¬ (texts): {texts_before} â†’ {len(section_materials['texts'])} (é™åˆ¶: {max_texts}, åˆ é™¤: {max(0, texts_before - max_texts)})")
    logger.info(f"  å…¬å¼ (equations): {equations_before} â†’ {len(section_materials['equations'])} (é™åˆ¶: {max_equations}, åˆ é™¤: {max(0, equations_before - max_equations)})")
    logger.info(f"  å›¾è¡¨ (figures): {figures_before} â†’ {len(section_materials['figures'])} (é™åˆ¶: {max_figures}, åˆ é™¤: {max(0, figures_before - max_figures)})")
    logger.info(f"  è¡¨æ ¼ (tables): {tables_before} â†’ {len(section_materials['tables'])} (é™åˆ¶: {max_tables}, åˆ é™¤: {max(0, tables_before - max_tables)})")
    
    # ğŸ“Š è®¡ç®—æœ€ç»ˆç»Ÿè®¡
    final_total = sum(len(materials) for materials in section_materials.values())
    total_deleted_by_limit = (texts_before + equations_before + figures_before + tables_before) - final_total
    logger.info(f"ğŸ“Š æ•°é‡é™åˆ¶å½±å“: åˆ é™¤äº† {total_deleted_by_limit} æ¡ç‰¹å®šææ–™")
    logger.info(f"ğŸ“Š æœ€ç»ˆææ–™æ€»æ•°: {final_total} æ¡")
    
    return section_materials


async def gather_section_materials(
    section_info: Dict, 
    db, 
    main_topic,
    similarity_calculator_class, 
    llm_call_func,
    citation_manager,
    logger = None,  # æ–°å¢ï¼šæ—¥å¿—è®°å½•å™¨å‚æ•°
    max_texts: int = 50,
    max_equations: int = 10,
    max_figures: int = 10,
    max_tables: int = 10
    ) -> List[Dict]:
    """
    æ”¶é›†ç« èŠ‚ç›¸å…³ææ–™ï¼Œç»“åˆé€šç”¨ææ–™å’Œç« èŠ‚ç‰¹å®šææ–™
    
    è„šæœ¬ç›®æ ‡: ä¸ºç‰¹å®šç« èŠ‚æ”¶é›†å’Œæ•´åˆç›¸å…³çš„ç ”ç©¶ææ–™
    ä¸Šä¸‹æ–‡: ä»multi_agent.pyä¸­æå–çš„ç« èŠ‚ææ–™æ”¶é›†é€»è¾‘
    è¾“å…¥:
    - section_info: ç« èŠ‚ä¿¡æ¯
    - db: æ•°æ®åº“å¯¹è±¡
    - similarity_calculator_class: ç›¸ä¼¼åº¦è®¡ç®—å™¨ç±»
    - llm_call_func: LLMè°ƒç”¨å‡½æ•°
    - citation_manager: å¼•ç”¨ç®¡ç†å™¨
    æ‰§è¡Œæ­¥éª¤:
    1. æå–ç« èŠ‚æ ‡è¯†ä¿¡æ¯
    2. ä»ä¸Šä¸‹æ–‡ä¸­è·å–é€šç”¨ç›¸å…³ææ–™
    3. è®¡ç®—ææ–™ä¸ç« èŠ‚çš„ç›¸å…³åº¦
    4. è¡¥å……æ•°æ®åº“æœç´¢ç»“æœ
    5. è·å–ç« èŠ‚ç‰¹å®šææ–™
    6. åˆå¹¶æ‰€æœ‰ææ–™å¹¶å¤„ç†å¼•ç”¨
    è¾“å‡º: æ•´åˆåçš„ææ–™åˆ—è¡¨
    
    Args:
        section_info: åŒ…å«ç« èŠ‚ä¿¡æ¯çš„å­—å…¸
        db: æ•°æ®åº“å¯¹è±¡ï¼Œéœ€è¦æœ‰search_contentæ–¹æ³•
        main_topic: ä¸»é¢˜ä¿¡æ¯
        similarity_calculator_class: ç›¸ä¼¼åº¦è®¡ç®—å™¨ç±»
        llm_call_func: LLMè°ƒç”¨å‡½æ•°
        citation_manager: å¼•ç”¨ç®¡ç†å™¨å¯¹è±¡
        logger (optional): æ—¥å¿—è®°å½•å™¨ï¼Œç”¨äºè®°å½•è¿‡ç¨‹ä¿¡æ¯. Defaults to None.
        
    Returns:
        æ•´åˆåçš„ææ–™åˆ—è¡¨
    """
    # è®¾ç½®æ—¥å¿—è®°å½•å™¨
    if logger is None:
        # å¦‚æœæ²¡æœ‰ä¼ å…¥loggerï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„æ§åˆ¶å°æ—¥å¿—å™¨
        import logging
        logger = logging.getLogger("gather_section_materials")
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
    
    # ç« èŠ‚æ ‡è¯†ä¿¡æ¯
    chapter_title = section_info.get("title", "")
    
    # ç¿»è¯‘ç« èŠ‚æ ‡é¢˜
    translated_titles = await translate_keywords_batch([chapter_title], llm_call_func)
    chapter_title = translated_titles[0] if translated_titles else chapter_title

    # ğŸ†• å¤„ç† Introduction çš„ç‰¹æ®Šæƒ…å†µ
    if chapter_title:
        # è·å–ä¸»é¢˜ä¿¡æ¯ï¼Œä½œä¸º Introduction çš„æ›¿æ¢å†…å®¹
        topic = main_topic
        
        # æƒ…å†µ1ï¼šå¦‚æœ chapter_title åªåŒ…å« "Introduction"ï¼ˆéœ€è¦æ³¨æ„å¤§å°å†™çš„å˜åŒ–ï¼Œå¯èƒ½æ˜¯INTRODUCTIONï¼Œintroductionç­‰ç­‰
        if chapter_title.lower() == "introduction":
            if topic:
                chapter_title = topic
                logger.info(f"ğŸ”„ æ£€æµ‹åˆ°å•ç‹¬çš„ 'Introduction' ç« èŠ‚ï¼Œå·²æ›¿æ¢ä¸ºä¸»é¢˜: {topic}")
            else:
                logger.warning("âš ï¸ æ£€æµ‹åˆ° 'Introduction' ç« èŠ‚ä½†æœªæ‰¾åˆ°ä¸»é¢˜ä¿¡æ¯")
        
        # æƒ…å†µ2ï¼šå¦‚æœåŒ…å« "Introduction" ä½†è¿˜æœ‰å…¶ä»–å†…å®¹ï¼Œåˆ™åˆ é™¤ "Introduction"
        elif "introduction" in chapter_title.lower():
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ é™¤ "Introduction" åŠå…¶å‰åçš„åˆ†éš”ç¬¦
            # åˆ é™¤ "Introduction" ä»¥åŠå¯èƒ½çš„åˆ†éš”ç¬¦ï¼ˆå†’å·ã€ç ´æŠ˜å·ã€ç©ºæ ¼ç­‰ï¼‰
            cleaned_title = re.sub(r'\bIntroduction\b[:\-\s]*', '', chapter_title)
            cleaned_title = re.sub(r'[:\-\s]*\bIntroduction\b', '', cleaned_title)
            # æ¸…ç†å¤šä½™çš„ç©ºæ ¼å’Œæ ‡ç‚¹
            cleaned_title = re.sub(r'\s+', ' ', cleaned_title).strip()
            cleaned_title = re.sub(r'^[:\-\s]+|[:\-\s]+$', '', cleaned_title)
            
            if cleaned_title and cleaned_title != chapter_title:
                chapter_title = cleaned_title
                logger.info(f"ğŸ”„ ä»ç« èŠ‚æ ‡é¢˜ä¸­åˆ é™¤äº† 'Introduction'ï¼Œä¿ç•™å†…å®¹: {chapter_title}")

    logger.info(f"ğŸ“ å¤„ç†åçš„ç« èŠ‚æ ‡é¢˜: {chapter_title}")


    # 2. ğŸ”§æ”¹è¿›ï¼šè·å–ç« èŠ‚ç‰¹å®šææ–™ï¼ˆä½¿ç”¨ç¿»è¯‘åçš„ç« èŠ‚æ ‡é¢˜ä½œä¸ºç›¸ä¼¼åº¦è®¡ç®—çš„ä¸»è¦ä¾æ®ï¼‰
    logger.info(f"ğŸ¯ ä½¿ç”¨ç« èŠ‚æ ‡é¢˜ '{chapter_title}' ä½œä¸ºç›¸ä¼¼åº¦è®¡ç®—çš„ä¸»è¦ä¾æ®")
    section_specific_materials = await search_section_specific_materials(
        section_info, db, similarity_calculator_class, llm_call_func,
        chapter_title_english=chapter_title,  # ğŸ”§æ”¹è¿›ï¼šä½¿ç”¨ç¿»è¯‘åçš„ç« èŠ‚æ ‡é¢˜ä½œä¸ºç›¸ä¼¼åº¦è®¡ç®—ä¾æ®
        logger=logger,  # ğŸ†• ä¼ å…¥æ—¥å¿—è®°å½•å™¨
        max_texts=max_texts, max_equations=max_equations, max_figures=max_figures, max_tables=max_tables
    )

    
    # ğŸ†• åˆ†ç±»è¿”å›ææ–™ï¼Œè€Œä¸æ˜¯æ··åˆåœ¨ä¸€èµ·
    categorized_materials = {
        "texts": section_specific_materials.get("texts", []),
        "equations": section_specific_materials.get("equations", []),
        "figures": section_specific_materials.get("figures", []),
        "tables": section_specific_materials.get("tables", [])
    }
    
    # ä¸ºæ‰€æœ‰ææ–™å¤„ç†å¼•ç”¨
    all_materials_for_citation = []
    for material_type, materials in categorized_materials.items():
        logger.info(f"ğŸ“Š ç« èŠ‚ç‰¹å®š{material_type}ææ–™æ•°é‡: {len(materials)} æ¡")
        all_materials_for_citation.extend(materials)
    
    # å¤„ç†ææ–™å¼•ç”¨ï¼Œä¸ºæ¯ä¸ªææ–™æ·»åŠ åˆ°å¼•ç”¨ç®¡ç†å™¨ä¸­
    citation_ids = citation_manager.process_materials_for_citations(all_materials_for_citation)
    
    # ğŸ“Š è¾“å‡ºå®Œæ•´çš„ææ–™æ”¶é›†ç»Ÿè®¡
    total_count = sum(len(materials) for materials in categorized_materials.values())
    logger.info(f"ğŸ“š ç« èŠ‚ '{chapter_title}' ææ–™æ”¶é›†å®Œæˆ:")
    logger.info(f"  ææ–™æ€»æ•°: {total_count} æ¡")
    logger.info(f"  åˆ†ç±»ç»Ÿè®¡: æ–‡æœ¬{len(categorized_materials['texts'])}, å…¬å¼{len(categorized_materials['equations'])}, å›¾ç‰‡{len(categorized_materials['figures'])}, è¡¨æ ¼{len(categorized_materials['tables'])}")
    
    return categorized_materials



def clean_generated_content(content: str) -> str:
    """
    æ¸…ç†ç”Ÿæˆçš„å†…å®¹ï¼Œç§»é™¤å¯èƒ½çš„æŒ‡ä»¤æ€§æ–‡æœ¬
    
    Args:
        content: å¾…æ¸…ç†çš„æ–‡æœ¬å†…å®¹
        
    Returns:
        æ¸…ç†åçš„æ–‡æœ¬å†…å®¹
    """
    # ç§»é™¤å¯èƒ½çš„å‰å¯¼è¯´æ˜æ–‡æœ¬
    content = re.sub(r"^(å¥½çš„|ä¸‹é¢æ˜¯|ä»¥ä¸‹æ˜¯|è¿™æ˜¯|æˆ‘å°†|æˆ‘ä¼š|æ ¹æ®è¦æ±‚|åŸºäºæä¾›çš„ææ–™).*?\n\n", "", content, flags=re.DOTALL)
    
    # ç§»é™¤å¯èƒ½çš„ç»“å°¾è¯´æ˜æ–‡æœ¬
    content = re.sub(r"\n\n(ä»¥ä¸Šæ˜¯|è¿™å°±æ˜¯|å¸Œæœ›è¿™ä¸ª|æˆ‘å·²ç»å®Œæˆ).*?$", "", content, flags=re.DOTALL)
    
    return content.strip()


def extract_authors_from_source(source: str) -> str:
    """
    ä»æ¥æºå­—ç¬¦ä¸²ä¸­æå–ä½œè€…ä¿¡æ¯
    
    Args:
        source: æ¥æºå­—ç¬¦ä¸²ï¼ˆè®ºæ–‡åç§°ç­‰ï¼‰
        
    Returns:
        æå–çš„ä½œè€…ä¿¡æ¯
    """
    # ç®€å•çš„ä½œè€…æå–é€»è¾‘
    if "brown" in source.lower():
        return "Brown et al."
    elif "vaswani" in source.lower():
        return "Vaswani et al."
    elif "devlin" in source.lower():
        return "Devlin et al."
    elif "radford" in source.lower():
        return "Radford et al."
    else:
        return "æœªçŸ¥ä½œè€…"


def clean_academic_citations(text: str) -> str:
    """
    æ¸…ç†å­¦æœ¯æ–‡æœ¬ä¸­çš„å¼•ç”¨ä¿¡æ¯
    
    è„šæœ¬ç›®æ ‡: å»é™¤æ–‡æœ¬ä¸­çš„å­¦æœ¯å¼•ç”¨ï¼Œæé«˜LLMå¯¹æ ¸å¿ƒå†…å®¹çš„å…³æ³¨åº¦
    ä¸Šä¸‹æ–‡: åœ¨æœç´¢ç›¸å…³ææ–™æ—¶ï¼Œå»é™¤æ— æ„ä¹‰çš„å¼•ç”¨ä¿¡æ¯
    è¾“å…¥: åŒ…å«å­¦æœ¯å¼•ç”¨çš„æ–‡æœ¬
    æ‰§è¡Œæ­¥éª¤:
    1. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å„ç§å¼•ç”¨æ ¼å¼
    2. å»é™¤åŒ¹é…åˆ°çš„å¼•ç”¨å†…å®¹
    3. æ¸…ç†å¤šä½™çš„ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·
    4. è¿”å›æ¸…ç†åçš„æ–‡æœ¬
    è¾“å‡º: å»é™¤å¼•ç”¨åçš„å¹²å‡€æ–‡æœ¬
    
    Args:
        text (str): éœ€è¦æ¸…ç†çš„æ–‡æœ¬
        
    Returns:
        str: å»é™¤å¼•ç”¨åçš„æ–‡æœ¬
        
    Examples:
        >>> text = "LLMs are powerful (Tang et al., 2024). They solve tasks (Liu et al., 2024; Li et al., 2024a)."
        >>> clean_academic_citations(text)
        "LLMs are powerful. They solve tasks."
    """
    if not text or not isinstance(text, str):
        return text
    
    import re
    
    # ä¿å­˜åŸå§‹æ–‡æœ¬é•¿åº¦ç”¨äºç»Ÿè®¡
    original_length = len(text)
    
    # 1. åŒ¹é…æ ‡å‡†å­¦æœ¯å¼•ç”¨æ ¼å¼ï¼šåŒ…æ‹¬åœ†æ‹¬å·å¼•ç”¨å’Œæ–¹æ‹¬å·æ•°å­—å¼•ç”¨
    # æ”¯æŒå¤šç§æ ¼å¼ï¼š
    # - (Tang et al., 2024)
    # - (Liu et al., 2024; Li et al., 2024a; Wang, 2023)
    # - (Smith & Jones, 2023)
    # - (Brown et al., 2022a,b)
    # - [14, 37, 40, 48, 63, 75, 76, 90, 96]
    # - [1-5, 10, 15-20]
    # - [14a, 37b]
    citation_patterns = [
        # åŒ¹é…æ–¹æ‹¬å·æ•°å­—å¼•ç”¨ï¼ˆæœ€å¸¸è§çš„æ ¼å¼ï¼‰
        # [14, 37, 40, 48, 63, 75, 76, 90, 96]
        # [1-5, 10, 15-20]
        # [14a, 37b]
        r'\[\s*\d+[a-z]?(?:\s*[-â€“]\s*\d+[a-z]?)?(?:\s*[,;]\s*\d+[a-z]?(?:\s*[-â€“]\s*\d+[a-z]?)?)*\s*\]',
        
        # åŒ¹é…åŒ…å« "et al." çš„åœ†æ‹¬å·å¼•ç”¨
        r'\([^)]*et al\.[^)]*\d{4}[a-z]?[^)]*\)',
        
        # åŒ¹é…æ ‡å‡†çš„ä½œè€…-å¹´ä»½æ ¼å¼ï¼ŒåŒ…æ‹¬å¤šä½œè€…ç”¨åˆ†å·åˆ†éš”çš„æƒ…å†µ
        r'\([^)]*[A-Z][a-z]+(?:\s+(?:&|and)\s+[A-Z][a-z]+)*\s*,\s*\d{4}[a-z]?[^)]*\)',
        
        # åŒ¹é…åŒ…å«å¤šä¸ªå¼•ç”¨çš„å¤æ‚æ ¼å¼ï¼ˆç”¨åˆ†å·åˆ†éš”ï¼‰
        r'\([^)]*\d{4}[a-z]?(?:\s*[;,]\s*[^)]*\d{4}[a-z]?)*[^)]*\)',
        
        # åŒ¹é…ç®€å•çš„å¹´ä»½å¼•ç”¨
        r'\(\s*\d{4}[a-z]?\s*\)',
        
        # åŒ¹é… ibid., op. cit. ç­‰å­¦æœ¯å¼•ç”¨
        r'\([^)]*(?:ibid\.|op\.\s*cit\.|loc\.\s*cit\.)[^)]*\)',
        
        # åŒ¹é…å•ä¸ªæ–¹æ‹¬å·æ•°å­—å¼•ç”¨ï¼ˆé˜²æ­¢é—æ¼ï¼‰
        r'\[\s*\d+[a-z]?\s*\]',
    ]
    
    # åº”ç”¨æ‰€æœ‰å¼•ç”¨æ¨¡å¼
    for pattern in citation_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    # 2. æ¸…ç†ç”±äºåˆ é™¤å¼•ç”¨å¯¼è‡´çš„å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·
    # åˆ é™¤å¤šä¸ªè¿ç»­ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text)
    
    # åˆ é™¤å¤šä¸ªè¿ç»­çš„é€—å·æˆ–åˆ†å·
    text = re.sub(r'[,;]\s*[,;]+', ',', text)
    
    # åˆ é™¤å¥å·å‰çš„å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+\.', '.', text)
    
    # åˆ é™¤å¥å­å¼€å¤´çš„é€—å·æˆ–åˆ†å·
    text = re.sub(r'^\s*[,;]\s*', '', text)
    text = re.sub(r'\.\s*[,;]\s*', '. ', text)
    
    # åˆ é™¤å¤šä¸ªè¿ç»­çš„å¥å·
    text = re.sub(r'\.{2,}', '.', text)
    
    # ğŸ†• å¤„ç†åˆ é™¤æ–¹æ‹¬å·å¼•ç”¨åçš„ç‰¹æ®Šæƒ…å†µ
    # åˆ é™¤å•è¯é—´çš„å¤šä½™ç©ºæ ¼ï¼ˆå¦‚ "field [14, 37] applications" å˜æˆ "field  applications"ï¼‰
    text = re.sub(r'([a-zA-Z])\s{2,}([a-zA-Z])', r'\1 \2', text)
    
    # åˆ é™¤å¥æœ«å¼•ç”¨åé—ç•™çš„ç©ºæ ¼å’Œæ ‡ç‚¹é—®é¢˜
    # å¦‚ "applications . " æ”¹ä¸º "applications."
    text = re.sub(r'\s+([.!?;,])', r'\1', text)
    
    # åˆ é™¤æ®µè½å¼€å¤´çš„ç©ºæ ¼å’Œå¥‡æ€ªå­—ç¬¦
    text = re.sub(r'^\s*[.;,]\s*', '', text)
    
    # å¤„ç†å¦‚ "within the medical field . They can" è¿™æ ·çš„æƒ…å†µ
    text = re.sub(r'([a-zA-Z])\s+\.\s+([A-Z])', r'\1. \2', text)
    
    # æ¸…ç†é¦–å°¾ç©ºæ ¼
    text = text.strip()
    
    # ç»Ÿè®¡æ¸…ç†æ•ˆæœ
    cleaned_length = len(text)
    if original_length > cleaned_length:
        reduction = original_length - cleaned_length
        reduction_percent = (reduction / original_length) * 100
        # è¿™é‡Œä½¿ç”¨ç®€å•çš„printï¼Œå®é™…ä½¿ç”¨æ—¶å¯ä»¥æ›¿æ¢ä¸ºlogger
        # print(f"ğŸ“ å¼•ç”¨æ¸…ç†: åˆ é™¤ {reduction} å­—ç¬¦ ({reduction_percent:.1f}%)")
    
    return text


def extract_title_from_source(source: str) -> str:
    """
    ä»æ¥æºå­—ç¬¦ä¸²ä¸­æå–æ ‡é¢˜
    
    Args:
        source: æ¥æºå­—ç¬¦ä¸²ï¼ˆæ–‡ä»¶åç­‰ï¼‰
        
    Returns:
        æå–çš„æ ‡é¢˜
    """
    # ç§»é™¤æ–‡ä»¶æ‰©å±•åå’Œè·¯å¾„
    title = source.replace(".pdf_result", "").replace("_", " ")
    # æ¸…ç†ç‰¹æ®Šå­—ç¬¦
    title = re.sub(r'[^\w\s-]', '', title)
    return title.strip()


class LLMLogger:
    """æ—¥å¿—è®°å½•å™¨ï¼Œç”¨äºè®°å½•LLMè°ƒç”¨çš„è¾“å…¥å’Œè¾“å‡º"""
    
    def __init__(self, log_dir="./logs"):
        """åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨"""
        self.log_dir = log_dir
        os.makedirs(log_dir, exist_ok=True)
        
        # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„æ—¥å¿—æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_file = f"{log_dir}/llm_calls_{timestamp}.log"
        self.json_log_file = f"{log_dir}/llm_calls_{timestamp}.json"
        
        # åˆå§‹åŒ–JSONæ—¥å¿—
        self.json_logs = []
        
        # è®¾ç½®æ–‡æœ¬æ—¥å¿—
        logging.basicConfig(
            filename=self.log_file,
            level=logging.INFO,
            format='%(asctime)s [%(levelname)s] %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        self.logger = logging.getLogger("LLM_Logger")
        
        # æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        console_formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')
        console_handler.setFormatter(console_formatter)
        self.logger.addHandler(console_handler)
    
    def log_call(self, agent_name: str, model_name: str, messages: list, response: dict, task_type: str = None):
        """è®°å½•ä¸€æ¬¡LLMè°ƒç”¨"""
        # åˆ›å»ºæ—¥å¿—æ¡ç›®
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "agent_name": agent_name,
            "model_name": model_name,
            "task_type": task_type,
            "messages": messages,
            "response": response
        }
        
        # ä¿å­˜åˆ°JSONæ—¥å¿—
        self.json_logs.append(log_entry)
        self.save_json_logs()
        
        # è®°å½•åˆ°æ–‡æœ¬æ—¥å¿—
        self.logger.info(f"Agent: {agent_name} | Model: {model_name} | Task: {task_type}")
        self.logger.info(f"Input messages count: {len(messages)}")
        
        # è®°å½•æœ€åä¸€æ¡è¾“å…¥æ¶ˆæ¯
        if messages and len(messages) > 0:
            last_msg = messages[-1].get("content", "")
            self.logger.info(f"Last input message (truncated): {last_msg[:80]}...")
        
        # è®°å½•å“åº”å†…å®¹
        response_content = response.get("content", "")
        if response_content:
            self.logger.info(f"Response (truncated): {response_content[:80]}...")
            
        # è®°å½•ä½¿ç”¨æƒ…å†µ
        if "usage" in response:
            usage = response["usage"]
            self.logger.info(f"Usage: {usage}")
        
        self.logger.info("-" * 50)
    
    def log_parsed_structure(self, agent_name: str, task_type: str, parsed_structure: dict):
        """è®°å½•è§£æåçš„ç»“æ„åŒ–æ•°æ®"""
        # åˆ›å»ºæ—¥å¿—æ¡ç›®
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "agent_name": agent_name,
            "task_type": task_type,
            "parsed_structure": parsed_structure
        }
        
        # ä¿å­˜åˆ°JSONæ—¥å¿—
        self.json_logs.append(log_entry)
        self.save_json_logs()
        
        # è®°å½•åˆ°æ–‡æœ¬æ—¥å¿—
        self.logger.info(f"Agent: {agent_name} | Task: {task_type} | Parsed Structure")
        self.logger.info(f"Parsed structure keys: {list(parsed_structure.keys())}")
        
        # è®°å½•è§£æç»“æœçš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        if "chapters" in parsed_structure:
            chapters_count = len(parsed_structure["chapters"])
            self.logger.info(f"Parsed {chapters_count} chapters")

            # ç»Ÿè®¡å­ç« èŠ‚æ•°é‡
            subsections_count = 0
            # æ£€æŸ¥chaptersæ˜¯åˆ—è¡¨è¿˜æ˜¯å­—å…¸
            if isinstance(parsed_structure["chapters"], list):
                for chapter in parsed_structure["chapters"]:
                    if "subsections" in chapter:
                        subsections_count += len(chapter["subsections"])
            elif isinstance(parsed_structure["chapters"], dict):
                for chapter_id, chapter in parsed_structure["chapters"].items():
                    if "subsections" in chapter:
                        subsections_count += len(chapter["subsections"])

            self.logger.info(f"Parsed {subsections_count} subsections")
        
        self.logger.info("-" * 50)
    
    def save_json_logs(self):
        """ä¿å­˜JSONæ—¥å¿—åˆ°æ–‡ä»¶"""
        try:
            with open(self.json_log_file, 'w', encoding='utf-8') as f:
                json.dump(self.json_logs, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ä¿å­˜JSONæ—¥å¿—å¤±è´¥: {e}")


async def translate_keywords_batch(keywords: List[str], llm_call_func) -> List[str]:
    """
    æ‰¹é‡ç¿»è¯‘å…³é”®è¯ï¼Œæé«˜æ•ˆç‡
    
    Args:
        keywords: éœ€è¦ç¿»è¯‘çš„å…³é”®è¯åˆ—è¡¨
        llm_call_func: LLMè°ƒç”¨å‡½æ•°ï¼Œåº”è¯¥æ¥å—promptå’Œtask_typeå‚æ•°
        
    Returns:
        ç¿»è¯‘åçš„å…³é”®è¯åˆ—è¡¨
    """
    if not keywords:
        return []
    
    # åˆ†ç¦»éœ€è¦ç¿»è¯‘å’Œä¸éœ€è¦ç¿»è¯‘çš„è¯æ±‡
    english_keywords = []
    need_translation = []
    need_translation_indices = []
    
    for i, keyword in enumerate(keywords):
        if not keyword or keyword.strip().isascii():
            english_keywords.append(keyword.strip() if keyword else "")
        else:
            # è¿™é‡Œå¯ä»¥æ·»åŠ ç¼“å­˜æ£€æŸ¥çš„é€»è¾‘
            english_keywords.append("")  # å ä½ç¬¦
            need_translation.append(keyword.strip())
            need_translation_indices.append(i)
    
    # æ‰¹é‡ç¿»è¯‘éœ€è¦ç¿»è¯‘çš„è¯æ±‡
    if need_translation:
        try:
            batch_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è®¡ç®—æœºé¢†åŸŸçš„å­¦æœ¯ç¿»è¯‘ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹ä¸­æ–‡å­¦æœ¯æœ¯è¯­ç²¾ç¡®ç¿»è¯‘ä¸ºè‹±æ–‡å­¦æœ¯æœ¯è¯­ã€‚

    ã€ç¿»è¯‘è¦æ±‚ã€‘
    1. å¿…é¡»ä½¿ç”¨æ ‡å‡†çš„å­¦æœ¯è‹±æ–‡æœ¯è¯­
    2. ä¿æŒæœ¯è¯­çš„ä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§
    3. ä¸¥æ ¼æŒ‰ç…§ç»™å®šé¡ºåºé€ä¸€ç¿»è¯‘
    4. åªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦ä»»ä½•è§£é‡Šæˆ–æè¿°
    5. æ¯ä¸ªæœ¯è¯­è¾“å‡ºä¸€ä¸ªæœ€è´´è¿‘çš„ç¿»è¯‘ç»“æœ

    ã€è¾“å…¥æœ¯è¯­ã€‘
    {chr(10).join(f"{i+1}. {term}" for i, term in enumerate(need_translation))}

    ã€è¾“å‡ºæ ¼å¼ã€‘
    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼é€è¡Œè¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ï¼š
    1. [ç¬¬ä¸€ä¸ªæœ¯è¯­çš„è‹±æ–‡ç¿»è¯‘]
    2. [ç¬¬äºŒä¸ªæœ¯è¯­çš„è‹±æ–‡ç¿»è¯‘]
    3. [ç¬¬ä¸‰ä¸ªæœ¯è¯­çš„è‹±æ–‡ç¿»è¯‘]
    ...

    å¼€å§‹ç¿»è¯‘ï¼š"""
            
            response = await llm_call_func(batch_prompt, task_type="batch_translation")
            
            if response.get("content"):
                translated_lines = response["content"].strip().split('\n')
                
                # å¤„ç†ç¿»è¯‘ç»“æœ - æ”¹è¿›çš„è§£æé€»è¾‘
                successful_translations = 0
                for i, line in enumerate(translated_lines):
                    if i < len(need_translation):
                        # æ¸…ç†ç¿»è¯‘ç»“æœ
                        clean_translation = line.strip()
                        
                        # ç§»é™¤å¯èƒ½çš„ç¼–å·å’Œæ ¼å¼æ ‡è®°
                        clean_translation = re.sub(r'^\d+[.)\s]+', '', clean_translation)
                        clean_translation = re.sub(r'^[-*]\s+', '', clean_translation)
                        clean_translation = re.sub(r'^\[', '', clean_translation)
                        clean_translation = re.sub(r'\]$', '', clean_translation)
                        clean_translation = clean_translation.strip('"\'')
                        clean_translation = clean_translation.strip()
                        
                        # éªŒè¯ç¿»è¯‘è´¨é‡
                        is_valid_translation = (
                            clean_translation and 
                            len(clean_translation) > 0 and
                            not clean_translation.lower().startswith("æŠ±æ­‰") and
                            not clean_translation.lower().startswith("å¯¹ä¸èµ·") and
                            not clean_translation.lower().startswith("ç¿»è¯‘") and
                            not "ä»¥ä¸‹æ˜¯" in clean_translation and
                            not "æŒ‰ç…§" in clean_translation and
                            clean_translation != need_translation[i]  # ç¡®ä¿ä¸æ˜¯åŸæ–‡
                        )
                        
                        if is_valid_translation:
                            # æ›´æ–°ç»“æœåˆ—è¡¨
                            index = need_translation_indices[i]
                            english_keywords[index] = clean_translation
                            successful_translations += 1
                        else:
                            # ç¿»è¯‘å¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡
                            index = need_translation_indices[i]
                            english_keywords[index] = need_translation[i]
                            print(f"âš ï¸ ç¿»è¯‘è´¨é‡ä¸ç¬¦åˆè¦æ±‚ï¼Œä½¿ç”¨åŸæ–‡: {need_translation[i]} (LLMè¾“å‡º: '{clean_translation}')")
                
                print(f"ğŸ“Š æ‰¹é‡ç¿»è¯‘ç»Ÿè®¡: æˆåŠŸ {successful_translations}/{len(need_translation)} ä¸ªæœ¯è¯­")
            
        except Exception as e:
            print(f"âš ï¸ æ‰¹é‡ç¿»è¯‘é”™è¯¯: {e}ï¼Œä½¿ç”¨åŸæ–‡")
            # ç¿»è¯‘å¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡
            for i, original in enumerate(need_translation):
                index = need_translation_indices[i]
                english_keywords[index] = original
    
    # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
    result = [kw for kw in english_keywords if kw.strip()]
    return result


def parse_outline_response(response_content: str, topic: str, subtopics: List[str] = None, found_start_marker: str = None) -> Dict:
    """
    è§£æPlannerç”Ÿæˆçš„å¤§çº²å“åº”ï¼Œæå–ç»“æ„åŒ–çš„å¤§çº²ä¿¡æ¯
    
    è„šæœ¬ç›®æ ‡: å°†LLMç”Ÿæˆçš„å¤§çº²å“åº”è§£æä¸ºç»“æ„åŒ–çš„æ•°æ®æ ¼å¼
    ä¸Šä¸‹æ–‡: ä»multi_agent.pyä¸­æå–çš„å¤§çº²è§£æé€»è¾‘
    è¾“å…¥: LLMå“åº”å†…å®¹ã€ä¸»é¢˜å’Œå­ä¸»é¢˜
    æ‰§è¡Œæ­¥éª¤: 
    1. æ£€æŸ¥å“åº”æœ‰æ•ˆæ€§
    2. æå–å¤§çº²æ ‡è®°ä¹‹é—´çš„å†…å®¹  
    3. è§£ææ¦‚è¿°å’Œç« èŠ‚ç»“æ„
    4. æ„å»ºç»“æ„åŒ–å¤§çº²å¯¹è±¡
    5. å¤‡ç”¨è§£ææ–¹æ¡ˆå¤„ç†
    è¾“å‡º: ç»“æ„åŒ–çš„å¤§çº²å­—å…¸
    
    Args:
        response_content: LLMçš„å“åº”å†…å®¹
        topic: ç»¼è¿°ä¸»é¢˜
        subtopics: å­ä¸»é¢˜åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        ç»“æ„åŒ–çš„å¤§çº²å­—å…¸ï¼ŒåŒ…å«topicã€subtopicsã€overviewã€chaptersç­‰å­—æ®µ
        
    Raises:
        ValueError: å½“å“åº”å†…å®¹ä¸ºç©ºæˆ–æ— æ•ˆæ—¶
    """
    if not response_content:
        raise ValueError("å¤§çº²ç”Ÿæˆå¤±è´¥ - LLMå“åº”å†…å®¹ä¸ºç©º")
    
    # ğŸ†• æ”¾å®½å¼€å¤´æ ‡è®°é™åˆ¶ - æ”¯æŒå¤šç§å¼€å¤´æ ‡è®°
    outline_text = response_content
    
    # å…ˆå°è¯•æ‰¾åˆ°å®Œæ•´çš„æ ‡è®°å¯¹
    outline_match = re.search(r"===å¤§çº²å¼€å§‹===(.*?)===å¤§çº²ç»“æŸ===", response_content, re.DOTALL)
    if outline_match:
        outline_text = outline_match.group(1)
        print("âœ“ æ‰¾åˆ°å®Œæ•´çš„===å¤§çº²å¼€å§‹===å’Œ===å¤§çº²ç»“æŸ===æ ‡è®°")
    else:
        # å°è¯•å¤šç§å¼€å¤´æ ‡è®°
        start_markers = ["ã€ä¼˜åŒ–åå¤§çº²ã€‘", "===å¤§çº²å¼€å§‹===", "ã€ç»¼è¿°æ¦‚è¿°ã€‘", "===ä¼˜åŒ–ç»“æœå¼€å§‹==="]
        
        # å¦‚æœä¼ å…¥äº†found_start_markerï¼Œä¼˜å…ˆä½¿ç”¨å®ƒ
        if found_start_marker:
            start_markers = [found_start_marker] + [m for m in start_markers if m != found_start_marker]
        
        found_start = False
        for marker in start_markers:
            start_pos = response_content.find(marker)
            if start_pos != -1:
                # æ‰¾åˆ°å¼€å¤´æ ‡è®°ï¼ŒæŸ¥æ‰¾ç»“æŸæ ‡è®°
                end_pos = response_content.find("===å¤§çº²ç»“æŸ===", start_pos)
                if end_pos != -1:
                    # æ‰¾åˆ°äº†ç»“æŸæ ‡è®°
                    outline_text = response_content[start_pos + len(marker):end_pos]
                    print(f"âœ“ æ‰¾åˆ°å®Œæ•´æ ‡è®°å¯¹: {marker} ... ===å¤§çº²ç»“æŸ===")
                else:
                    # æ²¡æœ‰ç»“æŸæ ‡è®°ï¼Œä½¿ç”¨ä»å¼€å¤´æ ‡è®°åˆ°ç»“å°¾çš„å†…å®¹
                    outline_text = response_content[start_pos + len(marker):]
                    print(f"âš ï¸ åªæ‰¾åˆ°å¼€å¤´æ ‡è®°: {marker}ï¼Œä½¿ç”¨åˆ°ç»“å°¾çš„å†…å®¹")
                found_start = True
                break
        
        if not found_start:
            print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„å¼€å¤´æ ‡è®°ï¼Œä½¿ç”¨å®Œæ•´å“åº”è§£æ")
    
    # æ„å»ºç»“æ„åŒ–å¤§çº²
    outline = {
        "topic": topic,
        "subtopics": subtopics or [],  # ğŸ†• åŒ…å«å­ä¸»é¢˜ä¿¡æ¯
        "overview": "",  # å°†ä»å“åº”ä¸­æå–
        "chapters": []   # å°†ä»å“åº”ä¸­æå–
    }
    
    # æå–æ¦‚è¿° - ç²¾ç¡®åŒ¹é…ã€ç»¼è¿°æ¦‚è¿°ã€‘æ ‡ç­¾ä¹‹åçš„å†…å®¹ï¼Œç›´åˆ°ã€ç« èŠ‚ç»“æ„ã€‘æˆ–ã€ä¼˜åŒ–åå¤§çº²ã€‘æ ‡ç­¾
    overview_match = re.search(r"ã€ç»¼è¿°æ¦‚è¿°ã€‘\s*(.*?)(?=ã€ç« èŠ‚ç»“æ„ã€‘|ã€ä¼˜åŒ–åå¤§çº²ã€‘|$)", outline_text, re.DOTALL)
    if overview_match:
        outline["overview"] = overview_match.group(1).strip()
    else:
        # å¤‡ç”¨æ–¹æ¡ˆï¼šå°è¯•æŸ¥æ‰¾"æ¦‚è¿°"æˆ–"æ‘˜è¦"å…³é”®è¯
        alt_overview = re.search(r"(?:æ¦‚è¿°|æ‘˜è¦)[:ï¼š](.*?)(?=\d+\.\s|\n\d+\.|\[\[|$)", outline_text, re.DOTALL)
        if alt_overview:
            outline["overview"] = alt_overview.group(1).strip()
    
    # æå–æ‰€æœ‰ä¸€çº§ç« èŠ‚
    chapter_matches = []
    chapter_pattern = re.compile(r"(\d+)\.\s+(.*?)(?:\n|$)((?:(?!\d+\.\s+).)*?)(?=\d+\.\s+|\Z)", re.DOTALL)
    for match in chapter_pattern.finditer(outline_text):
        chapter_id = match.group(1)
        chapter_title = match.group(2).strip()
        chapter_content = match.group(3).strip()
        chapter_matches.append((chapter_id, chapter_title, chapter_content))

    # å¤„ç†æ¯ä¸ªç« èŠ‚
    for chapter_id, chapter_title, chapter_content in chapter_matches:
        # åˆå§‹åŒ–ç« èŠ‚æ•°æ®
        chapter = {
            "id": chapter_id,
            "title": chapter_title,
            "description": "",
            "subsections": []
        }
        
        # æå–ç« èŠ‚æè¿°å’Œå­ç« èŠ‚
        lines = chapter_content.split('\n')
        description_lines = []
        current_subsection = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # æ£€æŸ¥æ˜¯å¦ä¸ºå­ç« èŠ‚
            subsection_match = re.match(r"(\d+\.\d+)\s+(.*?)$", line)
            if subsection_match:
                # å¦‚æœæœ‰å½“å‰å­ç« èŠ‚ï¼Œä¿å­˜å®ƒ
                if current_subsection:
                    chapter["subsections"].append(current_subsection)
                    
                # åˆ›å»ºæ–°å­ç« èŠ‚
                subsection_id = subsection_match.group(1)
                subsection_title = subsection_match.group(2).strip()
                current_subsection = {
                    "id": subsection_id,
                    "title": subsection_title,
                    "description": ""
                }
            # å¤„ç†å­ç« èŠ‚æè¿°
            elif current_subsection is not None:
                # å¦‚æœä¸æ˜¯ä»¥æ•°å­—å¼€å¤´çš„æ–°å­ç« èŠ‚ï¼Œå°±æ˜¯å½“å‰å­ç« èŠ‚çš„æè¿°
                current_subsection["description"] += line + " "
            # å¤„ç†ç« èŠ‚æè¿°
            else:
                # å¦‚æœä¸æ˜¯å­ç« èŠ‚ï¼Œä¹Ÿä¸æ˜¯ç©ºè¡Œï¼Œå°±æ˜¯ç« èŠ‚æè¿°
                description_lines.append(line)
        
        # æ·»åŠ æœ€åä¸€ä¸ªå­ç« èŠ‚
        if current_subsection:
            chapter["subsections"].append(current_subsection)
        
        # è®¾ç½®ç« èŠ‚æè¿°
        chapter["description"] = " ".join(description_lines).strip()
        
        # æ¸…ç†æ‰€æœ‰æè¿°ï¼ˆç§»é™¤å¤šä½™ç©ºæ ¼ï¼‰
        chapter["description"] = re.sub(r'\s+', ' ', chapter["description"]).strip()
        for subsection in chapter["subsections"]:
            subsection["description"] = re.sub(r'\s+', ' ', subsection["description"]).strip()
        
        # æ·»åŠ åˆ°å¤§çº²
        outline["chapters"].append(chapter)
    
    # å¤„ç†å¯èƒ½çš„ä¸å®Œæ•´æ ¼å¼
    if not outline["chapters"]:
        # å¤‡ç”¨è§£ææ–¹æ¡ˆï¼šå°è¯•æŸ¥æ‰¾ä»»ä½•æ•°å­—ç¼–å·çš„æ ‡é¢˜
        simple_chapters = re.findall(r"(\d+)[\.ã€]\s*(.*?)(?:\n|$)((?:(?!\d+[\.ã€]\s+).)*?)(?=\d+[\.ã€]\s+|\Z)", 
                                outline_text, re.DOTALL)
        
        for chap_id, chap_title, chap_content in simple_chapters:
            chapter = {
                "id": chap_id,
                "title": chap_title.strip(),
                "description": chap_content.strip(),
                "subsections": []
            }
            
            # å°è¯•æå–å­ç« èŠ‚
            subsections = re.findall(r"(\d+\.\d+)[\.ã€]?\s*(.*?)(?:\n|$)((?:(?!\d+\.?\d+[\.ã€]?\s+).)*?)(?=\d+\.?\d+[\.ã€]?\s+|\Z)", 
                                chap_content, re.DOTALL)
            
            for sub_id, sub_title, sub_content in subsections:
                subsection = {
                    "id": sub_id,
                    "title": sub_title.strip(),
                    "description": sub_content.strip()
                }
                chapter["subsections"].append(subsection)
                
            # å¦‚æœæ‰¾åˆ°å­ç« èŠ‚ï¼Œä»ç« èŠ‚æè¿°ä¸­ç§»é™¤
            if chapter["subsections"]:
                # æå–å­ç« èŠ‚ä¹‹å‰çš„å†…å®¹ä½œä¸ºç« èŠ‚æè¿°
                pre_subsection = re.match(r"(.*?)(?=\d+\.\d+[\.ã€]?\s+)", chap_content, re.DOTALL)
                if pre_subsection:
                    chapter["description"] = pre_subsection.group(1).strip()
                    
            outline["chapters"].append(chapter)
    
    return outline


def build_detailed_planning_section(enriched_outline: Dict) -> str:
    """
    æ„å»ºè¯¦ç»†çš„ç« èŠ‚è§„åˆ’å†…å®¹ï¼Œç”¨äºæ‘˜è¦ç”Ÿæˆ
    
    è„šæœ¬ç›®æ ‡: å°†enriched_outlineä¸­çš„è¯¦ç»†è§„åˆ’ä¿¡æ¯æ ¼å¼åŒ–ä¸ºå¯è¯»çš„æ–‡æœ¬
    ä¸Šä¸‹æ–‡: ä»multi_agent.pyä¸­æå–çš„æ‘˜è¦ç”Ÿæˆéƒ¨åˆ†çš„è¯¦ç»†è§„åˆ’é€»è¾‘
    è¾“å…¥: enriched_outline - åŒ…å«è¯¦ç»†ç« èŠ‚è§„åˆ’çš„å­—å…¸
    æ‰§è¡Œæ­¥éª¤:
    1. æ£€æŸ¥è¾“å…¥æ•°æ®çš„æœ‰æ•ˆæ€§
    2. å¤„ç†ç« èŠ‚æ•°æ®ç»“æ„ï¼ˆå­—å…¸æˆ–åˆ—è¡¨ï¼‰
    3. æ ¼å¼åŒ–ç« èŠ‚ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€å†…å®¹æŒ‡å¼•ã€å…³é”®è¯ç­‰ï¼‰
    4. å¤„ç†å­ç« èŠ‚çš„è¯¦ç»†ä¿¡æ¯
    5. æ„å»ºå®Œæ•´çš„è§„åˆ’æ–‡æœ¬
    è¾“å‡º: æ ¼å¼åŒ–çš„è¯¦ç»†è§„åˆ’æ–‡æœ¬å­—ç¬¦ä¸²
    
    Args:
        enriched_outline: åŒ…å«è¯¦ç»†ç« èŠ‚è§„åˆ’ä¿¡æ¯çš„å­—å…¸
        
    Returns:
        æ ¼å¼åŒ–çš„è¯¦ç»†è§„åˆ’å†…å®¹å­—ç¬¦ä¸²
    """
    planning_content = ""
    
    # æ£€æŸ¥è¾“å…¥æ•°æ®çš„æœ‰æ•ˆæ€§
    if not enriched_outline or not enriched_outline.get("chapters"):
        return planning_content
    
    planning_content += "\nã€è¯¦ç»†ç« èŠ‚è§„åˆ’ç»“æ„ã€‘\n"
    
    chapters_data = enriched_outline.get("chapters", {})
    
    # å¤„ç†ä¸¤ç§å¯èƒ½çš„æ•°æ®ç»“æ„ï¼šå­—å…¸æˆ–åˆ—è¡¨
    if isinstance(chapters_data, dict):
        # æŒ‰ç« èŠ‚IDæ’åº
        sorted_chapter_items = sorted(chapters_data.items(), key=lambda x: x[0])
        
        for chapter_id, chapter_data in sorted_chapter_items:
            planning_content += _format_chapter_details(chapter_id, chapter_data)
    
    elif isinstance(chapters_data, list):
        # ä¿æŒå‘åå…¼å®¹çš„åˆ—è¡¨æ ¼å¼å¤„ç†
        for chapter_data in chapters_data:
            chapter_id = chapter_data.get("id", "")
            planning_content += _format_chapter_details(chapter_id, chapter_data)
    
    return planning_content


def _format_chapter_details(chapter_id: str, chapter_data: Dict) -> str:
    """
    æ ¼å¼åŒ–å•ä¸ªç« èŠ‚çš„è¯¦ç»†ä¿¡æ¯
    
    Args:
        chapter_id: ç« èŠ‚ID
        chapter_data: ç« èŠ‚æ•°æ®å­—å…¸
        
    Returns:
        æ ¼å¼åŒ–çš„ç« èŠ‚è¯¦ç»†ä¿¡æ¯å­—ç¬¦ä¸²
    """
    chapter_content = ""
    
    chapter_title = chapter_data.get("title", "æœªå‘½åç« èŠ‚")
    content_guide = chapter_data.get("content_guide", "")
    keywords = chapter_data.get("keywords", [])
    research_focus = chapter_data.get("research_focus", [])
    
    chapter_content += f"\n==== ç¬¬{chapter_id}ç« : {chapter_title} ====\n"
    
    # ç« èŠ‚å†…å®¹æŒ‡å¼•ï¼ˆå®Œæ•´æ˜¾ç¤ºï¼‰
    if content_guide:
        chapter_content += f"ã€ç« èŠ‚å†…å®¹æŒ‡å¼•ã€‘\n{content_guide}\n\n"
    
    # ç« èŠ‚å…³é”®è¯
    if keywords:
        chapter_content += f"ã€ç« èŠ‚å…³é”®è¯ã€‘\n{', '.join(keywords)}\n\n"
    
    # é‡ç‚¹ç ”ç©¶é¢†åŸŸ
    if research_focus:
        chapter_content += f"ã€é‡ç‚¹ç ”ç©¶é¢†åŸŸã€‘\n"
        for i, focus in enumerate(research_focus, 1):
            chapter_content += f"{i}. {focus}\n"
        chapter_content += "\n"
    
    # è¯¦ç»†å­ç« èŠ‚ä¿¡æ¯
    subsections = chapter_data.get("subsections", {})
    if subsections:
        chapter_content += f"ã€å­ç« èŠ‚è¯¦ç»†è§„åˆ’ã€‘\n"
        
        if isinstance(subsections, dict):
            # æŒ‰å­ç« èŠ‚IDæ’åº
            sorted_subsection_items = sorted(subsections.items(), key=lambda x: x[0])
            
            for sub_id, sub_data in sorted_subsection_items:
                chapter_content += _format_subsection_details(sub_id, sub_data)
        
        elif isinstance(subsections, list):
            for sub_data in subsections:
                sub_id = sub_data.get("id", "")
                chapter_content += _format_subsection_details(sub_id, sub_data)
    
    chapter_content += "=" * 60 + "\n"
    
    return chapter_content


def _format_subsection_details(sub_id: str, sub_data: Dict) -> str:
    """
    æ ¼å¼åŒ–å•ä¸ªå­ç« èŠ‚çš„è¯¦ç»†ä¿¡æ¯
    
    Args:
        sub_id: å­ç« èŠ‚ID
        sub_data: å­ç« èŠ‚æ•°æ®å­—å…¸
        
    Returns:
        æ ¼å¼åŒ–çš„å­ç« èŠ‚è¯¦ç»†ä¿¡æ¯å­—ç¬¦ä¸²
    """
    subsection_content = ""
    
    sub_title = sub_data.get("title", "")
    sub_content_guide = sub_data.get("content_guide", "")
    key_points = sub_data.get("key_points", [])
    writing_guide = sub_data.get("writing_guide", "")
    
    subsection_content += f"\n  â—† {sub_id} {sub_title}\n"
    
    if sub_content_guide:
        subsection_content += f"    ã€å†…å®¹æ¦‚è¦ã€‘{sub_content_guide}\n"
    
    if key_points:
        subsection_content += f"    ã€å…³é”®è¦ç‚¹ã€‘\n"
        for i, point in enumerate(key_points, 1):
            subsection_content += f"      â€¢ {point}\n"
    
    if writing_guide:
        subsection_content += f"    ã€å†™ä½œå»ºè®®ã€‘{writing_guide}\n"
    
    subsection_content += "\n"
    
    return subsection_content


def parse_abstract_response(raw_response: str) -> tuple[str, list]:
    """
    è§£æLLMæ‘˜è¦å“åº”ï¼Œæå–æ‘˜è¦å†…å®¹å’Œå…³é”®è¯ï¼Œè¿‡æ»¤æ‰æ€è€ƒè¿‡ç¨‹
    
    Args:
        raw_response: LLMçš„åŸå§‹å“åº”æ–‡æœ¬
        
    Returns:
        tuple: (æ‘˜è¦æ–‡æœ¬, å…³é”®è¯åˆ—è¡¨)
    """
    # é¦–å…ˆå°è¯•æå–===æ‘˜è¦å¼€å§‹===å’Œ===æ‘˜è¦ç»“æŸ===ä¹‹é—´çš„å†…å®¹
    abstract_match = re.search(r"===æ‘˜è¦å¼€å§‹===(.*?)===æ‘˜è¦ç»“æŸ===", raw_response, re.DOTALL)
    
    if abstract_match:
        # æ‰¾åˆ°äº†æ ‡è®°ï¼Œæå–æ‘˜è¦éƒ¨åˆ†
        abstract_section = abstract_match.group(1).strip()
        print("âœ… æˆåŠŸæå–æ ‡è®°å†…çš„æ‘˜è¦å†…å®¹")
    else:
        # æ²¡æ‰¾åˆ°æ ‡è®°ï¼Œå°è¯•å¤‡ç”¨è§£ææ–¹æ¡ˆ
        print("âš ï¸ æœªæ‰¾åˆ°æ‘˜è¦æ ‡è®°ï¼Œå°è¯•å¤‡ç”¨è§£ææ–¹æ¡ˆ...")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ€è€ƒè¿‡ç¨‹æ ‡è®°ï¼Œå¦‚æœæœ‰åˆ™æˆªå–ä¹‹å‰çš„å†…å®¹
        if "===æ€è€ƒè¿‡ç¨‹å¼€å§‹===" in raw_response:
            abstract_section = raw_response.split("===æ€è€ƒè¿‡ç¨‹å¼€å§‹===")[0].strip()
            print("âœ… æ ¹æ®æ€è€ƒè¿‡ç¨‹æ ‡è®°æˆªå–æ‘˜è¦å†…å®¹")
        elif "æ€è€ƒè¿‡ç¨‹è®°å½•" in raw_response:
            # æ›´å®½æ¾çš„åŒ¹é…
            abstract_section = raw_response.split("æ€è€ƒè¿‡ç¨‹è®°å½•")[0].strip()
            print("âœ… æ ¹æ®æ€è€ƒè¿‡ç¨‹å…³é”®è¯æˆªå–æ‘˜è¦å†…å®¹")
        elif "---" in raw_response:
            # å¦‚æœæœ‰åˆ†éš”ç¬¦ï¼Œå–åˆ†éš”ç¬¦å‰çš„å†…å®¹
            abstract_section = raw_response.split("---")[0].strip()
            print("âœ… æ ¹æ®åˆ†éš”ç¬¦æˆªå–æ‘˜è¦å†…å®¹")
        else:
            # æœ€åçš„å…œåº•æ–¹æ¡ˆï¼Œä½¿ç”¨å…¨éƒ¨å†…å®¹
            abstract_section = raw_response
            print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•åˆ†éš”æ ‡è®°ï¼Œä½¿ç”¨å®Œæ•´å“åº”")
    
    # ä»æ‘˜è¦éƒ¨åˆ†æå–å…³é”®è¯
    keywords = []
    keywords_pattern = r"(?:\*\*å…³é”®è¯[:ï¼š]\*\*|å…³é”®è¯[:ï¼š])(.*?)(?=\n\n|\n#|\n\*\*|$)"
    keywords_match = re.search(keywords_pattern, abstract_section, re.DOTALL)
    
    if keywords_match:
        keywords_text = keywords_match.group(1).strip()
        # ç§»é™¤å¯èƒ½çš„markdownæ ¼å¼æ ‡è®°
        keywords_text = re.sub(r'\*\*', '', keywords_text)
        # åˆ†å‰²å…³é”®è¯
        keywords = [k.strip() for k in re.split(r"[,ï¼Œ;ï¼›ã€\s]+", keywords_text) if k.strip()]
        print(f"âœ… æå–åˆ° {len(keywords)} ä¸ªå…³é”®è¯")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°å…³é”®è¯")
    
    # æ¸…ç†æ‘˜è¦æ–‡æœ¬ï¼Œç§»é™¤å…³é”®è¯éƒ¨åˆ†
    abstract_text = re.sub(r"(?:\*\*å…³é”®è¯[:ï¼š]\*\*|å…³é”®è¯[:ï¼š]).*?(?=\n\n|\n#|$)", "", abstract_section, flags=re.DOTALL)
    
    # æ¸…ç†å¤šä½™çš„ç©ºè¡Œå’Œé¦–å°¾ç©ºç™½
    abstract_text = re.sub(r'\n\s*\n\s*\n', '\n\n', abstract_text)  # åˆå¹¶å¤šä¸ªç©ºè¡Œ
    abstract_text = abstract_text.strip()
    
    # ç¡®ä¿æ‘˜è¦æœ‰åˆé€‚çš„æ ‡é¢˜æ ¼å¼
    if not abstract_text.startswith("#"):
        # æ²¡æœ‰ä»»ä½•æ ‡é¢˜ï¼Œæ·»åŠ æ ‡é¢˜
        abstract_text = "# æ‘˜è¦\n\n" + abstract_text
    elif not re.match(r"^#\s*æ‘˜è¦", abstract_text):
        # æœ‰æ ‡é¢˜ä½†ä¸æ˜¯"æ‘˜è¦"æ ‡é¢˜ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒæ•´
        if abstract_text.startswith("##"):
            # å¦‚æœæ˜¯äºŒçº§æ ‡é¢˜ï¼Œæ”¹ä¸ºä¸€çº§æ ‡é¢˜
            abstract_text = re.sub(r"^##\s*", "# ", abstract_text)
    # å¦‚æœå·²ç»æ˜¯æ­£ç¡®çš„"# æ‘˜è¦"æ ¼å¼ï¼Œä¿æŒåŸæ ·
        
    return abstract_text, keywords


def _deduplicate_keywords(keywords: List[str], outline: Dict, context: Dict = None) -> List[str]:
    """
    å»é™¤ä¸topicå’Œsubtopicsé‡å¤çš„å…³é”®è¯
    
    Args:
        keywords: åŸå§‹å…³é”®è¯åˆ—è¡¨
        outline: åŒ…å«topicä¿¡æ¯çš„å¤§çº²å­—å…¸
        context: åŒ…å«subtopicsç­‰ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å­—å…¸ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        å»é‡åçš„å…³é”®è¯åˆ—è¡¨
    """
    if not keywords:
        return keywords
    
    # ğŸ”§ æ·»åŠ ç±»å‹æ£€æŸ¥é˜²æŠ¤æªæ–½
    if not isinstance(outline, dict):
        print(f"âš ï¸ _deduplicate_keywords: outlineå‚æ•°ç±»å‹å¼‚å¸¸ï¼ŒæœŸæœ›dictï¼Œå®é™…{type(outline)}ï¼Œè·³è¿‡å»é‡")
        return keywords
    
    # è·å–éœ€è¦å»é‡çš„è¯æ±‡
    duplicate_terms = set()
    
    # ä»outlineä¸­è·å–ä¸»é¢˜å’Œå­ä¸»é¢˜
    topic = outline.get("topic", "")
    if topic:
        # å°†topicæ‹†åˆ†ä¸ºå•è¯å¹¶æ·»åŠ åˆ°å»é‡é›†åˆ
        topic_words = re.split(r'[,ï¼Œ;ï¼›ã€\s\-_]+', topic.lower())
        duplicate_terms.update([word.strip() for word in topic_words if word.strip()])
        # ä¹Ÿæ·»åŠ å®Œæ•´çš„topic
        duplicate_terms.add(topic.lower().strip())
    
    # å¤„ç†subtopicsï¼ˆä¼˜å…ˆä»contextä¸­è·å–ï¼Œå†ä»outlineä¸­è·å–ï¼‰
    subtopics = []
    if context:
        subtopics = context.get("subtopics", [])
        # å¦‚æœcontextä¸­æœ‰main_topicï¼Œä¹Ÿå¯ä»¥ä½œä¸ºè¡¥å……
        main_topic = context.get("main_topic", "")
        if main_topic and main_topic != topic:
            subtopics.append(main_topic)
    
    # å¦‚æœcontextä¸­æ²¡æœ‰æ‰¾åˆ°ï¼Œä»outlineä¸­æŸ¥æ‰¾
    if not subtopics:
        subtopics = outline.get("subtopics", [])
        if not subtopics:
            # å°è¯•å…¶ä»–å¯èƒ½çš„å­—æ®µå
            subtopics = outline.get("sub_topics", [])
    
    if subtopics:
        for subtopic in subtopics:
            if subtopic:
                # å°†æ¯ä¸ªsubtopicæ‹†åˆ†ä¸ºå•è¯å¹¶æ·»åŠ åˆ°å»é‡é›†åˆ
                subtopic_words = re.split(r'[,ï¼Œ;ï¼›ã€\s\-_]+', subtopic.lower())
                duplicate_terms.update([word.strip() for word in subtopic_words if word.strip()])
                # ä¹Ÿæ·»åŠ å®Œæ•´çš„subtopic
                duplicate_terms.add(subtopic.lower().strip())
    
    # ä»ç« èŠ‚æ ‡é¢˜ä¸­æå–å¯èƒ½é‡å¤çš„è¯æ±‡
    chapters = outline.get("chapters", [])
    if isinstance(chapters, list):
        for chapter in chapters:
            chapter_title = chapter.get("title", "")
            if chapter_title:
                # æå–ç« èŠ‚æ ‡é¢˜ä¸­çš„å…³é”®è¯
                title_words = re.split(r'[,ï¼Œ;ï¼›ã€\s\-_]+', chapter_title.lower())
                duplicate_terms.update([word.strip() for word in title_words if word.strip() and len(word.strip()) > 2])
    
    # æ·»åŠ ä¸€äº›é€šç”¨çš„å­¦æœ¯è¯æ±‡åˆ°å»é‡åˆ—è¡¨
    common_academic_terms = {
        "ç ”ç©¶", "æ–¹æ³•", "æŠ€æœ¯", "ç³»ç»Ÿ", "æ¨¡å‹", "ç®—æ³•", "åº”ç”¨", "åˆ†æ", "è®¾è®¡", "å®ç°",
        "research", "method", "technique", "system", "model", "algorithm", "application", 
        "analysis", "design", "implementation", "approach", "framework", "study", "evaluation"
    }
    duplicate_terms.update(common_academic_terms)
    
    # è¿›è¡Œå»é‡å¤„ç†
    deduplicated_keywords = []
    for keyword in keywords:
        keyword_clean = keyword.strip()
        if not keyword_clean:
            continue
            
        # æ£€æŸ¥æ˜¯å¦ä¸å»é‡è¯æ±‡é‡å¤ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        keyword_lower = keyword_clean.lower()
        
        # æ£€æŸ¥å®Œå…¨åŒ¹é…
        if keyword_lower in duplicate_terms:
            continue
            
        # æ£€æŸ¥æ˜¯å¦ä¸ºå»é‡è¯æ±‡çš„å­å­—ç¬¦ä¸²æˆ–çˆ¶å­—ç¬¦ä¸²
        is_duplicate = False
        for term in duplicate_terms:
            if len(term) < 2:  # è·³è¿‡å¤ªçŸ­çš„è¯
                continue
            # å¦‚æœå…³é”®è¯åŒ…å«åœ¨å»é‡è¯æ±‡ä¸­ï¼Œæˆ–å»é‡è¯æ±‡åŒ…å«åœ¨å…³é”®è¯ä¸­
            if (term in keyword_lower and len(term) > len(keyword_lower) * 0.7) or \
               (keyword_lower in term and len(keyword_lower) > len(term) * 0.7):
                is_duplicate = True
                break
        
        if not is_duplicate:
            deduplicated_keywords.append(keyword_clean)
    
    # è®°å½•å»é‡æƒ…å†µ
    removed_count = len(keywords) - len(deduplicated_keywords)
    if removed_count > 0:
        removed_keywords = [kw for kw in keywords if kw.strip().lower() not in [dk.lower() for dk in deduplicated_keywords]]
        print(f"ğŸ§¹ å…³é”®è¯å»é‡: ç§»é™¤äº† {removed_count} ä¸ªé‡å¤å…³é”®è¯")
        print(f"   ç§»é™¤çš„å…³é”®è¯: {', '.join(removed_keywords[:5])}{'...' if len(removed_keywords) > 5 else ''}")
    
    return deduplicated_keywords


def  parse_full_enrichment(response_text: str, outline: Dict) -> Dict:
    """è§£æLLMè¿”å›çš„å®Œæ•´å¤§çº²ä¸°å¯Œå†…å®¹ï¼Œé€‚åº”æ›´æ–°åçš„è¾“å‡ºæ ¼å¼"""
    # ğŸ”§ æ·»åŠ è¾“å…¥å‚æ•°ç±»å‹æ£€æŸ¥
    if not isinstance(response_text, str):
        print(f"âŒ parse_full_enrichment: response_textå‚æ•°ç±»å‹å¼‚å¸¸ï¼ŒæœŸæœ›strï¼Œå®é™…{type(response_text)}")
        return {"topic": "æœªçŸ¥", "overview": "æ— æ¦‚è¿°", "chapters": {}}
    
    if not isinstance(outline, dict):
        print(f"âŒ parse_full_enrichment: outlineå‚æ•°ç±»å‹å¼‚å¸¸ï¼ŒæœŸæœ›dictï¼Œå®é™…{type(outline)}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        outline = {"topic": "æœªçŸ¥", "overview": "æ— æ¦‚è¿°", "chapters": {}}
    
    # ğŸ”§ ä¿®æ”¹ï¼šä»åŸå§‹å¤§çº²ä¸­ä¿ç•™ topic å’Œ overview ä¿¡æ¯
    enrichment = {
        "topic": outline.get("topic", "æœªçŸ¥"),
        "overview": outline.get("overview", "æ— æ¦‚è¿°"),
        "chapters": {}
    }
    
    # ğŸ†• æ›´å®½æ¾çš„å†…å®¹æå–é€»è¾‘ - æ”¯æŒå¤šç§æ ¼å¼
    content_text = None
    
    # æ–¹æ¡ˆ1: å°è¯•æ ‡å‡†çš„å†…å®¹è§„åˆ’æ ‡è®°
    content_match = re.search(r"===å†…å®¹è§„åˆ’å¼€å§‹===(.*?)===å†…å®¹è§„åˆ’ç»“æŸ===", 
                            response_text, re.DOTALL)
    if content_match:
        content_text = content_match.group(1).strip()
        print("âœ… æˆåŠŸæå–å®Œæ•´çš„æ ‡å‡†å†…å®¹è§„åˆ’")
    
    # æ–¹æ¡ˆ2: å°è¯•ä¼˜åŒ–ç»“æœæ ‡è®°æ ¼å¼
    if not content_text:
        # æŸ¥æ‰¾===ä¼˜åŒ–ç»“æœå¼€å§‹===åˆ°===å†…å®¹è§„åˆ’ç»“æŸ===ä¹‹é—´çš„å†…å®¹
        opt_match = re.search(r"===ä¼˜åŒ–ç»“æœå¼€å§‹===(.*?)===å†…å®¹è§„åˆ’ç»“æŸ===", 
                            response_text, re.DOTALL)
        if opt_match:
            content_text = opt_match.group(1).strip()
            print("âœ… æˆåŠŸæå–ä¼˜åŒ–ç»“æœæ ¼å¼çš„å†…å®¹è§„åˆ’")
    
    # æ–¹æ¡ˆ3: å°è¯•ä»ã€ç« èŠ‚å†…å®¹æŒ‡å¼•ã€‘å¼€å§‹åˆ°===å†…å®¹è§„åˆ’ç»“æŸ===
    if not content_text:
        guide_match = re.search(r"ã€ç« èŠ‚å†…å®¹æŒ‡å¼•ã€‘(.*?)===å†…å®¹è§„åˆ’ç»“æŸ===", 
                              response_text, re.DOTALL)
        if guide_match:
            content_text = guide_match.group(1).strip()
            print("âœ… æˆåŠŸä»ç« èŠ‚å†…å®¹æŒ‡å¼•å¼€å§‹æå–")
    
    # æ–¹æ¡ˆ4: ä»ç¬¬ä¸€ä¸ªç« èŠ‚æ ‡é¢˜å¼€å§‹åˆ°ä»»æ„ç»“æŸæ ‡è®°
    if not content_text:
        chapter_to_end = re.search(r"(# ç¬¬\d+ç« .*?)(?:===.*?ç»“æŸ===|ã€æ˜¯å¦ç»§ç»­è¿­ä»£ã€‘|å†™ä½œæŒ‡å¯¼å®Œæ•´æ€§:|$)", 
                                 response_text, re.DOTALL)
        if chapter_to_end:
            content_text = chapter_to_end.group(1).strip()
            print("âœ… æˆåŠŸä»ç¬¬ä¸€ä¸ªç« èŠ‚æ ‡é¢˜å¼€å§‹æå–")
    
    # æ–¹æ¡ˆ5: æœ€å®½æ¾æ¨¡å¼ - åªè¦æ‰¾åˆ°ç« èŠ‚å†…å®¹å°±å°è¯•æå–
    if not content_text:
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„å¼€å§‹ä½ç½®
        start_patterns = [
            r"===å†…å®¹è§„åˆ’å¼€å§‹===",
            r"===ä¼˜åŒ–ç»“æœå¼€å§‹===",
            r"ã€ä¼˜åŒ–åä¸°å¯Œå¤§çº²ã€‘",
            r"ã€ç« èŠ‚å†…å®¹æŒ‡å¼•ã€‘",
            r"# ç¬¬\d+ç« "
        ]
        
        start_pos = -1
        start_pattern_found = ""
        
        for pattern in start_patterns:
            match = re.search(pattern, response_text)
            if match:
                start_pos = match.start()
                start_pattern_found = pattern
                break
        
        if start_pos >= 0:
            # æŸ¥æ‰¾å¯èƒ½çš„ç»“æŸä½ç½®
            remaining_text = response_text[start_pos:]
            end_patterns = [
                r"===å†…å®¹è§„åˆ’ç»“æŸ===",
                r"===ä¼˜åŒ–ç»“æœç»“æŸ===",
                r"ã€æ˜¯å¦ç»§ç»­è¿­ä»£ã€‘",
                r"å†™ä½œæŒ‡å¯¼å®Œæ•´æ€§:",
                r"ç»¼åˆè´¨é‡è¯„åˆ†:"
            ]
            
            end_pos = len(remaining_text)  # é»˜è®¤åˆ°æ–‡æœ¬ç»“å°¾
            
            for pattern in end_patterns:
                match = re.search(pattern, remaining_text)
                if match:
                    end_pos = match.start()
                    break
            
            # å¦‚æœä»ä¼˜åŒ–ç»“æœå¼€å§‹ï¼Œå°è¯•å»æ‰å‰é¢çš„æ ‡è®°
            extracted_text = remaining_text[:end_pos].strip()
            if start_pattern_found in [r"===ä¼˜åŒ–ç»“æœå¼€å§‹===", r"ã€ä¼˜åŒ–åä¸°å¯Œå¤§çº²ã€‘"]:
                # æŸ¥æ‰¾å®é™…ç« èŠ‚å†…å®¹å¼€å§‹ä½ç½®
                chapter_start = re.search(r"# ç¬¬\d+ç« ", extracted_text)
                if chapter_start:
                    content_text = extracted_text[chapter_start.start():].strip()
                else:
                    content_text = extracted_text
            else:
                content_text = extracted_text
            
            print(f"âœ… å®½æ¾æ¨¡å¼æˆåŠŸæå–ï¼Œä» {start_pattern_found} å¼€å§‹")
    
    # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æå–åˆ°å†…å®¹
    if not content_text:
        print("âŒ æ‰€æœ‰è§£ææ–¹æ¡ˆéƒ½å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å†…å®¹è¿›è¡Œç« èŠ‚åŒ¹é…")
        content_text = response_text
        
    # æ¸…ç†æå–çš„å†…å®¹
    if content_text:
        # ç§»é™¤å¯èƒ½çš„æ€è€ƒè¿‡ç¨‹éƒ¨åˆ†
        if "ã€æ€è€ƒè¿‡ç¨‹è®°å½•ã€‘" in content_text:
            content_text = content_text.split("ã€æ€è€ƒè¿‡ç¨‹è®°å½•ã€‘")[0].strip()
        
        # ç§»é™¤å¯èƒ½æ®‹ç•™çš„æ ‡è®°
        content_text = re.sub(r"^(===.*?===|ã€.*?ã€‘)\s*", "", content_text, flags=re.MULTILINE)
        content_text = content_text.strip()
    
    # ğŸ†• æ›´å®½æ¾çš„ç« èŠ‚åŒ¹é…é€»è¾‘
    chapter_matches = []
    
    # æ–¹æ¡ˆ1: æ ‡å‡†çš„ç« èŠ‚æ¨¡å¼
    standard_pattern = r"#\s*ç¬¬(\d+)ç« [ï¼š:]\s*(.+?)\s*\n(.*?)(?=\n#\s*ç¬¬\d+ç« [ï¼š:]|\Z)"
    chapter_matches = re.findall(standard_pattern, content_text, re.DOTALL)
    
    if chapter_matches:
        print(f"âœ… æ ‡å‡†æ¨¡å¼æ‰¾åˆ° {len(chapter_matches)} ä¸ªç« èŠ‚")
    else:
        # æ–¹æ¡ˆ2: æ›´å®½æ¾çš„ç« èŠ‚æ¨¡å¼ï¼ˆæ”¯æŒä¸åŒçš„åˆ†éš”ç¬¦ï¼‰
        loose_patterns = [
            r"#\s*ç¬¬(\d+)ç« [ï¼š:ï¼š]\s*(.+?)\s*\n(.*?)(?=\n#\s*ç¬¬\d+ç« |\Z)",  # æ”¯æŒä¸­æ–‡å†’å·
            r"#\s*(\d+)[\.ã€\s]*(.+?)\s*\n(.*?)(?=\n#\s*\d+|\Z)",  # æ•°å­—å¼€å¤´
            r"#\s*ç¬¬(\d+)ç« \s*(.+?)\s*\n(.*?)(?=\n#|\Z)",  # æ— å†’å·
        ]
        
        for i, pattern in enumerate(loose_patterns):
            chapter_matches = re.findall(pattern, content_text, re.DOTALL)
            if chapter_matches:
                print(f"âœ… å®½æ¾æ¨¡å¼{i+1}æ‰¾åˆ° {len(chapter_matches)} ä¸ªç« èŠ‚")
                break
    
    # æ–¹æ¡ˆ3: å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œå°è¯•æœ€å®½æ¾çš„åŒ¹é…
    if not chapter_matches:
        print("ğŸ”„ ä½¿ç”¨æœ€å®½æ¾çš„åŒ¹é…æ¨¡å¼...")
        # æŸ¥æ‰¾æ‰€æœ‰ä»¥#å¼€å¤´çš„è¡Œä½œä¸ºç« èŠ‚æ ‡é¢˜
        lines = content_text.split('\n')
        chapter_lines = []
        
        for i, line in enumerate(lines):
            if re.match(r'^\s*#\s*', line) and ('ç« ' in line or 'ç¬¬' in line):
                chapter_lines.append((i, line.strip()))
        
        print(f"ğŸ” æ‰¾åˆ° {len(chapter_lines)} ä¸ªå¯èƒ½çš„ç« èŠ‚æ ‡é¢˜è¡Œ")
        
        # æ„å»ºç« èŠ‚åŒ¹é…
        for j, (line_idx, title_line) in enumerate(chapter_lines):
            # æå–ç« èŠ‚å·å’Œæ ‡é¢˜
            title_match = re.search(r'#\s*(?:ç¬¬)?(\d+)ç« ?[ï¼š:ï¼š]?\s*(.+)', title_line)
            if title_match:
                chapter_num = title_match.group(1)
                chapter_title = title_match.group(2).strip()
            else:
                # å¤‡ç”¨æå–
                chapter_num = str(j + 1)
                chapter_title = re.sub(r'^\s*#+\s*', '', title_line)
            
            # æå–ç« èŠ‚å†…å®¹ï¼ˆä»å½“å‰è¡Œåˆ°ä¸‹ä¸€ä¸ªç« èŠ‚æ ‡é¢˜è¡Œï¼‰
            start_line = line_idx + 1
            if j + 1 < len(chapter_lines):
                end_line = chapter_lines[j + 1][0]
            else:
                end_line = len(lines)
            
            chapter_content = '\n'.join(lines[start_line:end_line]).strip()
            
            if chapter_content:  # åªæœ‰å½“æœ‰å†…å®¹æ—¶æ‰æ·»åŠ 
                chapter_matches.append((chapter_num, chapter_title, chapter_content))
        
        print(f"âœ… æœ€å®½æ¾æ¨¡å¼æ„å»ºäº† {len(chapter_matches)} ä¸ªç« èŠ‚")
    
    # å¦‚æœå®Œå…¨æ²¡æœ‰åŒ¹é…åˆ°ï¼Œç»™å‡ºè¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯ï¼ˆä½†æ›´ç®€æ´ï¼‰
    if not chapter_matches:
        print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç« èŠ‚å†…å®¹ï¼Œæ˜¾ç¤ºå‰300å­—ç¬¦ç”¨äºè°ƒè¯•:")
        print(content_text[:300] + "..." if len(content_text) > 300 else content_text)
        print("âš ï¸ æŸ¥æ‰¾æ‰€æœ‰ # å¼€å¤´çš„è¡Œ:")
        lines = content_text.split('\n')
        hash_lines = [line for line in lines[:10] if line.strip().startswith('#')]
        for line in hash_lines:
            print(f"  æ‰¾åˆ°: {line.strip()}")
    else:
        print(f"ğŸ“Š æœ€ç»ˆæˆåŠŸè§£æ {len(chapter_matches)} ä¸ªç« èŠ‚")
    
    # å¤„ç†æ¯ä¸ªç« èŠ‚
    for chapter_num, chapter_title, chapter_content in chapter_matches:
        chapter_id = chapter_num
        
        # åˆå§‹åŒ–ç« èŠ‚æ•°æ®
        chapter_data = {
            "id": chapter_id,
            "title": chapter_title,
            "content_guide": "",
            "keywords": [],
            "research_focus": [],
            "subsections": {}
        }
        
        # æå–ç« èŠ‚å†…å®¹æŒ‡å¼• - é€‚åº”æ–°æ ¼å¼ "ç« èŠ‚å†…å®¹æŒ‡å¼•:"
        content_guide_match = re.search(r"ç« èŠ‚å†…å®¹æŒ‡å¼•[ï¼š:](.*?)(?=\n##|\n#|\Z)", 
                                    chapter_content, re.DOTALL)
        if not content_guide_match:
            # å°è¯•æ—§æ ¼å¼
            content_guide_match = re.search(r"##\s*ç« èŠ‚å†…å®¹æŒ‡å¼•\s*\n(.*?)(?=\n##|$)", 
                                    chapter_content, re.DOTALL)
        
        if content_guide_match:
            chapter_data["content_guide"] = content_guide_match.group(1).strip()
        
        # æå–ç« èŠ‚å…³é”®è¯
        keywords_match = re.search(r"##\s*æœ¬ç« èŠ‚å…³é”®è¯.*?\n(.*?)(?=\n##|\n###|$)", 
                                chapter_content, re.DOTALL)
        if keywords_match:
            keywords_text = keywords_match.group(1).strip()
            # æå–å¼•å·ä¸­çš„å…³é”®è¯æˆ–ç”¨é€—å·åˆ†éš”çš„å…³é”®è¯
            keywords = re.findall(r'"([^"]+)"', keywords_text)
            if not keywords:
                keywords = [k.strip() for k in re.split(r'[,ï¼Œ;ï¼›ã€\s]+', keywords_text) if k.strip()]
            
            # å»é‡å¤„ç†ï¼šç§»é™¤ä¸topicå’Œsubtopicsé‡å¤çš„å…³é”®è¯
            # æ³¨æ„ï¼šcontextå‚æ•°æœªä¼ é€’ï¼Œsubtopicså°†ä»outlineä¸­è·å–ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            keywords = _deduplicate_keywords(keywords, outline)
            chapter_data["keywords"] = keywords
        
        # æå–é‡ç‚¹ç ”ç©¶é¢†åŸŸ
        research_focus_match = re.search(r"##\s*é‡ç‚¹ç ”ç©¶é¢†åŸŸ\s*\n(.*?)(?=\n##|\n###|$)", 
                                    chapter_content, re.DOTALL)
        if research_focus_match:
            research_text = research_focus_match.group(1).strip()
            research_points = []
            for line in research_text.split("\n"):
                line = line.strip()
                if line and re.match(r"^\d+\.", line):
                    research_points.append(re.sub(r"^\d+\.\s*", "", line))
            chapter_data["research_focus"] = research_points
        
        # ä½¿ç”¨æ›´ç²¾ç¡®çš„å­ç« èŠ‚åŒ¹é…æ¨¡å¼ - ä¿®å¤ç‰ˆæœ¬
        # è¿™ä¸ªæ¨¡å¼ä¼šæ­£ç¡®åŒ¹é…åˆ°ä¸‹ä¸€ä¸ªå­ç« èŠ‚çš„å¼€å§‹ï¼Œæˆ–è€…åˆ°å†…å®¹ç»“æŸ
        subsection_pattern = r"###\s*(\d+\.\d+)\s+(.+?)\s*\n(.*?)(?=\n###\s*\d+\.\d+|\n#\s*ç¬¬\d+ç« |\Z)"
        subsection_matches = re.findall(subsection_pattern, chapter_content, re.DOTALL)
        
        for subsection_id, subsection_title, subsection_content in subsection_matches:
            # åˆå§‹åŒ–å­ç« èŠ‚æ•°æ®ç»“æ„
            subsection_data = {
                "id": subsection_id,
                "title": subsection_title,
                "content_guide": "",
                "key_points": [],
                "writing_guide": ""
            }
            
            # å¤„ç†å­ç« èŠ‚ - è°ƒè¯•ä¿¡æ¯å·²ç§»é™¤ï¼Œä¿æŒç®€æ´è¾“å‡º
            
            # æå–å†…å®¹æ¦‚è¦ - é€‚åº”æ–°æ ¼å¼ "å†…å®¹æ¦‚è¦:"
            content_overview = re.search(r"å†…å®¹æ¦‚è¦[ï¼š:](.*?)(?=\n####|\n###|\Z)", 
                                    subsection_content, re.DOTALL)
            if not content_overview:
                # å°è¯•æ—§æ ¼å¼
                content_overview = re.search(r"####\s*å†…å®¹æ¦‚è¦\s*\n(.*?)(?=\n####|$)", 
                                    subsection_content, re.DOTALL)
            
            if content_overview:
                subsection_data["content_guide"] = content_overview.group(1).strip()
            
            # æå–å…³é”®è¦ç‚¹ - ä¿®å¤ç‰ˆæœ¬ï¼Œæ”¯æŒå¤šç§æ ¼å¼
            key_points_match = re.search(r"####\s*å…³é”®è¦ç‚¹\s*\n(.*?)(?=\n####|\n###|\Z)", 
                                        subsection_content, re.DOTALL)
            
            if key_points_match:
                key_points_text = key_points_match.group(1).strip()
                points = []
                
                for line in key_points_text.split("\n"):
                    line = line.strip()
                    if not line:
                        continue
                    
                    # åŒ¹é…ç¼–å·æ ¼å¼ï¼š1. 2. 3. ç­‰
                    if re.match(r"^\d+\.\s+", line):
                        # ç§»é™¤ç¼–å·å‰ç¼€ï¼Œä¿ç•™å†…å®¹
                        clean_point = re.sub(r"^\d+\.\s+", "", line)
                        if clean_point:
                            points.append(clean_point)
                    # åŒ¹é…å…¶ä»–ç¼–å·æ ¼å¼ï¼š-, *, â€¢ ç­‰
                    elif re.match(r"^[-*â€¢]\s+", line):
                        clean_point = re.sub(r"^[-*â€¢]\s+", "", line)
                        if clean_point:
                            points.append(clean_point)
                
                subsection_data["key_points"] = points
            else:
                print(f"âš ï¸ å­ç« èŠ‚ {subsection_id} æœªæ‰¾åˆ°å…³é”®è¦ç‚¹")
            
            # æå–å†™ä½œå»ºè®® - ä¿®å¤ç‰ˆæœ¬ï¼Œæ”¯æŒå¤šç§æ ¼å¼  
            writing_guide_match = re.search(r"####\s*å†™ä½œå»ºè®®\s*\n(.*?)(?=\n####|\n###|\Z)", 
                                            subsection_content, re.DOTALL)
            
            if writing_guide_match:
                writing_guide_text = writing_guide_match.group(1).strip()
                
                # æ¸…ç†æ–‡æœ¬ï¼Œä¿æŒæ®µè½ç»“æ„ä½†ç§»é™¤å¤šä½™æ¢è¡Œ
                writing_guide_text = re.sub(r'\n+', ' ', writing_guide_text)
                writing_guide_text = re.sub(r'\s+', ' ', writing_guide_text)
                
                subsection_data["writing_guide"] = writing_guide_text
            else:
                print(f"âš ï¸ å­ç« èŠ‚ {subsection_id} æœªæ‰¾åˆ°å†™ä½œå»ºè®®")
            
            # ğŸ†• ç®€åŒ–çš„è°ƒè¯•ä¿¡æ¯
            total_items = len(subsection_data["key_points"]) + (1 if subsection_data["writing_guide"] else 0)
            if total_items == 0:
                print(f"âš ï¸ å­ç« èŠ‚ {subsection_id} è§£æä¸å®Œæ•´ (è¦ç‚¹:{len(subsection_data['key_points'])}, æŒ‡å¯¼:{'æœ‰' if subsection_data['writing_guide'] else 'æ— '})")
            else:
                pass
            # å°†å­ç« èŠ‚æ•°æ®æ·»åŠ åˆ°ç« èŠ‚ä¸­
            chapter_data["subsections"][subsection_id] = subsection_data
        
        # å°†ç« èŠ‚æ•°æ®æ·»åŠ åˆ°ç»“æœä¸­
        enrichment["chapters"][chapter_id] = chapter_data
    
    # ä¸ºäº†æ›´å¥½çš„åˆ†é…ç»™å†™ä½œæ™ºèƒ½ä½“ï¼Œç¡®ä¿æ‰€æœ‰ç« èŠ‚IDéƒ½æœ‰å¯¹åº”çš„å†…å®¹
    original_chapters = outline.get("chapters", [])
    
    # ğŸ”§ ä¿®å¤ï¼šå¤„ç†chapterså¯èƒ½æ˜¯å­—å…¸æˆ–åˆ—è¡¨çš„æƒ…å†µ
    if isinstance(original_chapters, dict):
        # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
        chapter_list = list(original_chapters.values())
        print(f"ğŸ”§ æ£€æµ‹åˆ°chaptersæ˜¯å­—å…¸æ ¼å¼ï¼ŒåŒ…å« {len(chapter_list)} ä¸ªç« èŠ‚")
    elif isinstance(original_chapters, list):
        # å¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
        chapter_list = original_chapters
    else:
        # å¦‚æœæ—¢ä¸æ˜¯å­—å…¸ä¹Ÿä¸æ˜¯åˆ—è¡¨ï¼Œä½¿ç”¨ç©ºåˆ—è¡¨
        chapter_list = []
        print(f"âš ï¸ chaptersæ ¼å¼å¼‚å¸¸ï¼Œç±»å‹: {type(original_chapters)}ï¼Œä½¿ç”¨ç©ºåˆ—è¡¨")
    
    for chapter in chapter_list:
        # ğŸ”§ æ·»åŠ ç±»å‹æ£€æŸ¥
        if not isinstance(chapter, dict):
            print(f"âš ï¸ è·³è¿‡éå­—å…¸ç±»å‹çš„ç« èŠ‚: {type(chapter)} - {str(chapter)[:50]}")
            continue
            
        chapter_id = chapter.get("id", "")
        if chapter_id and chapter_id not in enrichment["chapters"]:
            print(f"âš ï¸ ä¸ºæœªè§£æçš„ç« èŠ‚ {chapter_id} æ·»åŠ é»˜è®¤ç»“æ„")
            # ä¸ºæœªè§£æåˆ°çš„ç« èŠ‚æ·»åŠ ä¸€ä¸ªç©ºç»“æ„
            enrichment["chapters"][chapter_id] = {
                "id": chapter_id,
                "title": chapter.get("title", ""),
                "content_guide": "è¯·æ ¹æ®å¤§çº²å†…å®¹æ’°å†™æœ¬ç« èŠ‚ã€‚",
                "research_focus": [],
                "keywords": [],
                "subsections": {}
            }
            
            # æ·»åŠ å­ç« èŠ‚
            subsections = chapter.get("subsections", [])
            # ğŸ”§ åŒæ ·å¤„ç†subsectionså¯èƒ½æ˜¯å­—å…¸çš„æƒ…å†µ
            if isinstance(subsections, dict):
                subsections = list(subsections.values())
            elif not isinstance(subsections, list):
                subsections = []
                
            for subsection in subsections:
                if not isinstance(subsection, dict):
                    continue
                subsection_id = subsection.get("id", "")
                if subsection_id:
                    enrichment["chapters"][chapter_id]["subsections"][subsection_id] = {
                        "id": subsection_id,
                        "title": subsection.get("title", ""),
                        "content_guide": "è¯·æ ¹æ®å¤§çº²å†…å®¹æ’°å†™æœ¬å°èŠ‚ã€‚",
                        "key_points": [],
                        "writing_guide": ""
                    }
    
    # æœ€ç»ˆè§£æç»“æœç»Ÿè®¡
    parsed_chapters = len(enrichment["chapters"])
    expected_chapters = len(original_chapters)
    
    if parsed_chapters > 0:
        # ç»Ÿè®¡å­ç« èŠ‚æ•°é‡
        total_subsections = sum(len(ch.get("subsections", {})) for ch in enrichment["chapters"].values())
        print(f"ğŸ“Š å­ç« èŠ‚ç»Ÿè®¡: å…± {total_subsections} ä¸ªå­ç« èŠ‚")
        
    return enrichment

def create_numbered_materials_mapping(materials: List[Dict], section_info: Dict) -> Dict:
    """
    ä¸ºææ–™åˆ›å»ºå¸¦ç¼–å·çš„æ˜ å°„è¡¨ï¼Œç±»ä¼¼llm_review_generatorçš„æ–¹å¼
    
    è¯´æ˜ï¼šæ ¹æ®database_setupçš„å¤„ç†é€»è¾‘ï¼Œä¸åŒç±»å‹çš„ææ–™éœ€è¦åŒ…å«ä¸åŒçš„å…ƒæ•°æ®å­—æ®µï¼š
    - æ–‡æœ¬ææ–™ï¼šåŸºç¡€å­—æ®µ + text_level
    - å…¬å¼ææ–™ï¼šåŸºç¡€å­—æ®µ + equation_text, text_format, context_before/after
    - å›¾ç‰‡ææ–™ï¼šåŸºç¡€å­—æ®µ + img_path, img_caption, img_footnote, modalityç­‰
    - è¡¨æ ¼ææ–™ï¼šåŸºç¡€å­—æ®µ + img_path, table_caption, table_footnote, has_table_bodyç­‰
    """
    
    numbered_materials = {}
    
    # æŒ‰ç±»å‹åˆ†ç»„ææ–™
    text_materials = []
    equation_materials = []
    figure_materials = []
    table_materials = []

    chapter_id = section_info.get("id", "")
    
    # ğŸ” è¯¦ç»†åˆ†ææ¯ä¸ªææ–™çš„ç±»å‹
    type_analysis = {}
    
    for i, material in enumerate(materials):
        # æ£€æŸ¥å¤šä¸ªå¯èƒ½çš„content_typeå­—æ®µ
        content_type_from_metadata = material.get("metadata", {}).get("content_type", None)
        content_type_direct = material.get("content_type", None)
        type_field = material.get("type", None)
        
        # ä¼˜å…ˆçº§ï¼šmetadata.content_type > content_type > type > default
        if content_type_from_metadata:
            content_type = content_type_from_metadata
            type_source = "metadata.content_type"
        elif content_type_direct:
            content_type = content_type_direct
            type_source = "content_type"
        elif type_field:
            content_type = type_field
            type_source = "type"
        else:
            content_type = "text"
            type_source = "default"
        
        # è®°å½•ç±»å‹åˆ†æ
        type_key = f"{content_type}({type_source})"
        type_analysis[type_key] = type_analysis.get(type_key, 0) + 1
        
        
        if content_type in ["text", "texts"]:
            text_materials.append(material)
        elif content_type in ["equation", "equations"]:
            equation_materials.append(material)
        elif content_type in ["image", "images", "figure", "figures", "image_text"]:  # âœ… åŒ…å«äº†image_text
            figure_materials.append(material)
        elif content_type in ["table", "tables", "table_text", "table_image"]:        # âœ… åŒ…å«äº†table_text
            table_materials.append(material)
        else:
            text_materials.append(material)
        
    # ä¸ºæ¯ç§ç±»å‹çš„ææ–™åˆ†é…ç¼–å·
    material_counter = 1
    
    # æ–‡æœ¬ææ–™ [æ–‡æœ¬1], [æ–‡æœ¬2], ...
    for i, material in enumerate(text_materials, 1):
        temp_id = f"{chapter_id}-æ–‡æœ¬{i}"
        metadata = material.get("metadata", {})
        
        numbered_materials[temp_id] = {
            "type": "text",
            "global_id": material_counter,
            "content": material.get("content", ""),
            "paper": material.get("paper", "æœªçŸ¥æ¥æº"),
            "page": metadata.get("page_idx", "-1"),
            "relevance_score": material.get("relevance_score", 0.0),
            "material_id": material.get("id", f"material_{material_counter}"),
            # æ–‡æœ¬ç‰¹æœ‰å­—æ®µ
            "text_level": metadata.get("text_level", 0),
            "order_in_paper": metadata.get("order_in_paper", -1),
            "original_data": metadata.get("original_data", "")
        }
        material_counter += 1
    
    # å…¬å¼ææ–™ [å…¬å¼1], [å…¬å¼2], ...
    for i, material in enumerate(equation_materials, 1):
        temp_id = f"{chapter_id}-å…¬å¼{i}"
        metadata = material.get("metadata", {})
        
        numbered_materials[temp_id] = {
            "type": "equation",
            "global_id": material_counter,
            "content": material.get("content", ""),
            "paper": material.get("paper", "æœªçŸ¥æ¥æº"),
            "page": metadata.get("page_idx", "-1"),
            "relevance_score": material.get("relevance_score", 0.0),
            "material_id": material.get("id", f"material_{material_counter}"),
            # å…¬å¼ç‰¹æœ‰å­—æ®µ
            "equation_text": metadata.get("equation_text", material.get("content", "")),
            "text_format": metadata.get("text_format", "unknown"),
            "context_before": metadata.get("context_before", ""),
            "context_after": metadata.get("context_after", ""),
            "has_context": metadata.get("has_context", False),
            "order_in_paper": metadata.get("order_in_paper", -1),
            "original_data": metadata.get("original_data", "")
        }
        material_counter += 1
    
    # å›¾ç‰‡ææ–™ [å›¾ç‰‡1], [å›¾ç‰‡2], ...
    for i, material in enumerate(figure_materials, 1):
        temp_id = f"{chapter_id}-å›¾ç‰‡{i}"
        metadata = material.get("metadata", {})
        
        numbered_materials[temp_id] = {
            "type": "figure",
            "global_id": material_counter,
            "content": material.get("content", ""),
            "paper": material.get("paper", "æœªçŸ¥æ¥æº"),
            "page": metadata.get("page_idx", "-1"),
            "relevance_score": material.get("relevance_score", 0.0),
            "material_id": material.get("id", f"material_{material_counter}"),
            # å›¾ç‰‡ç‰¹æœ‰å­—æ®µ âœ… æ·»åŠ äº†ç¼ºå¤±çš„å…³é”®ä¿¡æ¯
            "img_path": metadata.get("img_path", ""),  # å›¾ç‰‡è·¯å¾„ - å…³é”®ä¿¡æ¯
            "img_caption": metadata.get("img_caption", ""),
            "img_footnote": metadata.get("img_footnote", ""),
            "modality": metadata.get("modality", "image"),  # image æˆ– text
            "related_image_id": metadata.get("related_image_id", ""),
            "reference_texts": metadata.get("reference_texts", ""),
            "has_references": metadata.get("has_references", False),
            "search_key_used": metadata.get("search_key_used", ""),
            "order_in_paper": metadata.get("order_in_paper", -1),
            "original_data": metadata.get("original_data", "")
        }
        material_counter += 1
    
    # è¡¨æ ¼ææ–™ [è¡¨æ ¼1], [è¡¨æ ¼2], ...
    for i, material in enumerate(table_materials, 1):
        temp_id = f"{chapter_id}-è¡¨æ ¼{i}"
        metadata = material.get("metadata", {})
        
        numbered_materials[temp_id] = {
            "type": "table",
            "global_id": material_counter,
            "content": material.get("content", ""),
            "paper": material.get("paper", "æœªçŸ¥æ¥æº"),
            "page": metadata.get("page_idx", "-1"),
            "relevance_score": material.get("relevance_score", 0.0),
            "material_id": material.get("id", f"material_{material_counter}"),
            # è¡¨æ ¼ç‰¹æœ‰å­—æ®µ âœ… æ·»åŠ äº†ç¼ºå¤±çš„å…³é”®ä¿¡æ¯
            "img_path": metadata.get("img_path", ""),  # è¡¨æ ¼å›¾ç‰‡è·¯å¾„ - å…³é”®ä¿¡æ¯
            "table_caption": metadata.get("table_caption", ""),
            "table_footnote": metadata.get("table_footnote", ""),
            "has_table_body": metadata.get("has_table_body", False),
            "has_table_image": metadata.get("has_table_image", False),
            "modality": metadata.get("modality", "text"),  # image æˆ– text
            "reference_texts": metadata.get("reference_texts", ""),
            "has_references": metadata.get("has_references", False),
            "search_key_used": metadata.get("search_key_used", ""),
            "order_in_paper": metadata.get("order_in_paper", -1),
            "original_data": metadata.get("original_data", "")
        }
        material_counter += 1
    
    return numbered_materials


def extract_citation_mapping(content: str, numbered_materials: Dict) -> Dict:
    """
    ä»ç”Ÿæˆçš„å†…å®¹ä¸­æå–å¼•ç”¨æ˜ å°„
    
    è¯´æ˜ï¼šæ ¹æ®ä¸åŒææ–™ç±»å‹ï¼Œå®Œæ•´ä¿ç•™æ‰€æœ‰ç‰¹æœ‰å­—æ®µä¿¡æ¯ï¼Œç¡®ä¿æœ€ç»ˆJSONæ–‡ä»¶åŒ…å«å®Œæ•´ä¿¡æ¯
    - å›¾ç‰‡ææ–™ï¼šåŒ…å«img_path, img_caption, modalityç­‰å›¾ç‰‡ç‰¹æœ‰ä¿¡æ¯
    - è¡¨æ ¼ææ–™ï¼šåŒ…å«table_caption, has_table_body, has_table_imageç­‰è¡¨æ ¼ç‰¹æœ‰ä¿¡æ¯  
    - å…¬å¼ææ–™ï¼šåŒ…å«equation_text, text_format, context_before/afterç­‰å…¬å¼ç‰¹æœ‰ä¿¡æ¯
    - æ–‡æœ¬ææ–™ï¼šåŒ…å«text_level, order_in_paperç­‰æ–‡æœ¬ç‰¹æœ‰ä¿¡æ¯
    """
    import re
    
    citation_mapping = {}
    
    # æŸ¥æ‰¾æ‰€æœ‰çš„å¼•ç”¨æ ‡è¯†ç¬¦ï¼Œå¦‚[æ–‡æœ¬1], [å…¬å¼2], [å›¾ç‰‡1], [è¡¨æ ¼1]ç­‰
    # å…ˆæ‰¾åˆ°æ‰€æœ‰æ–¹æ‹¬å·å†…å®¹ï¼Œç„¶ååˆ†å‰²å¤„ç†è¿ç»­å¼•ç”¨
    pattern = r'\[([^\]]+)\]'
    bracket_contents = re.findall(pattern, content)
    
    # æ”¶é›†æ‰€æœ‰å•ç‹¬çš„å¼•ç”¨
    all_citations = []
    for bracket_content in bracket_contents:
        # åˆ†å‰²è¿ç»­å¼•ç”¨ï¼š[6-æ–‡æœ¬11, 6-å…¬å¼6] -> ['6-æ–‡æœ¬11', '6-å…¬å¼6']
        # å¤„ç†é€—å·åˆ†éš”çš„å¼•ç”¨
        individual_citations = [cite.strip() for cite in bracket_content.split(',')]
        all_citations.extend(individual_citations)
    
    # å»é‡ä½†ä¿æŒé¡ºåº
    unique_citations = []
    for citation in all_citations:
        if citation and citation not in unique_citations:
            unique_citations.append(citation)
    
    for citation in unique_citations:
        # æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬çš„ææ–™æ ‡è¯†ç¬¦
        if citation in numbered_materials:
            material_info = numbered_materials[citation]
            material_type = material_info["type"]
            
            # åŸºç¡€ä¿¡æ¯ï¼ˆæ‰€æœ‰ç±»å‹éƒ½æœ‰ï¼‰
            citation_data = {
                "material_id": material_info["material_id"],
                "type": material_type,
                "paper": material_info["paper"],
                "page": material_info["page"],
                "content_preview": material_info["content"][:200] + "..." if len(material_info["content"]) > 200 else material_info["content"],
                "relevance_score": material_info["relevance_score"],
                "full_content": material_info["content"],
                "original_data": material_info["original_data"]
            }
            
            # æ ¹æ®ææ–™ç±»å‹æ·»åŠ ç‰¹æœ‰å­—æ®µ âœ…
            if material_type == "figure":
                # å›¾ç‰‡ææ–™ç‰¹æœ‰å­—æ®µ
                citation_data.update({
                    "img_path": material_info.get("img_path", ""),           # å›¾ç‰‡è·¯å¾„ - å…³é”®ä¿¡æ¯
                    "img_caption": material_info.get("img_caption", ""),
                    "img_footnote": material_info.get("img_footnote", ""),
                    "modality": material_info.get("modality", "image"),
                    "related_image_id": material_info.get("related_image_id", ""),
                    "reference_texts": material_info.get("reference_texts", ""),
                    "has_references": material_info.get("has_references", False),
                    "search_key_used": material_info.get("search_key_used", ""),
                    "order_in_paper": material_info.get("order_in_paper", -1)
                })
                
            elif material_type == "table":
                # è¡¨æ ¼ææ–™ç‰¹æœ‰å­—æ®µ
                citation_data.update({
                    "img_path": material_info.get("img_path", ""),           # è¡¨æ ¼å›¾ç‰‡è·¯å¾„ - å…³é”®ä¿¡æ¯
                    "table_caption": material_info.get("table_caption", ""),
                    "table_footnote": material_info.get("table_footnote", ""),
                    "has_table_body": material_info.get("has_table_body", False),
                    "has_table_image": material_info.get("has_table_image", False),
                    "modality": material_info.get("modality", "text"),
                    "reference_texts": material_info.get("reference_texts", ""),
                    "has_references": material_info.get("has_references", False),
                    "search_key_used": material_info.get("search_key_used", ""),
                    "order_in_paper": material_info.get("order_in_paper", -1)
                })
                
            elif material_type == "equation":
                # å…¬å¼ææ–™ç‰¹æœ‰å­—æ®µ
                citation_data.update({
                    "equation_text": material_info.get("equation_text", ""),
                    "text_format": material_info.get("text_format", "unknown"),
                    "context_before": material_info.get("context_before", ""),
                    "context_after": material_info.get("context_after", ""),
                    "has_context": material_info.get("has_context", False),
                    "order_in_paper": material_info.get("order_in_paper", -1)
                })
                
            elif material_type == "text":
                # æ–‡æœ¬ææ–™ç‰¹æœ‰å­—æ®µ
                citation_data.update({
                    "text_level": material_info.get("text_level", 0),
                    "order_in_paper": material_info.get("order_in_paper", -1)
                })
            
            citation_mapping[citation] = citation_data
        
    # ç»Ÿè®¡å„ç±»å‹å¼•ç”¨æ•°é‡
    type_counts = {}
    for citation_data in citation_mapping.values():
        material_type = citation_data["type"]
        type_counts[material_type] = type_counts.get(material_type, 0) + 1
    
    if type_counts:
        type_summary = ", ".join([f"{t}:{c}ä¸ª" for t, c in type_counts.items()])
        print(f"ğŸ“Š å¼•ç”¨ç±»å‹åˆ†å¸ƒ: {type_summary}")
    
    return citation_mapping


def write_section_citations(section_info: Dict, citation_mapping: Dict, numbered_materials: Dict, topic: str = "ç»¼è¿°") -> str:
    """
    å°†ç« èŠ‚å¼•ç”¨ä¿¡æ¯å†™å…¥ç»Ÿä¸€çš„JSONæ–‡ä»¶
    
    è¯´æ˜ï¼šé‡‡ç”¨ç»Ÿä¸€æ–‡ä»¶ç®¡ç†ç­–ç•¥ï¼Œæ‰€æœ‰writerçš„å¼•ç”¨ä¿¡æ¯éƒ½å†™å…¥åŒä¸€ä¸ªJSONæ–‡ä»¶
    - æ–‡ä»¶å‘½åæ ¼å¼ï¼š{topic}_citations_{timestamp}.json
    - æ”¯æŒå¤šä¸ªç« èŠ‚çš„å¼•ç”¨ä¿¡æ¯ç´¯ç§¯
    - ç¡®ä¿æ‰€æœ‰åŸå§‹ä¿¡æ¯å®Œæ•´ä¿å­˜ï¼ŒåŒ…æ‹¬img_pathç­‰å…³é”®å­—æ®µ
    - æ”¯æŒå¹¶å‘å†™å…¥ä¿æŠ¤
    """
    import json
    import os
    import threading
    from datetime import datetime
    from pathlib import Path
    
    # åˆ›å»ºå¼•ç”¨ç›®å½•
    citations_dir = "./chapter_citations"
    os.makedirs(citations_dir, exist_ok=True)
    
    # æŸ¥æ‰¾æˆ–ç”Ÿæˆç»Ÿä¸€æ–‡ä»¶åï¼š{topic}_citations_{timestamp}.json
    date_str = datetime.now().strftime("%Y%m%d")
    time_str = datetime.now().strftime("%H%M%S")
    
    # æ¸…ç†ä¸»é¢˜åç§°ä¸­çš„ç‰¹æ®Šå­—ç¬¦
    safe_topic = "".join(c for c in topic if c.isalnum() or c in [' ', '_', '-']).rstrip()
    safe_topic = safe_topic.replace(' ', '_') if safe_topic else "ç»¼è¿°"
    
    # ğŸ†• æ ¹æ®ç« èŠ‚IDåˆ¤æ–­æ˜¯å¦ä¸ºç¬¬ä¸€ä¸ªwriter
    section_id = section_info.get("id", "")
    is_first_writer = section_id in ["1", "1.1", "01", "001"] or section_id.startswith("1.")
    
    import glob
    pattern = f"{safe_topic}_citations_*.json"
    pattern_path = os.path.join(citations_dir, pattern)
    existing_files = glob.glob(pattern_path)
    
    if is_first_writer:
        # ç¬¬ä¸€ä¸ªwriterï¼šç›´æ¥åˆ›å»ºæ–°æ–‡ä»¶ï¼ˆå³ä½¿å·²æœ‰æ–‡ä»¶ä¹Ÿåˆ›å»ºæ–°çš„ï¼Œé¿å…å†²çªï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{safe_topic}_citations_{timestamp}.json"
        filepath = os.path.join(citations_dir, filename)
        print(f"ğŸ“„ ç¬¬ä¸€ä¸ªwriteråˆ›å»ºæ–°å¼•ç”¨æ–‡ä»¶: {filename}")
    else:
        # åç»­writerï¼šæŸ¥æ‰¾å¹¶ä½¿ç”¨å·²å­˜åœ¨çš„æ–‡ä»¶
        if existing_files:
            # ä½¿ç”¨å·²å­˜åœ¨çš„æ–‡ä»¶ï¼ˆå¦‚æœæœ‰å¤šä¸ªï¼Œä½¿ç”¨æœ€æ–°çš„ï¼‰
            filepath = max(existing_files, key=os.path.getmtime)
            print(f"ğŸ“„ åç»­writerä½¿ç”¨å·²å­˜åœ¨çš„å¼•ç”¨æ–‡ä»¶: {os.path.basename(filepath)}")
        else:
            # å¦‚æœæ˜¯åç»­writerä½†æ‰¾ä¸åˆ°æ–‡ä»¶ï¼Œå¯èƒ½ç¬¬ä¸€ä¸ªwriterè¿˜æ²¡æ‰§è¡Œ
            print(f"âš ï¸ ç« èŠ‚{section_id}æœªæ‰¾åˆ°å·²å­˜åœ¨çš„å¼•ç”¨æ–‡ä»¶ï¼Œç¬¬ä¸€ä¸ªwriterå¯èƒ½è¿˜æœªæ‰§è¡Œ")
            # ç­‰å¾…ä¸€æ®µæ—¶é—´å†é‡è¯•
            import time
            time.sleep(2)
            existing_files = glob.glob(pattern_path)
            if existing_files:
                filepath = max(existing_files, key=os.path.getmtime)
                print(f"ğŸ“„ é‡è¯•åæ‰¾åˆ°å¼•ç”¨æ–‡ä»¶: {os.path.basename(filepath)}")
            else:
                # ä»ç„¶æ‰¾ä¸åˆ°ï¼Œåˆ›å»ºä¸´æ—¶æ–‡ä»¶ï¼ˆä½†ä½¿ç”¨ç‰¹æ®Šæ ‡è®°ï¼‰
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"{safe_topic}_citations_temp_{section_id}_{timestamp}.json"
                filepath = os.path.join(citations_dir, filename)
                print(f"âš ï¸ åˆ›å»ºä¸´æ—¶å¼•ç”¨æ–‡ä»¶: {filename} ï¼ˆå»ºè®®æ£€æŸ¥writeræ‰§è¡Œé¡ºåºï¼‰")
    
    # ä½¿ç”¨æ–‡ä»¶é”ç¡®ä¿å¹¶å‘å†™å…¥å®‰å…¨
    lock_file = f"{filepath}.lock"
    
    # æ„å»ºå½“å‰ç« èŠ‚çš„å¼•ç”¨æ•°æ®
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    section_data = {
        "section_info": {
            "id": section_info.get("id", ""),
            "title": section_info.get("title", ""),
            "description": section_info.get("description", ""),
            "content_guide": section_info.get("content_guide", ""),
            "subsections": section_info.get("subsections", []),
            "timestamp": timestamp,
            "write_time": time_str
        },
        "citation_summary": {
            "total_materials_available": len(numbered_materials),
            "total_citations_used": len(citation_mapping),
            "usage_rate": round(len(citation_mapping) / len(numbered_materials), 3) if numbered_materials else 0,
            "citations_by_type": {}
        },
        "detailed_citations": citation_mapping,  # âœ… åŒ…å«æ‰€æœ‰ç±»å‹ç‰¹æœ‰å­—æ®µçš„å®Œæ•´å¼•ç”¨ä¿¡æ¯
        "available_materials": numbered_materials,  # âœ… åŒ…å«æ‰€æœ‰åŸå§‹ææ–™ä¿¡æ¯
        "material_statistics": {
            "by_type": {},
            "by_paper": {},
            "by_relevance": []
        }
    }
    
    # ç»Ÿè®¡å¼•ç”¨ç±»å‹
    for citation_id, citation_info in citation_mapping.items():
        material_type = citation_info["type"]
        section_data["citation_summary"]["citations_by_type"][material_type] = \
            section_data["citation_summary"]["citations_by_type"].get(material_type, 0) + 1
    
    # ç»Ÿè®¡ææ–™ç±»å‹
    for material_id, material_info in numbered_materials.items():
        material_type = material_info["type"]
        section_data["material_statistics"]["by_type"][material_type] = \
            section_data["material_statistics"]["by_type"].get(material_type, 0) + 1
        
        # ç»Ÿè®¡è®ºæ–‡æ¥æº
        paper = material_info["paper"]
        section_data["material_statistics"]["by_paper"][paper] = \
            section_data["material_statistics"]["by_paper"].get(paper, 0) + 1
    
    # æŒ‰ç›¸å…³åº¦æ’åº
    relevance_list = []
    for material_id, material_info in numbered_materials.items():
        relevance_list.append({
            "material_id": material_id,
            "relevance_score": material_info["relevance_score"],
            "used": material_id in citation_mapping,
            "type": material_info["type"],
            "paper": material_info["paper"]
        })
    
    section_data["material_statistics"]["by_relevance"] = sorted(
        relevance_list, key=lambda x: x["relevance_score"], reverse=True
    )
    
    # çº¿ç¨‹å®‰å…¨çš„æ–‡ä»¶å†™å…¥
    try:
        # ç®€å•çš„æ–‡ä»¶é”æœºåˆ¶
        while os.path.exists(lock_file):
            import time
            time.sleep(0.1)
        
        # åˆ›å»ºé”æ–‡ä»¶
        with open(lock_file, 'w') as f:
            f.write(str(os.getpid()))
        
        # è¯»å–ç°æœ‰æ•°æ®æˆ–åˆ›å»ºæ–°çš„æ•°æ®ç»“æ„
        if os.path.exists(filepath):
            with open(filepath, 'r', encoding='utf-8') as f:
                try:
                    citation_data = json.load(f)
                except json.JSONDecodeError:
                    print(f"âš ï¸ JSONæ–‡ä»¶æŸåï¼Œåˆ›å»ºæ–°æ–‡ä»¶: {filepath}")
                    citation_data = {"meta": {}, "sections": {}}
        else:
            citation_data = {
                "meta": {
                    "topic": topic,
                    "created_date": date_str,
                    "last_updated": timestamp,
                    "total_sections": 0,
                    "file_version": "2.0"  # æ ‡è®°ä¸ºæ–°ç‰ˆæœ¬æ ¼å¼
                },
                "sections": {}
            }
        
        # æ›´æ–°å…ƒä¿¡æ¯
        citation_data["meta"]["last_updated"] = timestamp
        citation_data["meta"]["total_sections"] = len(citation_data["sections"]) + 1
        
        # æ·»åŠ å½“å‰ç« èŠ‚æ•°æ®
        section_key = f"section_{section_info.get('id', 'unknown')}"
        citation_data["sections"][section_key] = section_data
        
        # å†™å…¥æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(citation_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ ç« èŠ‚å¼•ç”¨ä¿¡æ¯å·²å†™å…¥: {filepath}")
        print(f"ğŸ“Š å½“å‰æ–‡ä»¶åŒ…å« {citation_data['meta']['total_sections']} ä¸ªç« èŠ‚çš„å¼•ç”¨ä¿¡æ¯")
        
    finally:
        # æ¸…ç†é”æ–‡ä»¶
        if os.path.exists(lock_file):
            os.remove(lock_file)
    
    return filepath


def generate_bibliography_from_citations(topic: str = "ç»¼è¿°") -> str:
    """
    ä»ç»Ÿä¸€çš„å¼•ç”¨JSONæ–‡ä»¶ä¸­ç”Ÿæˆå‚è€ƒæ–‡çŒ®åˆ—è¡¨
    
    è¯´æ˜ï¼šè¯»å–write_section_citationsä¿å­˜çš„JSONæ–‡ä»¶ï¼Œæå–æ‰€æœ‰æ–‡æœ¬ç±»å‹å¼•ç”¨ï¼Œ
    æŒ‰è®ºæ–‡åˆ†ç»„æ•´ç†ï¼Œç”Ÿæˆæ ¼å¼åŒ–çš„å‚è€ƒæ–‡çŒ®åˆ—è¡¨
    
    Args:
        topic: ä¸»é¢˜åç§°ï¼Œç”¨äºå®šä½å¯¹åº”çš„å¼•ç”¨æ–‡ä»¶
        
    Returns:
        æ ¼å¼åŒ–çš„å‚è€ƒæ–‡çŒ®å­—ç¬¦ä¸²
    """
    import json
    import os
    from datetime import datetime
    from collections import defaultdict
    
    # ğŸ†• æŸ¥æ‰¾å·²å­˜åœ¨çš„å¼•ç”¨æ–‡ä»¶
    citations_dir = "./chapter_citations"
    safe_topic = "".join(c for c in topic if c.isalnum() or c in [' ', '_', '-']).rstrip()
    safe_topic = safe_topic.replace(' ', '_') if safe_topic else "ç»¼è¿°"
    
    # ğŸ†• æ™ºèƒ½æŸ¥æ‰¾å¼•ç”¨æ–‡ä»¶ï¼ˆä¼˜å…ˆä½¿ç”¨æ­£å¼æ–‡ä»¶ï¼Œé¿å…ä¸´æ—¶æ–‡ä»¶ï¼‰
    import glob
    
    # å…ˆæŸ¥æ‰¾æ­£å¼çš„å¼•ç”¨æ–‡ä»¶ï¼ˆä¸åŒ…å«tempæ ‡è®°ï¼‰
    formal_pattern = f"{safe_topic}_citations_[0-9]*_[0-9]*.json"
    formal_path = os.path.join(citations_dir, formal_pattern)
    formal_files = glob.glob(formal_path)
    
    # æŸ¥æ‰¾æ‰€æœ‰å¼•ç”¨æ–‡ä»¶ï¼ˆåŒ…æ‹¬ä¸´æ—¶æ–‡ä»¶ï¼‰
    all_pattern = f"{safe_topic}_citations*.json"
    all_path = os.path.join(citations_dir, all_pattern)
    all_files = glob.glob(all_path)
    
    if formal_files:
        # ä¼˜å…ˆä½¿ç”¨æ­£å¼æ–‡ä»¶
        filepath = max(formal_files, key=os.path.getmtime)
        print(f"ğŸ“š ä½¿ç”¨æ­£å¼å¼•ç”¨æ–‡ä»¶: {os.path.basename(filepath)}")
    elif all_files:
        # å¦‚æœæ²¡æœ‰æ­£å¼æ–‡ä»¶ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
        temp_files = [f for f in all_files if 'temp' in f]
        if temp_files:
            filepath = max(temp_files, key=os.path.getmtime)
            print(f"ğŸ“š ä½¿ç”¨ä¸´æ—¶å¼•ç”¨æ–‡ä»¶: {os.path.basename(filepath)} ï¼ˆå»ºè®®ç­‰å¾…æ­£å¼æ–‡ä»¶ç”Ÿæˆï¼‰")
        else:
            filepath = max(all_files, key=os.path.getmtime)
            print(f"ğŸ“š ä½¿ç”¨å¼•ç”¨æ–‡ä»¶: {os.path.basename(filepath)}")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å¼•ç”¨æ–‡ä»¶: {safe_topic}_citations*.json")
        return "# å‚è€ƒæ–‡çŒ®\n\næ— å¼•ç”¨æ–‡çŒ®ã€‚\n"
    
    try:
        # è¯»å–å¼•ç”¨æ•°æ®
        with open(filepath, 'r', encoding='utf-8') as f:
            citation_data = json.load(f)
        
        # æ”¶é›†æ‰€æœ‰æ–‡æœ¬ç±»å‹çš„å¼•ç”¨
        text_citations = {}  # {citation_key: citation_info}
        
        # éå†æ‰€æœ‰ç« èŠ‚çš„è¯¦ç»†å¼•ç”¨
        sections = citation_data.get("sections", {})
        for section_key, section_data in sections.items():
            detailed_citations = section_data.get("detailed_citations", {})
            
            for citation_key, citation_info in detailed_citations.items():
                if citation_info.get("type") == "text":
                    text_citations[citation_key] = citation_info
        
        if not text_citations:
            print("ğŸ“š æœªæ‰¾åˆ°æ–‡æœ¬ç±»å‹å¼•ç”¨")
            return "# å‚è€ƒæ–‡çŒ®\n\næ— å¼•ç”¨æ–‡çŒ®ã€‚\n"
        
        # æŒ‰è®ºæ–‡åˆ†ç»„æ•´ç†å¼•ç”¨
        papers_citations = defaultdict(list)  # {paper_name: [citation_info_list]}
        
        for citation_key, citation_info in text_citations.items():
            paper_name = citation_info.get("paper", "æœªçŸ¥æ¥æº")
            page = citation_info.get("page", "-1")
            
            # æ„å»ºå¼•ç”¨æ¡ç›®ä¿¡æ¯
            citation_entry = {
                "citation_key": citation_key,  # å¦‚ "1-æ–‡æœ¬2"
                "page": page,
                "relevance_score": citation_info.get("relevance_score", 0.0)
            }
            
            papers_citations[paper_name].append(citation_entry)
        
        # ç”Ÿæˆæ ¼å¼åŒ–çš„å‚è€ƒæ–‡çŒ®
        bibliography = "# å‚è€ƒæ–‡çŒ®\n\n"
        
        # æŒ‰è®ºæ–‡åç§°æ’åº
        sorted_papers = sorted(papers_citations.keys())
        
        for paper_name in sorted_papers:
            citations = papers_citations[paper_name]
            
            # æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åºå¼•ç”¨
            citations.sort(key=lambda x: x["relevance_score"], reverse=True)
            
            # æ·»åŠ è®ºæ–‡åç§°
            bibliography += f"**{paper_name}**\n"
            
            # ç”Ÿæˆç›¸å…³æ–‡æœ¬åˆ—è¡¨
            citation_list = []
            for i, citation in enumerate(citations, 1):
                citation_key = citation["citation_key"]
                page = citation["page"]
                page_text = f"ç¬¬{page}é¡µ" 
                citation_list.append(f"{i}. ç« èŠ‚{citation_key}ï¼Œ{page_text}")
            
            # æ·»åŠ ç›¸å…³æ–‡æœ¬
            bibliography += f"ï¼ˆç›¸å…³æ–‡æœ¬ï¼‰{'; '.join(citation_list)}\n\n"
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        total_papers = len(sorted_papers)
        total_citations = len(text_citations)
        bibliography += f"*æ€»è®¡ï¼š{total_papers} ç¯‡è®ºæ–‡ï¼Œ{total_citations} ä¸ªæ–‡æœ¬å¼•ç”¨ï¼Œé¡µç ä¸º-1è¡¨æ˜PDFè§£æé—®é¢˜ï¼Œè¯·å‚è€ƒå¼•ç”¨JSON*\n"
        
        print(f"ğŸ“š å‚è€ƒæ–‡çŒ®ç”Ÿæˆå®Œæˆ: {total_papers} ç¯‡è®ºæ–‡ï¼Œ{total_citations} ä¸ªå¼•ç”¨")
        return bibliography
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå‚è€ƒæ–‡çŒ®æ—¶å‡ºé”™: {e}")
        return "# å‚è€ƒæ–‡çŒ®\n\nå‚è€ƒæ–‡çŒ®ç”Ÿæˆå¤±è´¥ã€‚\n"


def generate_equations_from_citations(topic: str = "ç»¼è¿°") -> str:
    """
    ä»ç»Ÿä¸€çš„å¼•ç”¨JSONæ–‡ä»¶ä¸­ç”Ÿæˆå‚è€ƒå…¬å¼åˆ—è¡¨
    
    è¯´æ˜ï¼šè¯»å–write_section_citationsä¿å­˜çš„JSONæ–‡ä»¶ï¼Œæå–æ‰€æœ‰å…¬å¼ç±»å‹å¼•ç”¨ï¼Œ
    æŒ‰è®ºæ–‡åˆ†ç»„æ•´ç†ï¼Œç”Ÿæˆæ ¼å¼åŒ–çš„å‚è€ƒå…¬å¼åˆ—è¡¨
    
    Args:
        topic: ä¸»é¢˜åç§°ï¼Œç”¨äºå®šä½å¯¹åº”çš„å¼•ç”¨æ–‡ä»¶
        
    Returns:
        æ ¼å¼åŒ–çš„å‚è€ƒå…¬å¼å­—ç¬¦ä¸²
    """
    import json
    import os
    from datetime import datetime
    from collections import defaultdict
    
    # ğŸ†• æŸ¥æ‰¾å·²å­˜åœ¨çš„å¼•ç”¨æ–‡ä»¶
    citations_dir = "./chapter_citations"
    safe_topic = "".join(c for c in topic if c.isalnum() or c in [' ', '_', '-']).rstrip()
    safe_topic = safe_topic.replace(' ', '_') if safe_topic else "ç»¼è¿°"
    
    # ğŸ†• æ™ºèƒ½æŸ¥æ‰¾å¼•ç”¨æ–‡ä»¶ï¼ˆä¼˜å…ˆä½¿ç”¨æ­£å¼æ–‡ä»¶ï¼Œé¿å…ä¸´æ—¶æ–‡ä»¶ï¼‰
    import glob
    
    # å…ˆæŸ¥æ‰¾æ­£å¼çš„å¼•ç”¨æ–‡ä»¶ï¼ˆä¸åŒ…å«tempæ ‡è®°ï¼‰
    formal_pattern = f"{safe_topic}_citations_[0-9]*_[0-9]*.json"
    formal_path = os.path.join(citations_dir, formal_pattern)
    formal_files = glob.glob(formal_path)
    
    # æŸ¥æ‰¾æ‰€æœ‰å¼•ç”¨æ–‡ä»¶ï¼ˆåŒ…æ‹¬ä¸´æ—¶æ–‡ä»¶ï¼‰
    all_pattern = f"{safe_topic}_citations*.json"
    all_path = os.path.join(citations_dir, all_pattern)
    all_files = glob.glob(all_path)
    
    if formal_files:
        # ä¼˜å…ˆä½¿ç”¨æ­£å¼æ–‡ä»¶
        filepath = max(formal_files, key=os.path.getmtime)
        print(f"ğŸ”¢ ä½¿ç”¨æ­£å¼å¼•ç”¨æ–‡ä»¶ç”Ÿæˆå‚è€ƒå…¬å¼: {os.path.basename(filepath)}")
    elif all_files:
        # å¦‚æœæ²¡æœ‰æ­£å¼æ–‡ä»¶ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
        temp_files = [f for f in all_files if 'temp' in f]
        if temp_files:
            filepath = max(temp_files, key=os.path.getmtime)
            print(f"ğŸ”¢ ä½¿ç”¨ä¸´æ—¶å¼•ç”¨æ–‡ä»¶ç”Ÿæˆå‚è€ƒå…¬å¼: {os.path.basename(filepath)} ï¼ˆå»ºè®®ç­‰å¾…æ­£å¼æ–‡ä»¶ç”Ÿæˆï¼‰")
        else:
            filepath = max(all_files, key=os.path.getmtime)
            print(f"ğŸ”¢ ä½¿ç”¨å¼•ç”¨æ–‡ä»¶ç”Ÿæˆå‚è€ƒå…¬å¼: {os.path.basename(filepath)}")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å¼•ç”¨æ–‡ä»¶: {safe_topic}_citations*.json")
        return "# å‚è€ƒå…¬å¼\n\næ— å¼•ç”¨å…¬å¼ã€‚\n"
    
    try:
        # è¯»å–å¼•ç”¨æ•°æ®
        with open(filepath, 'r', encoding='utf-8') as f:
            citation_data = json.load(f)
        
        # æ”¶é›†æ‰€æœ‰å…¬å¼ç±»å‹çš„å¼•ç”¨
        equation_citations = {}  # {citation_key: citation_info}
        
        # éå†æ‰€æœ‰ç« èŠ‚çš„è¯¦ç»†å¼•ç”¨
        sections = citation_data.get("sections", {})
        for section_key, section_data in sections.items():
            detailed_citations = section_data.get("detailed_citations", {})
            
            for citation_key, citation_info in detailed_citations.items():
                if citation_info.get("type") == "equation":
                    equation_citations[citation_key] = citation_info
        
        if not equation_citations:
            print("ğŸ”¢ æœªæ‰¾åˆ°å…¬å¼ç±»å‹å¼•ç”¨")
            return "# å‚è€ƒå…¬å¼\n\næ— å¼•ç”¨å…¬å¼ã€‚\n"
        
        # æŒ‰è®ºæ–‡åˆ†ç»„æ•´ç†å¼•ç”¨
        papers_equations = defaultdict(list)  # {paper_name: [equation_info_list]}
        
        for citation_key, citation_info in equation_citations.items():
            paper_name = citation_info.get("paper", "æœªçŸ¥æ¥æº")
            page = citation_info.get("page", "-1")
            
            # æ„å»ºå…¬å¼å¼•ç”¨æ¡ç›®ä¿¡æ¯
            equation_entry = {
                "citation_key": citation_key,  # å¦‚ "1-å…¬å¼1"
                "page": page,
                "equation_text": citation_info.get("equation_text", ""),
                "context_before": citation_info.get("context_before", ""),
                "context_after": citation_info.get("context_after", ""),
                "text_format": citation_info.get("text_format", "latex"),
                "relevance_score": citation_info.get("relevance_score", 0.0)
            }
            
            papers_equations[paper_name].append(equation_entry)
        
        # ç”Ÿæˆæ ¼å¼åŒ–çš„å‚è€ƒå…¬å¼
        equations_section = "# å‚è€ƒå…¬å¼\n\n"
        
        # æŒ‰è®ºæ–‡åç§°æ’åº
        sorted_papers = sorted(papers_equations.keys())
        
        for paper_name in sorted_papers:
            equations = papers_equations[paper_name]
            
            # æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åºå…¬å¼
            equations.sort(key=lambda x: x["relevance_score"], reverse=True)
            
            # æ·»åŠ è®ºæ–‡åç§°
            equations_section += f"**{paper_name}(2025)**\n"
            
            # ç”Ÿæˆå…¬å¼åˆ—è¡¨
            for i, equation in enumerate(equations, 1):
                citation_key = equation["citation_key"]
                page = equation["page"]
                page_text = f"ç¬¬{page}é¡µ"
                
                # æ·»åŠ åŸºæœ¬ä¿¡æ¯è¡Œ
                equations_section += f"{i}. ç« èŠ‚{citation_key}ï¼Œ{page_text}\n"
                
                # æ·»åŠ å‰æ–‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if equation["context_before"]:
                    equations_section += f"å‰æ–‡ï¼š{equation['context_before']}\n"
                
                # æ·»åŠ å…¬å¼
                equation_text = equation["equation_text"]
                if equation_text:
                    equations_section += f"å…¬å¼ï¼š\n{equation_text}\n"
                
                # æ·»åŠ åæ–‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if equation["context_after"]:
                    equations_section += f"åæ–‡ï¼š{equation['context_after']}\n"
                
                # æ·»åŠ æ ¼å¼ä¿¡æ¯
                text_format = equation["text_format"]
                equations_section += f"æ ¼å¼ï¼š{text_format}\n\n"
            
            equations_section += "\n"
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        total_papers = len(sorted_papers)
        total_equations = len(equation_citations)
        equations_section += f"*æ€»è®¡ï¼š{total_papers} ç¯‡è®ºæ–‡ï¼Œ{total_equations} ä¸ªå…¬å¼å¼•ç”¨ï¼Œé¡µç ä¸º-1è¡¨æ˜PDFè§£æé—®é¢˜ï¼Œè¯·å‚è€ƒå¼•ç”¨JSON*\n"
        
        print(f"ğŸ”¢ å‚è€ƒå…¬å¼ç”Ÿæˆå®Œæˆ: {total_papers} ç¯‡è®ºæ–‡ï¼Œ{total_equations} ä¸ªå…¬å¼å¼•ç”¨")
        return equations_section
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå‚è€ƒå…¬å¼æ—¶å‡ºé”™: {e}")
        return "# å‚è€ƒå…¬å¼\n\nå‚è€ƒå…¬å¼ç”Ÿæˆå¤±è´¥ã€‚\n"


def generate_figures_from_citations(topic: str = "ç»¼è¿°") -> str:
    """
    ä»ç»Ÿä¸€çš„å¼•ç”¨JSONæ–‡ä»¶ä¸­ç”Ÿæˆå‚è€ƒå›¾ç‰‡åˆ—è¡¨
    
    è¯´æ˜ï¼šè¯»å–write_section_citationsä¿å­˜çš„JSONæ–‡ä»¶ï¼Œæå–æ‰€æœ‰å›¾ç‰‡ç±»å‹å¼•ç”¨ï¼Œ
    æŒ‰è®ºæ–‡åˆ†ç»„æ•´ç†ï¼Œç”Ÿæˆæ ¼å¼åŒ–çš„å‚è€ƒå›¾ç‰‡åˆ—è¡¨ï¼ŒåŒ…å«å›¾ç‰‡è·¯å¾„å’Œé¡µç ä¿¡æ¯
    
    è¾“å‡ºæ ¼å¼ï¼š
    **(è®ºæ–‡å)**
    1. ç« èŠ‚{chapter_ID}-å›¾ç‰‡NUMï¼Œç¬¬{page}é¡µï¼Œç« èŠ‚{chapter_ID}-å›¾ç‰‡NUMï¼Œç¬¬{page}é¡µ
    ({full_content})
    ![ç« èŠ‚{chapter_ID}](root_image_path/paper_name/img_path)
    
    Args:
        topic: ä¸»é¢˜åç§°ï¼Œç”¨äºå®šä½å¯¹åº”çš„å¼•ç”¨æ–‡ä»¶
        
    Returns:
        æ ¼å¼åŒ–çš„å‚è€ƒå›¾ç‰‡å­—ç¬¦ä¸²ï¼ŒåŒ…å«é¡µç ä¿¡æ¯å’Œæ­£ç¡®çš„å›¾ç‰‡è·¯å¾„ç»“æ„
    """
    import json
    import os
    from datetime import datetime
    from collections import defaultdict
    
    # è®¾ç½®æ ¹å›¾ç‰‡è·¯å¾„
    root_image_path = r"D:\Desktop\ZJU\download\dl3\direct_crawler\results"
    
    # æŸ¥æ‰¾å·²å­˜åœ¨çš„å¼•ç”¨æ–‡ä»¶
    citations_dir = "./chapter_citations"
    safe_topic = "".join(c for c in topic if c.isalnum() or c in [' ', '_', '-']).rstrip()
    safe_topic = safe_topic.replace(' ', '_') if safe_topic else "ç»¼è¿°"
    
    # æ™ºèƒ½æŸ¥æ‰¾å¼•ç”¨æ–‡ä»¶ï¼ˆä¼˜å…ˆä½¿ç”¨æ­£å¼æ–‡ä»¶ï¼Œé¿å…ä¸´æ—¶æ–‡ä»¶ï¼‰
    import glob
    
    # å…ˆæŸ¥æ‰¾æ­£å¼çš„å¼•ç”¨æ–‡ä»¶ï¼ˆä¸åŒ…å«tempæ ‡è®°ï¼‰
    formal_pattern = f"{safe_topic}_citations_[0-9]*_[0-9]*.json"
    formal_path = os.path.join(citations_dir, formal_pattern)
    formal_files = glob.glob(formal_path)
    
    # æŸ¥æ‰¾æ‰€æœ‰å¼•ç”¨æ–‡ä»¶ï¼ˆåŒ…æ‹¬ä¸´æ—¶æ–‡ä»¶ï¼‰
    all_pattern = f"{safe_topic}_citations*.json"
    all_path = os.path.join(citations_dir, all_pattern)
    all_files = glob.glob(all_path)
    
    if formal_files:
        # ä¼˜å…ˆä½¿ç”¨æ­£å¼æ–‡ä»¶
        filepath = max(formal_files, key=os.path.getmtime)
        print(f"ğŸ–¼ï¸ ä½¿ç”¨æ­£å¼å¼•ç”¨æ–‡ä»¶ç”Ÿæˆå‚è€ƒå›¾ç‰‡: {os.path.basename(filepath)}")
    elif all_files:
        # å¦‚æœæ²¡æœ‰æ­£å¼æ–‡ä»¶ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
        temp_files = [f for f in all_files if 'temp' in f]
        if temp_files:
            filepath = max(temp_files, key=os.path.getmtime)
            print(f"ğŸ–¼ï¸ ä½¿ç”¨ä¸´æ—¶å¼•ç”¨æ–‡ä»¶ç”Ÿæˆå‚è€ƒå›¾ç‰‡: {os.path.basename(filepath)} ï¼ˆå»ºè®®ç­‰å¾…æ­£å¼æ–‡ä»¶ç”Ÿæˆï¼‰")
        else:
            filepath = max(all_files, key=os.path.getmtime)
            print(f"ğŸ–¼ï¸ ä½¿ç”¨å¼•ç”¨æ–‡ä»¶ç”Ÿæˆå‚è€ƒå›¾ç‰‡: {os.path.basename(filepath)}")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å¼•ç”¨æ–‡ä»¶: {safe_topic}_citations*.json")
        return "# å‚è€ƒå›¾ç‰‡\n\næ— å¼•ç”¨å›¾ç‰‡ã€‚\n"
    
    try:
        # è¯»å–å¼•ç”¨æ•°æ®
        with open(filepath, 'r', encoding='utf-8') as f:
            citation_data = json.load(f)
        
        # æ”¶é›†æ‰€æœ‰å›¾ç‰‡ç±»å‹çš„å¼•ç”¨
        figure_citations = {}  # {citation_key: citation_info}
        
        # éå†æ‰€æœ‰ç« èŠ‚çš„è¯¦ç»†å¼•ç”¨
        sections = citation_data.get("sections", {})
        for section_key, section_data in sections.items():
            detailed_citations = section_data.get("detailed_citations", {})
            
            for citation_key, citation_info in detailed_citations.items():
                if citation_info.get("type") == "figure":
                    figure_citations[citation_key] = citation_info
        
        if not figure_citations:
            print("ğŸ–¼ï¸ æœªæ‰¾åˆ°å›¾ç‰‡ç±»å‹å¼•ç”¨")
            return "# å‚è€ƒå›¾ç‰‡\n\næ— å¼•ç”¨å›¾ç‰‡ã€‚\n"
        
        # æŒ‰è®ºæ–‡åˆ†ç»„æ•´ç†å¼•ç”¨ï¼Œå¹¶æ ¹æ®full_contentå»é‡
        papers_figures = defaultdict(dict)  # {paper_name: {full_content: [citation_key_list]}}
        
        for citation_key, citation_info in figure_citations.items():
            paper_name = citation_info.get("paper", "æœªçŸ¥æ¥æº")
            full_content = citation_info.get("full_content", "")
            
            # å¦‚æœfull_contentä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºæ–°æ¡ç›®
            if full_content not in papers_figures[paper_name]:
                papers_figures[paper_name][full_content] = {
                    "citation_keys": [],
                    "info": citation_info  # ä¿å­˜å¼•ç”¨ä¿¡æ¯
                }
            
            # æ·»åŠ ç« èŠ‚å¼•ç”¨é”®
            papers_figures[paper_name][full_content]["citation_keys"].append(citation_key)
        
        # ç”Ÿæˆæ ¼å¼åŒ–çš„å‚è€ƒå›¾ç‰‡
        figures_section = "# å‚è€ƒå›¾ç‰‡\n\n"
        
        # æŒ‰è®ºæ–‡åç§°æ’åº
        sorted_papers = sorted(papers_figures.keys())
        
        for paper_name in sorted_papers:
            figures_dict = papers_figures[paper_name]
            
            # æ·»åŠ è®ºæ–‡åç§°
            figures_section += f"**({paper_name})(2025)**\n"
            
            # æŒ‰full_contentç»„ç»‡å›¾ç‰‡
            figure_num = 1
            for full_content, figure_data in figures_dict.items():
                citation_keys = figure_data["citation_keys"]
                citation_info = figure_data["info"]
                
                # ç”Ÿæˆç« èŠ‚å¼•ç”¨é”®åˆ—è¡¨ï¼ˆåŒ…å«é¡µç ä¿¡æ¯ï¼‰
                citation_keys_with_pages = []
                for key in citation_keys:
                    # ä»åŸå§‹å¼•ç”¨æ•°æ®ä¸­è·å–é¡µç ä¿¡æ¯
                    key_citation_info = figure_citations.get(key, {})
                    page = key_citation_info.get("page", "-1")
                    page_text = f"ç¬¬{page}é¡µ" if page != "-1" else "é¡µç æœªçŸ¥"
                    citation_keys_with_pages.append(f"ç« èŠ‚{key}ï¼Œ{page_text}")
                
                citation_keys_text = "ï¼Œ".join(citation_keys_with_pages)
                
                # æ·»åŠ å›¾ç‰‡é¡¹
                figures_section += f"{figure_num}. {citation_keys_text}\n"
                
                # æ·»åŠ full_content
                if full_content:
                    figures_section += f"({full_content})\n"
                
                # æ·»åŠ å›¾ç‰‡è·¯å¾„
                img_path = citation_info.get("img_path", "")
                paper_name = citation_info.get("paper", "")
                if img_path and paper_name:
                    # æ„å»ºå®Œæ•´è·¯å¾„ï¼šroot_image_path/paper/img_path
                    full_img_path = os.path.join(root_image_path, paper_name, img_path).replace("\\", "/")
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªç« èŠ‚IDä½œä¸ºå›¾ç‰‡çš„altæ–‡æœ¬
                    first_citation_key = citation_keys[0] if citation_keys else "å›¾ç‰‡"
                    figures_section += f"\n\n![ç« èŠ‚{first_citation_key}]({full_img_path})\n"
                
                figure_num += 1
                figures_section += "\n"
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        total_papers = len(sorted_papers)
        total_figures = len(figure_citations)
        figures_section += f"*æ€»è®¡ï¼š{total_papers} ç¯‡è®ºæ–‡ï¼Œ{total_figures} ä¸ªå›¾ç‰‡å¼•ç”¨ï¼Œé¡µç ä¸º-1è¡¨æ˜PDFè§£æé—®é¢˜ï¼Œè¯·å‚è€ƒå¼•ç”¨JSON*\n"
        
        print(f"ğŸ–¼ï¸ å‚è€ƒå›¾ç‰‡ç”Ÿæˆå®Œæˆ: {total_papers} ç¯‡è®ºæ–‡ï¼Œ{total_figures} ä¸ªå›¾ç‰‡å¼•ç”¨")
        return figures_section
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå‚è€ƒå›¾ç‰‡æ—¶å‡ºé”™: {e}")
        return "# å‚è€ƒå›¾ç‰‡\n\nå‚è€ƒå›¾ç‰‡ç”Ÿæˆå¤±è´¥ã€‚\n"


def generate_tables_from_citations(topic: str = "ç»¼è¿°") -> str:
    """
    ä»ç»Ÿä¸€çš„å¼•ç”¨JSONæ–‡ä»¶ä¸­ç”Ÿæˆå‚è€ƒè¡¨æ ¼åˆ—è¡¨
    
    è¯´æ˜ï¼šè¯»å–write_section_citationsä¿å­˜çš„JSONæ–‡ä»¶ï¼Œæå–æ‰€æœ‰è¡¨æ ¼ç±»å‹å¼•ç”¨ï¼Œ
    æŒ‰è®ºæ–‡åˆ†ç»„æ•´ç†ï¼Œç”Ÿæˆæ ¼å¼åŒ–çš„å‚è€ƒè¡¨æ ¼åˆ—è¡¨ï¼ŒåŒ…å«è¡¨æ ¼è·¯å¾„å’Œè¯¦ç»†ä¿¡æ¯
    
    è¾“å‡ºæ ¼å¼ï¼š
    **(è®ºæ–‡å)**
    1. ç« èŠ‚{chapter_ID}-è¡¨æ ¼NUMï¼Œç¬¬{page}é¡µ
    {table_caption}+{table_footnote}
    Reference: {reference_texts}
    {table_body from original_data}
    ![ç« èŠ‚{chapter_ID}](root_image_path/paper_name/img_path)
    
    Args:
        topic: ä¸»é¢˜åç§°ï¼Œç”¨äºå®šä½å¯¹åº”çš„å¼•ç”¨æ–‡ä»¶
        
    Returns:
        æ ¼å¼åŒ–çš„å‚è€ƒè¡¨æ ¼å­—ç¬¦ä¸²ï¼ŒåŒ…å«è¡¨æ ¼è¯¦ç»†ä¿¡æ¯å’Œæ­£ç¡®çš„å›¾ç‰‡è·¯å¾„ç»“æ„
    """
    import json
    import os
    from datetime import datetime
    from collections import defaultdict
    
    # è®¾ç½®æ ¹å›¾ç‰‡è·¯å¾„
    root_image_path = r"D:\Desktop\ZJU\download\dl3\direct_crawler\results"
    
    # æŸ¥æ‰¾å·²å­˜åœ¨çš„å¼•ç”¨æ–‡ä»¶
    citations_dir = "./chapter_citations"
    safe_topic = "".join(c for c in topic if c.isalnum() or c in [' ', '_', '-']).rstrip()
    safe_topic = safe_topic.replace(' ', '_') if safe_topic else "ç»¼è¿°"
    
    # æ™ºèƒ½æŸ¥æ‰¾å¼•ç”¨æ–‡ä»¶ï¼ˆä¼˜å…ˆä½¿ç”¨æ­£å¼æ–‡ä»¶ï¼Œé¿å…ä¸´æ—¶æ–‡ä»¶ï¼‰
    import glob
    
    # å…ˆæŸ¥æ‰¾æ­£å¼çš„å¼•ç”¨æ–‡ä»¶ï¼ˆä¸åŒ…å«tempæ ‡è®°ï¼‰
    formal_pattern = f"{safe_topic}_citations_[0-9]*_[0-9]*.json"
    formal_path = os.path.join(citations_dir, formal_pattern)
    formal_files = glob.glob(formal_path)
    
    # æŸ¥æ‰¾æ‰€æœ‰å¼•ç”¨æ–‡ä»¶ï¼ˆåŒ…æ‹¬ä¸´æ—¶æ–‡ä»¶ï¼‰
    all_pattern = f"{safe_topic}_citations*.json"
    all_path = os.path.join(citations_dir, all_pattern)
    all_files = glob.glob(all_path)
    
    if formal_files:
        # ä¼˜å…ˆä½¿ç”¨æ­£å¼æ–‡ä»¶
        filepath = max(formal_files, key=os.path.getmtime)
        print(f"ğŸ“Š ä½¿ç”¨æ­£å¼å¼•ç”¨æ–‡ä»¶ç”Ÿæˆå‚è€ƒè¡¨æ ¼: {os.path.basename(filepath)}")
    elif all_files:
        # å¦‚æœæ²¡æœ‰æ­£å¼æ–‡ä»¶ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
        temp_files = [f for f in all_files if 'temp' in f]
        if temp_files:
            filepath = max(temp_files, key=os.path.getmtime)
            print(f"ğŸ“Š ä½¿ç”¨ä¸´æ—¶å¼•ç”¨æ–‡ä»¶ç”Ÿæˆå‚è€ƒè¡¨æ ¼: {os.path.basename(filepath)} ï¼ˆå»ºè®®ç­‰å¾…æ­£å¼æ–‡ä»¶ç”Ÿæˆï¼‰")
        else:
            filepath = max(all_files, key=os.path.getmtime)
            print(f"ğŸ“Š ä½¿ç”¨å¼•ç”¨æ–‡ä»¶ç”Ÿæˆå‚è€ƒè¡¨æ ¼: {os.path.basename(filepath)}")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å¼•ç”¨æ–‡ä»¶: {safe_topic}_citations*.json")
        return "# å‚è€ƒè¡¨æ ¼\n\næ— å¼•ç”¨è¡¨æ ¼ã€‚\n"
    
    try:
        # è¯»å–å¼•ç”¨æ•°æ®
        with open(filepath, 'r', encoding='utf-8') as f:
            citation_data = json.load(f)
        
        # æ”¶é›†æ‰€æœ‰è¡¨æ ¼ç±»å‹çš„å¼•ç”¨
        table_citations = {}  # {citation_key: citation_info}
        
        # éå†æ‰€æœ‰ç« èŠ‚çš„è¯¦ç»†å¼•ç”¨
        sections = citation_data.get("sections", {})
        for section_key, section_data in sections.items():
            detailed_citations = section_data.get("detailed_citations", {})
            
            for citation_key, citation_info in detailed_citations.items():
                if citation_info.get("type") == "table":
                    table_citations[citation_key] = citation_info
        
        if not table_citations:
            print("ğŸ“Š æœªæ‰¾åˆ°è¡¨æ ¼ç±»å‹å¼•ç”¨")
            return "# å‚è€ƒè¡¨æ ¼\n\næ— å¼•ç”¨è¡¨æ ¼ã€‚\n"
        
        # æŒ‰è®ºæ–‡åˆ†ç»„æ•´ç†å¼•ç”¨ï¼Œå¹¶æ ¹æ®table_captionå»é‡
        papers_tables = defaultdict(dict)  # {paper_name: {table_caption: [citation_key_list]}}
        
        for citation_key, citation_info in table_citations.items():
            paper_name = citation_info.get("paper", "æœªçŸ¥æ¥æº")
            table_caption = citation_info.get("table_caption", "")
            
            # å¦‚æœtable_captionä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºæ–°æ¡ç›®
            if table_caption not in papers_tables[paper_name]:
                papers_tables[paper_name][table_caption] = {
                    "citation_keys": [],
                    "info": citation_info  # ä¿å­˜å¼•ç”¨ä¿¡æ¯
                }
            
            # æ·»åŠ ç« èŠ‚å¼•ç”¨é”®
            papers_tables[paper_name][table_caption]["citation_keys"].append(citation_key)
        
        # ç”Ÿæˆæ ¼å¼åŒ–çš„å‚è€ƒè¡¨æ ¼
        tables_section = "# å‚è€ƒè¡¨æ ¼\n\n"
        
        # æŒ‰è®ºæ–‡åç§°æ’åº
        sorted_papers = sorted(papers_tables.keys())
        
        for paper_name in sorted_papers:
            tables_dict = papers_tables[paper_name]
            
            # æ·»åŠ è®ºæ–‡åç§°
            tables_section += f"**({paper_name})(2025)**\n"
            
            # æŒ‰table_captionç»„ç»‡è¡¨æ ¼
            table_num = 1
            for table_caption, table_data in tables_dict.items():
                citation_keys = table_data["citation_keys"]
                citation_info = table_data["info"]
                
                # ç”Ÿæˆç« èŠ‚å¼•ç”¨é”®åˆ—è¡¨ï¼ˆåŒ…å«é¡µç ä¿¡æ¯ï¼‰
                citation_keys_with_pages = []
                for key in citation_keys:
                    # ä»åŸå§‹å¼•ç”¨æ•°æ®ä¸­è·å–é¡µç ä¿¡æ¯
                    key_citation_info = table_citations.get(key, {})
                    page = key_citation_info.get("page", "-1")
                    page_text = f"ç¬¬{page}é¡µ" if page != "-1" else "é¡µç æœªçŸ¥"
                    citation_keys_with_pages.append(f"ç« èŠ‚{key}ï¼Œ{page_text}")
                
                citation_keys_text = "ï¼Œ".join(citation_keys_with_pages)
                
                # æ·»åŠ è¡¨æ ¼é¡¹
                tables_section += f"{table_num}. {citation_keys_text}\n"
                
                # æ·»åŠ table_captionå’Œtable_footnote
                table_caption_text = citation_info.get("table_caption", "")
                table_footnote = citation_info.get("table_footnote", "")
                caption_and_footnote = f"{table_caption_text}+{table_footnote}" if table_footnote else table_caption_text
                if caption_and_footnote:
                    tables_section += f"{caption_and_footnote}\n"
                
                # æ·»åŠ reference_texts
                reference_texts = citation_info.get("reference_texts", "")
                if reference_texts:
                    tables_section += f"Reference: {reference_texts}\n"
                
                # ä»original_dataä¸­æå–table_body
                original_data = citation_info.get("original_data", "")
                table_body = ""
                if original_data:
                    try:
                        # å°è¯•è§£æoriginal_dataï¼ˆå¯èƒ½æ˜¯JSONå­—ç¬¦ä¸²ï¼‰
                        if isinstance(original_data, str):
                            original_dict = json.loads(original_data)
                        else:
                            original_dict = original_data
                        
                        table_body = original_dict.get("table_body", "")
                    except (json.JSONDecodeError, AttributeError):
                        # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ç›´æ¥ä½œä¸ºå­—å…¸å¤„ç†
                        if isinstance(original_data, dict):
                            table_body = original_data.get("table_body", "")
                        else:
                            table_body = str(original_data)
                
                if table_body:
                    tables_section += f"{table_body}\n"
                
                # æ·»åŠ è¡¨æ ¼å›¾ç‰‡è·¯å¾„
                img_path = citation_info.get("img_path", "")
                if img_path and paper_name:
                    # æ„å»ºå®Œæ•´è·¯å¾„ï¼šroot_image_path/paper/img_path
                    full_img_path = os.path.join(root_image_path, paper_name, img_path).replace("\\", "/")
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªç« èŠ‚IDä½œä¸ºå›¾ç‰‡çš„altæ–‡æœ¬
                    first_citation_key = citation_keys[0] if citation_keys else "è¡¨æ ¼"
                    tables_section += f"\n\n![ç« èŠ‚{first_citation_key}]({full_img_path})\n"
                
                table_num += 1
                tables_section += "\n"
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        total_papers = len(sorted_papers)
        total_tables = len(table_citations)
        tables_section += f"*æ€»è®¡ï¼š{total_papers} ç¯‡è®ºæ–‡ï¼Œ{total_tables} ä¸ªè¡¨æ ¼å¼•ç”¨ï¼Œé¡µç ä¸º-1è¡¨æ˜PDFè§£æé—®é¢˜ï¼Œè¯·å‚è€ƒå¼•ç”¨JSON*\n"
        
        print(f"ğŸ“Š å‚è€ƒè¡¨æ ¼ç”Ÿæˆå®Œæˆ: {total_papers} ç¯‡è®ºæ–‡ï¼Œ{total_tables} ä¸ªè¡¨æ ¼å¼•ç”¨")
        return tables_section
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå‚è€ƒè¡¨æ ¼æ—¶å‡ºé”™: {e}")
        return "# å‚è€ƒè¡¨æ ¼\n\nå‚è€ƒè¡¨æ ¼ç”Ÿæˆå¤±è´¥ã€‚\n"


def insert_figures_into_document(full_document: str, topic: str = "ç»¼è¿°") -> str:
    """
    åœ¨å®Œæ•´æ–‡æ¡£ä¸­è‡ªåŠ¨æ’å…¥å›¾ç‰‡
    
    è¯´æ˜ï¼šæ£€ç´¢å…¨æ–‡ä¸­çš„å›¾ç‰‡å¼•ç”¨ï¼Œåœ¨ç¬¬ä¸€æ¬¡å¼•ç”¨ä½ç½®æ’å…¥å›¾ç‰‡é“¾æ¥ï¼Œ
    ç¡®ä¿æ¯ä¸ªå›¾ç‰‡åªæ’å…¥ä¸€æ¬¡
    
    Args:
        full_document: å®Œæ•´çš„æ–‡æ¡£å†…å®¹
        topic: ä¸»é¢˜åç§°ï¼Œç”¨äºå®šä½å¯¹åº”çš„å¼•ç”¨æ–‡ä»¶
        
    Returns:
        æ’å…¥å›¾ç‰‡åçš„æ–‡æ¡£å†…å®¹
    """
    import json
    import os
    import re
    from collections import defaultdict
    
    # è®¾ç½®æ ¹å›¾ç‰‡è·¯å¾„
    root_image_path = r"D:\Desktop\ZJU\download\dl3\direct_crawler\results"
    
    # æŸ¥æ‰¾å·²å­˜åœ¨çš„å¼•ç”¨æ–‡ä»¶
    citations_dir = "./chapter_citations"
    safe_topic = "".join(c for c in topic if c.isalnum() or c in [' ', '_', '-']).rstrip()
    safe_topic = safe_topic.replace(' ', '_') if safe_topic else "ç»¼è¿°"
    
    # æ™ºèƒ½æŸ¥æ‰¾å¼•ç”¨æ–‡ä»¶ï¼ˆä¼˜å…ˆä½¿ç”¨æ­£å¼æ–‡ä»¶ï¼Œé¿å…ä¸´æ—¶æ–‡ä»¶ï¼‰
    import glob
    
    # å…ˆæŸ¥æ‰¾æ­£å¼çš„å¼•ç”¨æ–‡ä»¶ï¼ˆä¸åŒ…å«tempæ ‡è®°ï¼‰
    formal_pattern = f"{safe_topic}_citations_[0-9]*_[0-9]*.json"
    formal_path = os.path.join(citations_dir, formal_pattern)
    formal_files = glob.glob(formal_path)
    
    # æŸ¥æ‰¾æ‰€æœ‰å¼•ç”¨æ–‡ä»¶ï¼ˆåŒ…æ‹¬ä¸´æ—¶æ–‡ä»¶ï¼‰
    all_pattern = f"{safe_topic}_citations*.json"
    all_path = os.path.join(citations_dir, all_pattern)
    all_files = glob.glob(all_path)
    
    if formal_files:
        # ä¼˜å…ˆä½¿ç”¨æ­£å¼æ–‡ä»¶
        filepath = max(formal_files, key=os.path.getmtime)
        print(f"ğŸ–¼ï¸ ä½¿ç”¨æ­£å¼å¼•ç”¨æ–‡ä»¶æ’å…¥å›¾ç‰‡: {os.path.basename(filepath)}")
    elif all_files:
        # å¦‚æœæ²¡æœ‰æ­£å¼æ–‡ä»¶ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
        temp_files = [f for f in all_files if 'temp' in f]
        if temp_files:
            filepath = max(temp_files, key=os.path.getmtime)
            print(f"ğŸ–¼ï¸ ä½¿ç”¨ä¸´æ—¶å¼•ç”¨æ–‡ä»¶æ’å…¥å›¾ç‰‡: {os.path.basename(filepath)} ï¼ˆå»ºè®®ç­‰å¾…æ­£å¼æ–‡ä»¶ç”Ÿæˆï¼‰")
        else:
            filepath = max(all_files, key=os.path.getmtime)
            print(f"ğŸ–¼ï¸ ä½¿ç”¨å¼•ç”¨æ–‡ä»¶æ’å…¥å›¾ç‰‡: {os.path.basename(filepath)}")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å¼•ç”¨æ–‡ä»¶ï¼Œè·³è¿‡å›¾ç‰‡æ’å…¥")
        return full_document
    
    try:
        # è¯»å–å¼•ç”¨æ•°æ®
        with open(filepath, 'r', encoding='utf-8') as f:
            citation_data = json.load(f)
        
        # æ”¶é›†æ‰€æœ‰å›¾ç‰‡ç±»å‹çš„å¼•ç”¨
        figure_citations = {}  # {citation_key: citation_info}
        
        # éå†æ‰€æœ‰ç« èŠ‚çš„è¯¦ç»†å¼•ç”¨
        sections = citation_data.get("sections", {})
        for section_key, section_data in sections.items():
            detailed_citations = section_data.get("detailed_citations", {})
            
            for citation_key, citation_info in detailed_citations.items():
                if citation_info.get("type") == "figure":
                    figure_citations[citation_key] = citation_info
        
        if not figure_citations:
            print("ğŸ–¼ï¸ æœªæ‰¾åˆ°å›¾ç‰‡ç±»å‹å¼•ç”¨ï¼Œè·³è¿‡å›¾ç‰‡æ’å…¥")
            return full_document
        
        # è¿½è¸ªå·²æ’å…¥çš„å›¾ç‰‡ï¼Œé¿å…é‡å¤æ’å…¥
        inserted_figures = set()
        
        # æŒ‰æ®µè½åˆ†å‰²æ–‡æ¡£
        paragraphs = full_document.split('\n\n')
        result_paragraphs = []
        
        for paragraph in paragraphs:
            # æ·»åŠ å½“å‰æ®µè½
            result_paragraphs.append(paragraph)
            
            # ä»æ®µè½ä¸­æå–æ‰€æœ‰å›¾ç‰‡å¼•ç”¨æ¨¡å¼
            # åŒ¹é… "æ•°å­—-å›¾ç‰‡æ•°å­—" æ ¼å¼çš„å¼•ç”¨
            figure_pattern = r'(\d+-å›¾ç‰‡\d+)'
            found_references = re.findall(figure_pattern, paragraph)
            
            # å»é‡å¹¶ä¿æŒé¡ºåº
            unique_references = []
            for ref in found_references:
                if ref not in unique_references:
                    unique_references.append(ref)
            
            # å¯¹æ¯ä¸ªæ‰¾åˆ°çš„å›¾ç‰‡å¼•ç”¨è¿›è¡Œå¤„ç†
            for citation_key in unique_references:
                # æ£€æŸ¥è¯¥å¼•ç”¨æ˜¯å¦åœ¨å·²çŸ¥çš„å›¾ç‰‡å¼•ç”¨ä¸­
                if citation_key not in figure_citations:
                    continue
                    
                citation_info = figure_citations[citation_key]
                
                # æ„å»ºå›¾ç‰‡å”¯ä¸€æ ‡è¯†ï¼ˆåŸºäºimg_pathå’Œpaperï¼‰
                img_path = citation_info.get("img_path", "")
                paper_name = citation_info.get("paper", "")
                figure_id = f"{paper_name}_{img_path}"
                
                # å¦‚æœå›¾ç‰‡å·²ç»æ’å…¥è¿‡ï¼Œè·³è¿‡
                if figure_id in inserted_figures:
                    continue
                
                if img_path and paper_name:
                    # æ„å»ºå®Œæ•´å›¾ç‰‡è·¯å¾„
                    full_img_path = os.path.join(root_image_path, paper_name, img_path).replace("\\", "/")
                    
                    # æ’å…¥å›¾ç‰‡ï¼ˆå¸¦æ ‡é¢˜ï¼‰
                    figure_markdown = f"\n\nç« èŠ‚{citation_key}\n\n![ç« èŠ‚{citation_key}]({full_img_path})\n"
                    result_paragraphs.append(figure_markdown)
                    
                    # æ ‡è®°è¯¥å›¾ç‰‡å·²æ’å…¥
                    inserted_figures.add(figure_id)
                    
                    print(f"ğŸ–¼ï¸ å·²æ’å…¥å›¾ç‰‡: ç« èŠ‚{citation_key}")
        
        # é‡æ–°ç»„åˆæ–‡æ¡£
        modified_document = '\n\n'.join(result_paragraphs)
        
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        modified_document = re.sub(r'\n{4,}', '\n\n\n', modified_document)
        
        total_inserted = len(inserted_figures)
        print(f"ğŸ–¼ï¸ å›¾ç‰‡æ’å…¥å®Œæˆ: å…±æ’å…¥ {total_inserted} å¼ å›¾ç‰‡")
        
        return modified_document
        
    except Exception as e:
        print(f"âŒ æ’å…¥å›¾ç‰‡æ—¶å‡ºé”™: {e}")
        return full_document


def insert_tables_into_document(full_document: str, topic: str = "ç»¼è¿°") -> str:
    """
    åœ¨å®Œæ•´æ–‡æ¡£ä¸­è‡ªåŠ¨æ’å…¥è¡¨æ ¼å›¾åƒ
    
    è¯´æ˜ï¼šæ£€ç´¢å…¨æ–‡ä¸­çš„è¡¨æ ¼å¼•ç”¨ï¼Œåœ¨ç¬¬ä¸€æ¬¡å¼•ç”¨ä½ç½®æ’å…¥è¡¨æ ¼å›¾åƒé“¾æ¥ï¼Œ
    ç¡®ä¿æ¯ä¸ªè¡¨æ ¼å›¾åƒåªæ’å…¥ä¸€æ¬¡
    
    Args:
        full_document: å®Œæ•´çš„æ–‡æ¡£å†…å®¹
        topic: ä¸»é¢˜åç§°ï¼Œç”¨äºå®šä½å¯¹åº”çš„å¼•ç”¨æ–‡ä»¶
        
    Returns:
        æ’å…¥è¡¨æ ¼å›¾åƒåçš„æ–‡æ¡£å†…å®¹
    """
    import json
    import os
    import re
    from collections import defaultdict
    
    # è®¾ç½®æ ¹å›¾ç‰‡è·¯å¾„
    root_image_path = r"D:\Desktop\ZJU\download\dl3\direct_crawler\results"
    
    # æŸ¥æ‰¾å·²å­˜åœ¨çš„å¼•ç”¨æ–‡ä»¶
    citations_dir = "./chapter_citations"
    safe_topic = "".join(c for c in topic if c.isalnum() or c in [' ', '_', '-']).rstrip()
    safe_topic = safe_topic.replace(' ', '_') if safe_topic else "ç»¼è¿°"
    
    # æ™ºèƒ½æŸ¥æ‰¾å¼•ç”¨æ–‡ä»¶ï¼ˆä¼˜å…ˆä½¿ç”¨æ­£å¼æ–‡ä»¶ï¼Œé¿å…ä¸´æ—¶æ–‡ä»¶ï¼‰
    import glob
    
    # å…ˆæŸ¥æ‰¾æ­£å¼çš„å¼•ç”¨æ–‡ä»¶ï¼ˆä¸åŒ…å«tempæ ‡è®°ï¼‰
    formal_pattern = f"{safe_topic}_citations_[0-9]*_[0-9]*.json"
    formal_path = os.path.join(citations_dir, formal_pattern)
    formal_files = glob.glob(formal_path)
    
    # æŸ¥æ‰¾æ‰€æœ‰å¼•ç”¨æ–‡ä»¶ï¼ˆåŒ…æ‹¬ä¸´æ—¶æ–‡ä»¶ï¼‰
    all_pattern = f"{safe_topic}_citations*.json"
    all_path = os.path.join(citations_dir, all_pattern)
    all_files = glob.glob(all_path)
    
    if formal_files:
        # ä¼˜å…ˆä½¿ç”¨æ­£å¼æ–‡ä»¶
        filepath = max(formal_files, key=os.path.getmtime)
        print(f"ğŸ“Š ä½¿ç”¨æ­£å¼å¼•ç”¨æ–‡ä»¶æ’å…¥è¡¨æ ¼: {os.path.basename(filepath)}")
    elif all_files:
        # å¦‚æœæ²¡æœ‰æ­£å¼æ–‡ä»¶ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
        temp_files = [f for f in all_files if 'temp' in f]
        if temp_files:
            filepath = max(temp_files, key=os.path.getmtime)
            print(f"ğŸ“Š ä½¿ç”¨ä¸´æ—¶å¼•ç”¨æ–‡ä»¶æ’å…¥è¡¨æ ¼: {os.path.basename(filepath)} ï¼ˆå»ºè®®ç­‰å¾…æ­£å¼æ–‡ä»¶ç”Ÿæˆï¼‰")
        else:
            filepath = max(all_files, key=os.path.getmtime)
            print(f"ğŸ“Š ä½¿ç”¨å¼•ç”¨æ–‡ä»¶æ’å…¥è¡¨æ ¼: {os.path.basename(filepath)}")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å¼•ç”¨æ–‡ä»¶ï¼Œè·³è¿‡è¡¨æ ¼æ’å…¥")
        return full_document
    
    try:
        # è¯»å–å¼•ç”¨æ•°æ®
        with open(filepath, 'r', encoding='utf-8') as f:
            citation_data = json.load(f)
        
        # æ”¶é›†æ‰€æœ‰è¡¨æ ¼ç±»å‹çš„å¼•ç”¨
        table_citations = {}  # {citation_key: citation_info}
        
        # éå†æ‰€æœ‰ç« èŠ‚çš„è¯¦ç»†å¼•ç”¨
        sections = citation_data.get("sections", {})
        for section_key, section_data in sections.items():
            detailed_citations = section_data.get("detailed_citations", {})
            
            for citation_key, citation_info in detailed_citations.items():
                if citation_info.get("type") == "table":
                    table_citations[citation_key] = citation_info
        
        if not table_citations:
            print("ğŸ“Š æœªæ‰¾åˆ°è¡¨æ ¼ç±»å‹å¼•ç”¨ï¼Œè·³è¿‡è¡¨æ ¼æ’å…¥")
            return full_document
        
        # è¿½è¸ªå·²æ’å…¥çš„è¡¨æ ¼ï¼Œé¿å…é‡å¤æ’å…¥
        inserted_tables = set()
        
        # æŒ‰æ®µè½åˆ†å‰²æ–‡æ¡£
        paragraphs = full_document.split('\n\n')
        result_paragraphs = []
        
        for paragraph in paragraphs:
            # æ·»åŠ å½“å‰æ®µè½
            result_paragraphs.append(paragraph)
            
            # ä»æ®µè½ä¸­æå–æ‰€æœ‰è¡¨æ ¼å¼•ç”¨æ¨¡å¼
            # åŒ¹é… "æ•°å­—-è¡¨æ ¼æ•°å­—" æ ¼å¼çš„å¼•ç”¨
            table_pattern = r'(\d+-è¡¨æ ¼\d+)'
            found_references = re.findall(table_pattern, paragraph)
            
            # å»é‡å¹¶ä¿æŒé¡ºåº
            unique_references = []
            for ref in found_references:
                if ref not in unique_references:
                    unique_references.append(ref)
            
            # å¯¹æ¯ä¸ªæ‰¾åˆ°çš„è¡¨æ ¼å¼•ç”¨è¿›è¡Œå¤„ç†
            for citation_key in unique_references:
                # æ£€æŸ¥è¯¥å¼•ç”¨æ˜¯å¦åœ¨å·²çŸ¥çš„è¡¨æ ¼å¼•ç”¨ä¸­
                if citation_key not in table_citations:
                    continue
                    
                citation_info = table_citations[citation_key]
                
                # æ„å»ºè¡¨æ ¼å”¯ä¸€æ ‡è¯†ï¼ˆåŸºäºimg_pathå’Œpaperï¼‰
                img_path = citation_info.get("img_path", "")
                paper_name = citation_info.get("paper", "")
                table_id = f"{paper_name}_{img_path}"
                
                # å¦‚æœè¡¨æ ¼å·²ç»æ’å…¥è¿‡ï¼Œè·³è¿‡
                if table_id in inserted_tables:
                    continue
                
                if img_path and paper_name:
                    # æ„å»ºå®Œæ•´è¡¨æ ¼å›¾åƒè·¯å¾„
                    full_img_path = os.path.join(root_image_path, paper_name, img_path).replace("\\", "/")
                    
                    # æ’å…¥è¡¨æ ¼å›¾åƒï¼ˆå¸¦æ ‡é¢˜ï¼‰
                    table_markdown = f"\n\nç« èŠ‚{citation_key}\n\n![ç« èŠ‚{citation_key}]({full_img_path})\n"
                    result_paragraphs.append(table_markdown)
                    
                    # æ ‡è®°è¯¥è¡¨æ ¼å·²æ’å…¥
                    inserted_tables.add(table_id)
                    
                    print(f"ğŸ“Š å·²æ’å…¥è¡¨æ ¼: ç« èŠ‚{citation_key}")
        
        # é‡æ–°ç»„åˆæ–‡æ¡£
        modified_document = '\n\n'.join(result_paragraphs)
        
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        modified_document = re.sub(r'\n{4,}', '\n\n\n', modified_document)
        
        total_inserted = len(inserted_tables)
        print(f"ğŸ“Š è¡¨æ ¼æ’å…¥å®Œæˆ: å…±æ’å…¥ {total_inserted} å¼ è¡¨æ ¼å›¾åƒ")
        
        return modified_document
        
    except Exception as e:
        print(f"âŒ æ’å…¥è¡¨æ ¼æ—¶å‡ºé”™: {e}")
        return full_document


def generate_table_of_contents(full_document: str, topic: str = "") -> str:
    """
    ä»å®Œæ•´æ–‡æ¡£ä¸­æå–æ ‡é¢˜å¹¶ç”Ÿæˆç›®å½•
    
    Args:
        full_document: å®Œæ•´çš„æ–‡æ¡£å†…å®¹
        topic: ä¸»é¢˜åç§°ï¼Œç”¨äºè¿‡æ»¤é‡å¤çš„ä¸»æ ‡é¢˜
        
    Returns:
        æ ¼å¼åŒ–çš„ç›®å½•å­—ç¬¦ä¸²
    """
    import re
    
    # æå–æ‰€æœ‰æ ‡é¢˜ï¼ˆæ”¯æŒå¤šçº§æ ‡é¢˜ï¼‰
    headings = []
    lines = full_document.split('\n')
    
    for line in lines:
        line = line.strip()
        if line.startswith('#'):
            # è®¡ç®—æ ‡é¢˜çº§åˆ«
            level = 0
            for char in line:
                if char == '#':
                    level += 1
                else:
                    break
            
            # æå–æ ‡é¢˜æ–‡æœ¬
            title_text = line[level:].strip()
            
            # è·³è¿‡æ‘˜è¦éƒ¨åˆ†ã€å‚è€ƒæ–‡çŒ®ã€å‚è€ƒå…¬å¼ã€ç›®å½•å’Œä¸»æ ‡é¢˜
            skip_titles = ['æ‘˜è¦', 'å‚è€ƒæ–‡çŒ®', 'å‚è€ƒå…¬å¼', 'ç›®å½•']
            if topic:
                skip_titles.append(topic)  # æ·»åŠ ä¸»æ ‡é¢˜åˆ°è·³è¿‡åˆ—è¡¨
                
            if title_text in skip_titles:
                continue
                
            headings.append({
                'level': level,
                'text': title_text,
                'original': line
            })
    
    if not headings:
        return "# ç›®å½•\n\næš‚æ— æ ‡é¢˜ç»“æ„ã€‚\n\n"
    
    # ç”Ÿæˆç›®å½•
    toc = "# ç›®å½•\n\n"
    
    for heading in headings:
        level = heading['level']
        text = heading['text']
        
        # æ ¹æ®çº§åˆ«æ·»åŠ ç¼©è¿›
        indent = '  ' * (level - 1)  # ç¬¬ä¸€çº§ä¸ç¼©è¿›ï¼Œç¬¬äºŒçº§ç¼©è¿›2ç©ºæ ¼ï¼Œä»¥æ­¤ç±»æ¨
        
        # ä¸ºä¸€çº§æ ‡é¢˜æ·»åŠ åºå·
        if level == 1:
            toc += f"{indent}- **{text}**\n"
        else:
            toc += f"{indent}- {text}\n"
    
    toc += "\n"
    return toc


def format_subtopics_section(subtopics: List[str]) -> str:
    """
    æ ¼å¼åŒ–subtopicsä¿¡æ¯
    
    Args:
        subtopics: subtopicsåˆ—è¡¨
        
    Returns:
        æ ¼å¼åŒ–çš„subtopicså­—ç¬¦ä¸²
    """
    if not subtopics:
        return ""
    
    subtopics_text = "**ç ”ç©¶å­ä¸»é¢˜ï¼š**\n\n"
    
    for i, subtopic in enumerate(subtopics, 1):
        subtopics_text += f"{i}. {subtopic}\n"
    
    subtopics_text += "\n"
    return subtopics_text


def build_subsection_guidance(section_info: Dict, section_guidance: Dict = None) -> str:
    """
    æ„å»ºå­ç« èŠ‚è¯¦ç»†å†…å®¹æŒ‡å¼•
    
    è„šæœ¬ç›®æ ‡: ä¸ºç« èŠ‚æ’°å†™ç”Ÿæˆè¯¦ç»†çš„å­ç« èŠ‚å†…å®¹æŒ‡å¼•æ–‡æœ¬
    ä¸Šä¸‹æ–‡: ä»multi_agent.pyä¸­æå–çš„å­ç« èŠ‚æŒ‡å¼•ç”Ÿæˆé€»è¾‘
    è¾“å…¥: 
    - section_info: ç« èŠ‚ä¿¡æ¯å­—å…¸
    - section_guidance: ç« èŠ‚æŒ‡å¼•å­—å…¸ï¼ˆå¯é€‰ï¼‰
    æ‰§è¡Œæ­¥éª¤:
    1. æ£€æŸ¥æ˜¯å¦æœ‰å­ç« èŠ‚ä¿¡æ¯
    2. å¤„ç†å­—å…¸æˆ–åˆ—è¡¨ä¸¤ç§æ•°æ®ç»“æ„
    3. ä¸ºæ¯ä¸ªå­ç« èŠ‚æå–è¯¦ç»†ä¿¡æ¯
    4. æ ¼å¼åŒ–ç”ŸæˆæŒ‡å¼•æ–‡æœ¬
    5. å¤„ç†æ— å­ç« èŠ‚çš„æƒ…å†µ
    è¾“å‡º: æ ¼å¼åŒ–çš„å­ç« èŠ‚æŒ‡å¼•æ–‡æœ¬
    
    Args:
        section_info: åŒ…å«ç« èŠ‚å’Œå­ç« èŠ‚ä¿¡æ¯çš„å­—å…¸
        section_guidance: ç« èŠ‚æŒ‡å¼•ä¿¡æ¯å­—å…¸ï¼Œç”¨äºè¡¥å……å­ç« èŠ‚è¯¦ç»†ä¿¡æ¯
        
    Returns:
        æ ¼å¼åŒ–çš„å­ç« èŠ‚è¯¦ç»†å†…å®¹æŒ‡å¼•å­—ç¬¦ä¸²
    """
    guidance_text = ""
    
    # æ·»åŠ å­ç« èŠ‚ä¿¡æ¯ - ä¼˜å…ˆä»section_infoè·å–å®Œæ•´ä¿¡æ¯
    subsections = section_info.get("subsections", {})
    if subsections:
        guidance_text += "\nã€å­ç« èŠ‚è¯¦ç»†å†…å®¹æŒ‡å¼•ã€‘\n"
        
        # å¤„ç†ä¸¤ç§å¯èƒ½çš„æ•°æ®ç»“æ„ï¼šå­—å…¸æˆ–åˆ—è¡¨
        if isinstance(subsections, dict):
            # å¦‚æœsubsectionsæ˜¯å­—å…¸ï¼Œéå†å…¶é¡¹
            for subsection_id, subsection in subsections.items():
                subsection_title = subsection.get("title", "")
                
                # ä¼˜å…ˆä»section_infoçš„å­ç« èŠ‚ä¿¡æ¯ä¸­è·å–ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»section_guidanceè·å–
                subsection_guide = {}
                if isinstance(subsection, dict):
                    # å¦‚æœsubsectionæœ¬èº«åŒ…å«è¯¦ç»†ä¿¡æ¯ï¼Œç›´æ¥ä½¿ç”¨
                    subsection_guide = subsection
                else:
                    # å¦åˆ™ä»section_guidanceä¸­è·å–
                    if section_guidance:
                        subsection_guide = section_guidance.get("subsections", {}).get(subsection_id, {})
                
                sub_content_guide = subsection_guide.get("content_guide", "")
                key_points = subsection_guide.get("key_points", [])
                writing_guide = subsection_guide.get("writing_guide", "")
                
                guidance_text += f"\nã€{subsection_id} {subsection_title}ã€‘\n"
                guidance_text += f"å†…å®¹æ¦‚è¦: {sub_content_guide}\n"
                
                if key_points:
                    guidance_text += "å…³é”®è¦ç‚¹:\n"
                    for i, point in enumerate(key_points, 1):
                        guidance_text += f"- {point}\n"
                
                if writing_guide:
                    guidance_text += f"å†™ä½œå»ºè®®: {writing_guide}\n"
                    
        elif isinstance(subsections, list):
            # å¦‚æœsubsectionsæ˜¯åˆ—è¡¨ï¼Œä¿æŒåŸæœ‰é€»è¾‘
            for subsection in subsections:
                subsection_id = subsection.get("id", "")
                subsection_title = subsection.get("title", "")
                
                # ä»section_guidanceä¸­è·å–å­ç« èŠ‚æŒ‡å¼•
                subsection_guide = {}
                if section_guidance:
                    subsection_guide = section_guidance.get("subsections", {}).get(subsection_id, {})
                
                sub_content_guide = subsection_guide.get("content_guide", "")
                key_points = subsection_guide.get("key_points", [])
                writing_guide = subsection_guide.get("writing_guide", "")
                
                guidance_text += f"\nã€{subsection_id} {subsection_title}ã€‘\n"
                guidance_text += f"å†…å®¹æ¦‚è¦: {sub_content_guide}\n"
                
                if key_points:
                    guidance_text += "å…³é”®è¦ç‚¹:\n"
                    for i, point in enumerate(key_points, 1):
                        guidance_text += f"- {point}\n"
                
                if writing_guide:
                    guidance_text += f"å†™ä½œå»ºè®®: {writing_guide}\n"
    else:
        guidance_text += "\nã€æ³¨æ„ã€‘æœ¬ç« èŠ‚æ— æ˜ç¡®å­ç« èŠ‚åˆ’åˆ†ï¼Œè¯·æ ¹æ®ç« èŠ‚ä¸»é¢˜å’Œå†…å®¹æŒ‡å¼•è¿›è¡Œåˆç†çš„å†…å®¹ç»„ç»‡ã€‚\n"
    
    return guidance_text


def _deduplicate_materials(materials: List[Dict]) -> List[Dict]:
    """
    å»é™¤é‡å¤ææ–™ï¼Œä¿ç•™ç›¸å…³åº¦æ›´é«˜çš„ç‰ˆæœ¬
    
    è„šæœ¬ç›®æ ‡: å»é™¤é€šç”¨ææ–™å’Œç‰¹å®šææ–™ä¹‹é—´çš„é‡å¤é¡¹
    ä¸Šä¸‹æ–‡: åœ¨åˆå¹¶é€šç”¨ææ–™å’Œç‰¹å®šææ–™åè°ƒç”¨
    è¾“å…¥: ææ–™åˆ—è¡¨ï¼Œæ¯ä¸ªææ–™åŒ…å«content, paper, page, relevance_scoreç­‰å­—æ®µ
    æ‰§è¡Œæ­¥éª¤:
    1. åŸºäºå†…å®¹hashè¿›è¡Œåˆæ­¥å»é‡
    2. åŸºäºpaper+pageç»„åˆè¿›è¡ŒäºŒæ¬¡å»é‡
    3. å¯¹é‡å¤é¡¹ä¿ç•™ç›¸å…³åº¦æ›´é«˜çš„ç‰ˆæœ¬
    4. è¾“å‡ºå»é‡ç»Ÿè®¡ä¿¡æ¯
    è¾“å‡º: å»é‡åçš„ææ–™åˆ—è¡¨
    
    Args:
        materials: åŒ…å«é‡å¤é¡¹çš„ææ–™åˆ—è¡¨
        
    Returns:
        å»é‡åçš„ææ–™åˆ—è¡¨
    """
    if not materials:
        return materials
    
    print(f"ğŸ”„ å¼€å§‹ææ–™å»é‡ï¼ŒåŸå§‹ææ–™æ•°é‡: {len(materials)} æ¡")
    
    # ç”¨äºè·Ÿè¸ªå·²è§è¿‡çš„å†…å®¹å’Œæ¥æº
    content_hash_map = {}  # content_hash -> material_with_highest_score
    paper_page_map = {}    # (paper, page) -> material_with_highest_score
    
    deduplicated = []
    content_duplicates = 0
    paper_page_duplicates = 0
    
    for material in materials:
        content = material.get("content", "")
        paper = material.get("paper", "")
        page = material.get("page", 0)
        relevance_score = material.get("relevance_score", 0.0)
        
        # ç”Ÿæˆå†…å®¹çš„hashå€¼ï¼ˆç”¨äºå¿«é€Ÿæ¯”è¾ƒï¼‰
        content_hash = hashlib.md5(content.encode('utf-8')).hexdigest() if content else "empty"
        
        # ç”Ÿæˆpaper+pageçš„ç»„åˆé”®
        paper_page_key = (paper, page)
        
        is_duplicate = False
        
        # 1. æ£€æŸ¥å†…å®¹æ˜¯å¦é‡å¤
        if content_hash in content_hash_map:
            existing_material = content_hash_map[content_hash]
            existing_score = existing_material.get("relevance_score", 0.0)
            
            if relevance_score > existing_score:
                # æ–°ææ–™ç›¸å…³åº¦æ›´é«˜ï¼Œæ›¿æ¢æ—§çš„
                content_hash_map[content_hash] = material
                # ä»deduplicatedä¸­ç§»é™¤æ—§ææ–™
                deduplicated = [m for m in deduplicated if hashlib.md5(m.get("content", "").encode('utf-8')).hexdigest() != content_hash]
                deduplicated.append(material)
            
            content_duplicates += 1
            is_duplicate = True
        
        # 2. æ£€æŸ¥paper+pageç»„åˆæ˜¯å¦é‡å¤
        elif paper_page_key in paper_page_map and paper and page > 0:
            existing_material = paper_page_map[paper_page_key]
            existing_score = existing_material.get("relevance_score", 0.0)
            
            if relevance_score > existing_score:
                # æ–°ææ–™ç›¸å…³åº¦æ›´é«˜ï¼Œæ›¿æ¢æ—§çš„
                paper_page_map[paper_page_key] = material
                # ä»deduplicatedä¸­ç§»é™¤æ—§ææ–™
                deduplicated = [m for m in deduplicated if not (m.get("paper") == paper and m.get("page") == page)]
                deduplicated.append(material)
            
            paper_page_duplicates += 1
            is_duplicate = True
        
        # 3. å¦‚æœä¸æ˜¯é‡å¤é¡¹ï¼Œç›´æ¥æ·»åŠ 
        if not is_duplicate:
            content_hash_map[content_hash] = material
            if paper and page > 0:
                paper_page_map[paper_page_key] = material
            deduplicated.append(material)
    
    # ğŸ“Š è¾“å‡ºå»é‡ç»Ÿè®¡
    total_removed = len(materials) - len(deduplicated)
    print(f"ğŸ“Š ææ–™å»é‡ç»Ÿè®¡:")
    print(f"  åŸå§‹ææ–™: {len(materials)} æ¡")
    print(f"  å†…å®¹é‡å¤: {content_duplicates} æ¡")
    print(f"  æ¥æºé‡å¤: {paper_page_duplicates} æ¡")
    print(f"  æ€»è®¡å»é™¤: {total_removed} æ¡")
    print(f"  æœ€ç»ˆä¿ç•™: {len(deduplicated)} æ¡")
    print(f"  å»é‡ç‡: {(total_removed / len(materials) * 100):.1f}%" if materials else "0%")
    
    return deduplicated


@dataclass
class Citation:
    """å•ä¸ªå¼•ç”¨çš„æ•°æ®ç»“æ„"""
    id: str  # å”¯ä¸€æ ‡è¯†ç¬¦
    authors: str  # ä½œè€…
    year: str  # å¹´ä»½
    title: str  # æ ‡é¢˜
    source: str  # æ¥æºï¼ˆè®ºæ–‡åæˆ–ææ–™ç¼–å·ï¼‰
    page: Optional[int] = None  # é¡µç 
    relevance_score: Optional[float] = None  # ç›¸å…³åº¦è¯„åˆ†


class CitationManager:
    """ç»Ÿä¸€çš„å¼•ç”¨ç®¡ç†ç³»ç»Ÿ"""
    
    def __init__(self):
        self.citations: Dict[str, Citation] = {}  # å­˜å‚¨æ‰€æœ‰å¼•ç”¨
        self.citation_counter = 1  # å¼•ç”¨ç¼–å·è®¡æ•°å™¨
        self.used_citations: List[str] = []  # å·²ä½¿ç”¨çš„å¼•ç”¨IDåˆ—è¡¨
    
    def add_citation(self, title: str, authors: str = None, year: str = "2025", 
                    source: str = None, page: int = None, relevance_score: float = None) -> str:
        """
        æ·»åŠ ä¸€ä¸ªæ–°å¼•ç”¨å¹¶è¿”å›å…¶å¼•ç”¨ID
        
        Args:
            title: è®ºæ–‡æ ‡é¢˜
            authors: ä½œè€…åˆ—è¡¨
            year: å‘è¡¨å¹´ä»½
            source: æ¥æºæ ‡è¯†
            page: é¡µç 
            relevance_score: ç›¸å…³åº¦è¯„åˆ†
            
        Returns:
            å¼•ç”¨ID (å¦‚ "1", "2", "3"...)
        """
        # ç”Ÿæˆå”¯ä¸€IDåŸºäºæ ‡é¢˜çš„å“ˆå¸Œ
        content_hash = hashlib.md5(title.encode('utf-8')).hexdigest()[:8]
        citation_id = str(self.citation_counter)
        
        # å¤„ç†ä½œè€…åç§°
        if not authors:
            # ä»æ ‡é¢˜æˆ–æ¥æºä¸­å°è¯•æå–ä½œè€…ä¿¡æ¯
            if source and any(keyword in source.lower() for keyword in ['et al', 'brown', 'vaswani', 'devlin']):
                authors = extract_authors_from_source(source)
            else:
                authors = "æœªçŸ¥ä½œè€…"
        
        # å¤„ç†æ ‡é¢˜
        if not title and source:
            title = extract_title_from_source(source)
        
        citation = Citation(
            id=citation_id,
            authors=authors,
            year=year,
            title=title,
            source=source or title,
            page=page,
            relevance_score=relevance_score
        )
        
        self.citations[citation_id] = citation
        self.citation_counter += 1
        
        return citation_id
    
    def get_citation_text(self, citation_id: str) -> str:
        """
        è·å–è§„èŒƒçš„å¼•ç”¨æ–‡æœ¬
        
        Args:
            citation_id: å¼•ç”¨ID
            
        Returns:
            æ ¼å¼åŒ–çš„å¼•ç”¨æ–‡æœ¬ï¼Œå¦‚ "[1]" æˆ– "(Smith et al., 2023)"
        """
        if citation_id not in self.citations:
            return f"[æœªçŸ¥å¼•ç”¨:{citation_id}]"
        
        citation = self.citations[citation_id]
        
        # ä½¿ç”¨æ•°å­—å¼•ç”¨æ ¼å¼
        return f"[{citation.id}]"
    
    def get_full_citation(self, citation_id: str) -> str:
        """
        è·å–å®Œæ•´çš„å¼•ç”¨æ ¼å¼ç”¨äºå‚è€ƒæ–‡çŒ®åˆ—è¡¨
        
        Args:
            citation_id: å¼•ç”¨ID
            
        Returns:
            å®Œæ•´çš„å¼•ç”¨æ ¼å¼
        """
        if citation_id not in self.citations:
            return f"[{citation_id}] æœªçŸ¥å¼•ç”¨"
        
        citation = self.citations[citation_id]
        
        # æ ¼å¼ï¼š[ç¼–å·] ä½œè€…. (å¹´ä»½). æ ‡é¢˜.
        return f"[{citation.id}] {citation.authors} ({citation.year}). {citation.title}."
    
    def mark_citation_used(self, citation_id: str):
        """æ ‡è®°å¼•ç”¨ä¸ºå·²ä½¿ç”¨"""
        if citation_id not in self.used_citations:
            self.used_citations.append(citation_id)
    
    def get_bibliography(self) -> str:
        """
        ç”Ÿæˆå®Œæ•´çš„å‚è€ƒæ–‡çŒ®åˆ—è¡¨
        
        Returns:
            æ ¼å¼åŒ–çš„å‚è€ƒæ–‡çŒ®åˆ—è¡¨
        """
        if not self.used_citations:
            return "# å‚è€ƒæ–‡çŒ®\n\næ— å¼•ç”¨æ–‡çŒ®ã€‚\n"
        
        bibliography = "# å‚è€ƒæ–‡çŒ®\n\n"
        
        # æŒ‰å¼•ç”¨IDæ’åº
        sorted_ids = sorted(self.used_citations, key=lambda x: int(x) if x.isdigit() else float('inf'))
        
        for citation_id in sorted_ids:
            if citation_id in self.citations:
                bibliography += f"{self.get_full_citation(citation_id)}\n\n"
        
        return bibliography
    
    def process_materials_for_citations(self, materials: List[Dict]) -> List[str]:
        """
        å¤„ç†ææ–™åˆ—è¡¨ï¼Œä¸ºæ¯ä¸ªææ–™æ·»åŠ å¼•ç”¨å¹¶è¿”å›å¼•ç”¨IDåˆ—è¡¨
        
        Args:
            materials: ææ–™åˆ—è¡¨
            
        Returns:
            å¼•ç”¨IDåˆ—è¡¨
        """
        citation_ids = []
        
        for material in materials:
            paper_name = material.get("paper", "æœªçŸ¥è®ºæ–‡")
            relevance_score = material.get("relevance_score", 0.0)
            page = material.get("page", None)
            
            # æ·»åŠ å¼•ç”¨
            citation_id = self.add_citation(
                title=paper_name,
                source=paper_name,
                page=page,
                relevance_score=relevance_score
            )
            citation_ids.append(citation_id)
            self.mark_citation_used(citation_id)
        
        return citation_ids
    
    def get_citation_stats(self) -> Dict:
        """
        è·å–å¼•ç”¨ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            åŒ…å«å¼•ç”¨ç»Ÿè®¡çš„å­—å…¸
        """
        return {
            "total_citations": len(self.citations),
            "used_citations": len(self.used_citations),
            "unused_citations": len(self.citations) - len(self.used_citations),
            "usage_rate": len(self.used_citations) / len(self.citations) if self.citations else 0
        }
    
    def clear_unused_citations(self):
        """æ¸…ç†æœªä½¿ç”¨çš„å¼•ç”¨"""
        unused_ids = [cid for cid in self.citations.keys() if cid not in self.used_citations]
        for cid in unused_ids:
            del self.citations[cid]
    
    def export_citations_json(self) -> Dict:
        """
        å¯¼å‡ºå¼•ç”¨æ•°æ®ä¸ºJSONæ ¼å¼
        
        Returns:
            åŒ…å«æ‰€æœ‰å¼•ç”¨ä¿¡æ¯çš„å­—å…¸
        """
        return {
            "citations": {
                cid: {
                    "id": citation.id,
                    "authors": citation.authors,
                    "year": citation.year,
                    "title": citation.title,
                    "source": citation.source,
                    "page": citation.page,
                    "relevance_score": citation.relevance_score
                }
                for cid, citation in self.citations.items()
            },
            "used_citations": self.used_citations,
            "stats": self.get_citation_stats()
        } 



def clean_material_references_enriched(enriched: Dict) -> Dict:
    """
    æ¸…æ´—ä¸°å¯Œå¤§çº²ä¸­çš„ææ–™å¼•ç”¨ä¿¡æ¯
    åˆ é™¤æ‰€æœ‰å½¢å¦‚"ç›¸å…³ææ–™"ã€"ææ–™NUM"ã€"(ææ–™X, ææ–™Y)"ç­‰å¼•ç”¨å†…å®¹
    """
    import re
    import copy
    
    if not enriched or not isinstance(enriched, dict):
        return enriched
    
    # æ·±æ‹·è´ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
    cleaned = copy.deepcopy(enriched)
    
    def clean_text(text: str) -> str:
        """æ¸…æ´—å•ä¸ªæ–‡æœ¬ä¸­çš„ææ–™å¼•ç”¨"""
        if not isinstance(text, str):
            return text
        
        # å®šä¹‰éœ€è¦æ¸…ç†çš„æ¨¡å¼
        patterns = [
            r'ï¼ˆç›¸å…³ææ–™[ï¼š:][^ï¼‰]*ï¼‰',           # ï¼ˆç›¸å…³ææ–™ï¼šææ–™36, ææ–™8, ææ–™98, ææ–™89ï¼‰
            r'\(ç›¸å…³ææ–™[ï¼š:][^)]*\)',           # (ç›¸å…³ææ–™ï¼šææ–™36, ææ–™8, ææ–™98, ææ–™89)
            r'ç›¸å…³ææ–™[ï¼š:][^ã€‚ï¼Œ\n]*[ã€‚ï¼Œ]?',      # ç›¸å…³ææ–™ï¼šææ–™36, ææ–™8, ææ–™98, ææ–™89
            r'å‚è€ƒææ–™[ï¼š:][^ã€‚ï¼Œ\n]*[ã€‚ï¼Œ]?',      # å‚è€ƒææ–™ï¼šææ–™1, ææ–™2
            r'ï¼ˆææ–™\d+[ï¼Œ,ã€\s]*[ææ–™\d+ï¼Œ,ã€\s]*[^ï¼‰]*ï¼‰',  # ï¼ˆææ–™36, ææ–™8, ææ–™98ï¼‰
            r'\(ææ–™\d+[ï¼Œ,ã€\s]*[ææ–™\d+ï¼Œ,ã€\s]*[^)]*\)',  # (ææ–™36, ææ–™8, ææ–™98)
            r'ææ–™\d+[ï¼Œ,ã€\s]*(?:ææ–™\d+[ï¼Œ,ã€\s]*)*',     # ææ–™36, ææ–™8, ææ–™98
            r'å‚è§ææ–™\d+',                        # å‚è§ææ–™1
            r'è§ææ–™\d+',                         # è§ææ–™1
            r'\[ææ–™\d+\]',                      # [ææ–™1]
        ]
        
        cleaned_text = text
        for pattern in patterns:
            cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.IGNORECASE)
        
        # æ¸…ç†å¤šä½™çš„ç©ºæ ¼ã€æ ‡ç‚¹ç¬¦å·
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text)  # å¤šä¸ªç©ºæ ¼å˜æˆå•ä¸ª
        cleaned_text = re.sub(r'[ï¼Œ,]\s*[ï¼Œ,]', 'ï¼Œ', cleaned_text)  # é‡å¤é€—å·
        cleaned_text = re.sub(r'[ã€‚]\s*[ã€‚]', 'ã€‚', cleaned_text)  # é‡å¤å¥å·
        cleaned_text = re.sub(r'\s*[ï¼Œ,]\s*$', '', cleaned_text)  # ç»“å°¾çš„é€—å·
        cleaned_text = re.sub(r'^\s*[ï¼Œ,]\s*', '', cleaned_text)  # å¼€å¤´çš„é€—å·
        cleaned_text = cleaned_text.strip()
        
        return cleaned_text
    
    def clean_list(items: list) -> list:
        """æ¸…æ´—åˆ—è¡¨ä¸­çš„æ¯ä¸ªå­—ç¬¦ä¸²é¡¹"""
        if not isinstance(items, list):
            return items
        return [clean_text(item) if isinstance(item, str) else item for item in items]
    
    # æ¸…æ´—é¡¶çº§å­—æ®µ
    if 'topic' in cleaned:
        cleaned['topic'] = clean_text(cleaned['topic'])
    if 'overview' in cleaned:
        cleaned['overview'] = clean_text(cleaned['overview'])
    
    # æ¸…æ´—ç« èŠ‚æ•°æ®
    chapters = cleaned.get('chapters', {})
    if isinstance(chapters, dict):
        # å­—å…¸æ ¼å¼çš„ç« èŠ‚
        for chapter_id, chapter in chapters.items():
            if isinstance(chapter, dict):
                # æ¸…æ´—ç« èŠ‚çº§åˆ«çš„å­—æ®µ
                if 'title' in chapter:
                    chapter['title'] = clean_text(chapter['title'])
                if 'content_guide' in chapter:
                    chapter['content_guide'] = clean_text(chapter['content_guide'])
                if 'keywords' in chapter:
                    chapter['keywords'] = clean_list(chapter['keywords'])
                if 'research_focus' in chapter:
                    chapter['research_focus'] = clean_list(chapter['research_focus'])
                
                # æ¸…æ´—å­ç« èŠ‚
                subsections = chapter.get('subsections', {})
                if isinstance(subsections, dict):
                    # å­—å…¸æ ¼å¼çš„å­ç« èŠ‚
                    for subsection_id, subsection in subsections.items():
                        if isinstance(subsection, dict):
                            if 'title' in subsection:
                                subsection['title'] = clean_text(subsection['title'])
                            if 'content_guide' in subsection:
                                subsection['content_guide'] = clean_text(subsection['content_guide'])
                            if 'key_points' in subsection:
                                subsection['key_points'] = clean_list(subsection['key_points'])
                            if 'writing_guide' in subsection:
                                subsection['writing_guide'] = clean_text(subsection['writing_guide'])
                elif isinstance(subsections, list):
                    # åˆ—è¡¨æ ¼å¼çš„å­ç« èŠ‚
                    for subsection in subsections:
                        if isinstance(subsection, dict):
                            if 'title' in subsection:
                                subsection['title'] = clean_text(subsection['title'])
                            if 'content_guide' in subsection:
                                subsection['content_guide'] = clean_text(subsection['content_guide'])
                            if 'key_points' in subsection:
                                subsection['key_points'] = clean_list(subsection['key_points'])
                            if 'writing_guide' in subsection:
                                subsection['writing_guide'] = clean_text(subsection['writing_guide'])
    
    elif isinstance(chapters, list):
        # åˆ—è¡¨æ ¼å¼çš„ç« èŠ‚
        for chapter in chapters:
            if isinstance(chapter, dict):
                # æ¸…æ´—ç« èŠ‚çº§åˆ«çš„å­—æ®µ
                if 'title' in chapter:
                    chapter['title'] = clean_text(chapter['title'])
                if 'content_guide' in chapter:
                    chapter['content_guide'] = clean_text(chapter['content_guide'])
                if 'keywords' in chapter:
                    chapter['keywords'] = clean_list(chapter['keywords'])
                if 'research_focus' in chapter:
                    chapter['research_focus'] = clean_list(chapter['research_focus'])
                
                # æ¸…æ´—å­ç« èŠ‚
                subsections = chapter.get('subsections', [])
                if isinstance(subsections, list):
                    for subsection in subsections:
                        if isinstance(subsection, dict):
                            if 'title' in subsection:
                                subsection['title'] = clean_text(subsection['title'])
                            if 'content_guide' in subsection:
                                subsection['content_guide'] = clean_text(subsection['content_guide'])
                            if 'key_points' in subsection:
                                subsection['key_points'] = clean_list(subsection['key_points'])
                            if 'writing_guide' in subsection:
                                subsection['writing_guide'] = clean_text(subsection['writing_guide'])
                elif isinstance(subsections, dict):
                    for subsection_id, subsection in subsections.items():
                        if isinstance(subsection, dict):
                            if 'title' in subsection:
                                subsection['title'] = clean_text(subsection['title'])
                            if 'content_guide' in subsection:
                                subsection['content_guide'] = clean_text(subsection['content_guide'])
                            if 'key_points' in subsection:
                                subsection['key_points'] = clean_list(subsection['key_points'])
                            if 'writing_guide' in subsection:
                                subsection['writing_guide'] = clean_text(subsection['writing_guide'])
    
    print("ğŸ§¹ å·²æ¸…æ´—ä¸°å¯Œå¤§çº²ä¸­çš„ææ–™å¼•ç”¨ä¿¡æ¯")
    return cleaned



def _parse_interpretation_response(response: str) -> Dict:
    """
    è§£æLLMçš„ç»¼è¿°ä¸»é¢˜è§£é‡Šå“åº”
    
    Args:
        response: LLMçš„åŸå§‹å“åº”
        
    Returns:
        è§£æåçš„ç»“æœå­—å…¸
    """
    result = {
        "standardized_topic": "",
        "standardized_subtopics": [],
        "analysis": ""
    }
    
    try:
        # æŸ¥æ‰¾è§£æç»“æœéƒ¨åˆ†
        start_marker = "===è§£æç»“æœå¼€å§‹==="
        end_marker = "===è§£æç»“æœç»“æŸ==="
        
        start_idx = response.find(start_marker)
        end_idx = response.find(end_marker)
        
        if start_idx == -1 or end_idx == -1:
            raise ValueError("æœªæ‰¾åˆ°è§£æç»“æœæ ‡è®°")
        
        result_section = response[start_idx + len(start_marker):end_idx].strip()
        
        # è§£æç»¼è¿°æ ¸å¿ƒä¸»é¢˜
        topic_match = re.search(r'ã€ç»¼è¿°æ ¸å¿ƒä¸»é¢˜ã€‘\s*\n?(.+?)(?=\nã€|$)', result_section, re.DOTALL)
        if topic_match:
            result["standardized_topic"] = topic_match.group(1).strip()
        
        # è§£æç»¼è¿°å…³é”®è¯çŸ©é˜µ
        subtopics_list = []
        
        # æå–å…³é”®è¯çŸ©é˜µéƒ¨åˆ†
        matrix_match = re.search(r'ã€ç»¼è¿°å…³é”®è¯çŸ©é˜µã€‘\s*\n?(.*?)(?=\nã€|$)', result_section, re.DOTALL)
        if matrix_match:
            matrix_content = matrix_match.group(1).strip()
            
            # è§£æå„ä¸ªç»´åº¦çš„å…³é”®è¯
            categories = [
                (r'æ ¸å¿ƒæŠ€æœ¯æ–¹æ³•:\s*(.+?)(?=\n|$)', "core_methods"),
                (r'é‡è¦åº”ç”¨é¢†åŸŸ:\s*(.+?)(?=\n|$)', "applications"),
                (r'è¯„ä¼°ä¸æ ‡å‡†:\s*(.+?)(?=\n|$)', "evaluation"),
                (r'äº¤å‰ä¸å‰æ²¿:\s*(.+?)(?=\n|$)', "frontiers")
            ]
            
            for pattern, category in categories:
                cat_match = re.search(pattern, matrix_content, re.MULTILINE)
                if cat_match:
                    keywords_text = cat_match.group(1).strip()
                    if keywords_text:
                        # åˆ†å‰²å¹¶æ¸…ç†å…³é”®è¯
                        keywords = [k.strip() for k in keywords_text.split(',') if k.strip()]
                        subtopics_list.extend(keywords)
        
        # å¦‚æœçŸ©é˜µè§£æå¤±è´¥ï¼Œå°è¯•è§£ææ—§æ ¼å¼ä½œä¸ºfallback
        if not subtopics_list:
            subtopics_match = re.search(r'ã€æ ‡å‡†åŒ–æ¬¡è¦ä¸»é¢˜ã€‘\s*\n?(.+?)(?=\nã€|$)', result_section, re.DOTALL)
            if subtopics_match:
                subtopics_text = subtopics_match.group(1).strip()
                if subtopics_text and subtopics_text != "æ— ":
                    subtopics_list = [s.strip() for s in subtopics_text.split(',') if s.strip()]
        
        result["standardized_subtopics"] = subtopics_list
        
        # è§£æç»¼è¿°ç­–ç•¥åˆ†æ
        analysis_match = re.search(r'ã€ç»¼è¿°ç­–ç•¥åˆ†æã€‘\s*\n?(.*?)(?=\n===|$)', result_section, re.DOTALL)
        if analysis_match:
            result["analysis"] = analysis_match.group(1).strip()
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç­–ç•¥åˆ†æï¼Œå°è¯•æ—§æ ¼å¼
        if not result["analysis"]:
            old_analysis_match = re.search(r'ã€è§£æåˆ†æã€‘\s*\n?(.*?)(?=\n===|$)', result_section, re.DOTALL)
            if old_analysis_match:
                result["analysis"] = old_analysis_match.group(1).strip()
        
    except Exception as e:
        print(f"âš ï¸ è§£æå“åº”æ ¼å¼æ—¶å‡ºé”™: {e}")
        # å°è¯•ç®€å•çš„æ–‡æœ¬æå–ä½œä¸ºfallback
        lines = response.split('\n')
        for line in lines:
            line = line.strip()
            if line and not line.startswith('===') and not line.startswith('ã€'):
                if not result["standardized_topic"] and len(line.split()) <= 5:
                    result["standardized_topic"] = line
                    break
    
    return result


def _format_enrichment_for_analysis( enriched: Dict) -> str:
    """æ ¼å¼åŒ–å½“å‰ä¸°å¯Œå¤§çº²ç”¨äºåˆ†æ"""
    if not enriched:
        return "å½“å‰ä¸°å¯Œå¤§çº²ä¸ºç©º"
    
    
    formatted = f"ä¸»é¢˜: {enriched.get('topic', 'æœªçŸ¥')}\n"
    formatted += f"æ¦‚è¿°: {enriched.get('overview', 'æ— æ¦‚è¿°')}\n\n"
    
    chapters = enriched.get("chapters", {})
    
    # ğŸ”§ ä¿®æ”¹ï¼šåŒæ—¶æ”¯æŒåˆ—è¡¨å’Œå­—å…¸æ ¼å¼çš„ç« èŠ‚æ•°æ®
    if isinstance(chapters, list):
        # å¤„ç†åˆ—è¡¨æ ¼å¼çš„ç« èŠ‚ (å¦‚æœå­˜åœ¨)
        for chapter in chapters:
            chapter_id = chapter.get('id', '?')
            chapter_title = chapter.get('title', 'æœªå‘½åç« èŠ‚')
            content_guide = chapter.get('content_guide', 'æ— æŒ‡å¼•')
            
            formatted += f"# ç¬¬{chapter_id}ç« ï¼š{chapter_title}\n"
            formatted += f"ç« èŠ‚å†…å®¹æŒ‡å¼•: {content_guide}\n"
            
            # æ·»åŠ å…³é”®è¯
            keywords = chapter.get("keywords", [])
            if keywords:
                formatted += f"å…³é”®è¯: {', '.join(keywords)}\n"
            
            # æ·»åŠ ç ”ç©¶é¢†åŸŸ
            research_focus = chapter.get("research_focus", [])
            if research_focus:
                formatted += f"ç ”ç©¶é¢†åŸŸ: {', '.join(research_focus)}\n"
            
            # å¤„ç†å­ç« èŠ‚
            subsections = chapter.get("subsections", [])
            if isinstance(subsections, list):
                for subsection in subsections:
                    sub_id = subsection.get('id', '?')
                    sub_title = subsection.get('title', 'æœªå‘½åå­ç« èŠ‚')
                    sub_content_guide = subsection.get('content_guide', 'æ— æ¦‚è¦')
                    
                    formatted += f"## {sub_id} {sub_title}\n"
                    formatted += f"å†…å®¹æ¦‚è¦: {sub_content_guide}\n"
                    
                    # å…³é”®è¦ç‚¹
                    key_points = subsection.get('key_points', [])
                    if key_points:
                        formatted += f"å…³é”®è¦ç‚¹: {', '.join(key_points)}\n"
                    
                    # å†™ä½œå»ºè®®
                    writing_guide = subsection.get('writing_guide', '')
                    if writing_guide:
                        formatted += f"å†™ä½œå»ºè®®: {writing_guide}\n"
            elif isinstance(subsections, dict):
                # å¤„ç†å­—å…¸æ ¼å¼çš„å­ç« èŠ‚
                for sub_id, subsection in subsections.items():
                    formatted += f"## {sub_id} {subsection.get('title', 'æœªå‘½åå­ç« èŠ‚')}\n"
                    formatted += f"å†…å®¹æ¦‚è¦: {subsection.get('content_guide', 'æ— æ¦‚è¦')}\n"
                    
                    # å…³é”®è¦ç‚¹
                    key_points = subsection.get('key_points', [])
                    if key_points:
                        formatted += f"å…³é”®è¦ç‚¹: {', '.join(key_points)}\n"
                    
                    # å†™ä½œå»ºè®®
                    writing_guide = subsection.get('writing_guide', '')
                    if writing_guide:
                        formatted += f"å†™ä½œå»ºè®®: {writing_guide}\n"
            
            formatted += "\n"
            
    elif isinstance(chapters, dict):
        # å¤„ç†å­—å…¸æ ¼å¼çš„ç« èŠ‚ (åŸæœ‰é€»è¾‘)
        for chapter_id, chapter in chapters.items():
            formatted += f"# ç¬¬{chapter_id}ç« ï¼š{chapter.get('title', 'æœªå‘½åç« èŠ‚')}\n"
            formatted += f"ç« èŠ‚å†…å®¹æŒ‡å¼•: {chapter.get('content_guide', 'æ— æŒ‡å¼•')}\n"
            
            # æ·»åŠ å…³é”®è¯
            keywords = chapter.get("keywords", [])
            if keywords:
                formatted += f"å…³é”®è¯: {', '.join(keywords)}\n"
            
            # æ·»åŠ ç ”ç©¶é¢†åŸŸ
            research_focus = chapter.get("research_focus", [])
            if research_focus:
                formatted += f"ç ”ç©¶é¢†åŸŸ: {', '.join(research_focus)}\n"
            
            # æ·»åŠ å­ç« èŠ‚
            subsections = chapter.get("subsections", {})
            if isinstance(subsections, dict):
                for sub_id, subsection in subsections.items():
                    formatted += f"## {sub_id} {subsection.get('title', 'æœªå‘½åå­ç« èŠ‚')}\n"
                    formatted += f"å†…å®¹æ¦‚è¦: {subsection.get('content_guide', 'æ— æ¦‚è¦')}\n"
                    
                    # å…³é”®è¦ç‚¹
                    key_points = subsection.get('key_points', [])
                    if key_points:
                        formatted += f"å…³é”®è¦ç‚¹: {', '.join(key_points)}\n"
                    
                    # å†™ä½œå»ºè®®
                    writing_guide = subsection.get('writing_guide', '')
                    if writing_guide:
                        formatted += f"å†™ä½œå»ºè®®: {writing_guide}\n"
            elif isinstance(subsections, list):
                # å¤„ç†åˆ—è¡¨æ ¼å¼çš„å­ç« èŠ‚
                for subsection in subsections:
                    sub_id = subsection.get('id', '?')
                    sub_title = subsection.get('title', 'æœªå‘½åå­ç« èŠ‚')
                    sub_content_guide = subsection.get('content_guide', 'æ— æ¦‚è¦')
                    
                    formatted += f"## {sub_id} {sub_title}\n"
                    formatted += f"å†…å®¹æ¦‚è¦: {sub_content_guide}\n"
                    
                    # å…³é”®è¦ç‚¹
                    key_points = subsection.get('key_points', [])
                    if key_points:
                        formatted += f"å…³é”®è¦ç‚¹: {', '.join(key_points)}\n"
                    
                    # å†™ä½œå»ºè®®
                    writing_guide = subsection.get('writing_guide', '')
                    if writing_guide:
                        formatted += f"å†™ä½œå»ºè®®: {writing_guide}\n"
            
            formatted += "\n"
    else:
        formatted += "ç« èŠ‚ä¿¡æ¯æ ¼å¼å¼‚å¸¸\n"
    
    return formatted


def _format_global_outline_for_prompt(global_summary: Dict) -> str:
    """
    å°†å…¨å±€æ¦‚è§ˆæ ¼å¼åŒ–ä¸ºé€‚åˆLLMç†è§£çš„æ–‡æœ¬
    æ§åˆ¶è¾“å‡ºé•¿åº¦ï¼Œçªå‡ºç»“æ„å…³ç³»
    """
    if not global_summary or not global_summary.get("chapters"):
        return "æ— å…¨å±€ç»“æ„ä¿¡æ¯"
    
    formatted_text = ""
    chapters = global_summary.get("chapters", {})
    
    # æŒ‰ç« èŠ‚IDæ’åº
    sorted_chapters = sorted(chapters.items(), key=lambda x: str(x[0]))
    
    for chapter_id, chapter_info in sorted_chapters:
        chapter_title = chapter_info.get("title", "")
        content_guide = chapter_info.get("content_guide", "")
        
        # æ·»åŠ ç« èŠ‚ä¿¡æ¯
        formatted_text += f"ç¬¬{chapter_id}ç« : {chapter_title}\n"
        
        # æ·»åŠ ç®€åŒ–çš„å†…å®¹æŒ‡å¼•ï¼ˆæˆªå–å‰200å­—ç¬¦ï¼‰
        if content_guide:
            short_guide = content_guide[:200] + "..." if len(content_guide) > 200 else content_guide
            formatted_text += f"  æ ¸å¿ƒå†…å®¹: {short_guide}\n"
        
        # æ·»åŠ å­ç« èŠ‚æ ‡é¢˜
        subsections = chapter_info.get("subsections", {})
        if subsections:
            formatted_text += "  å­ç« èŠ‚: "
            subsection_titles = []
            for subsection_id, subsection_info in sorted(subsections.items(), key=lambda x: str(x[0])):
                subsection_title = subsection_info.get("title", "")
                if subsection_title:
                    subsection_titles.append(f"{subsection_id} {subsection_title}")
            formatted_text += " | ".join(subsection_titles) + "\n"
        
        formatted_text += "\n"
    
    return formatted_text.strip()


def _extract_figure_caption(content: str) -> str:
    """ä»å›¾ç‰‡å†…å®¹ä¸­æå–captionéƒ¨åˆ†"""
    if not content:
        return ""
    
    # å›¾ç‰‡contentæ ¼å¼: "... Content: ..."æˆ‘ä»¬åªéœ€è¦Contentä¹‹å‰çš„éƒ¨åˆ†
    if " Content:" in content:
        return content.split(" Content:")[0].strip()
    else:
        return content.strip()

def _extract_table_caption(content: str) -> str:
    """ä»è¡¨æ ¼å†…å®¹ä¸­æå–captionéƒ¨åˆ†"""
    if not content:
        return ""
    
    # å›¾ç‰‡contentæ ¼å¼: "... Content: ..."æˆ‘ä»¬åªéœ€è¦Contentä¹‹å‰çš„éƒ¨åˆ†
    if " Content:" in content:
        return content.split(" Content:")[0].strip()
    else:
        return content.strip()

def _extract_quality_evaluation_writing(content: str) -> Dict:
    """æå–å†…å®¹è´¨é‡è¯„ä¼°ä¿¡æ¯"""
    evaluation = {"scores": {}}
    
    # è¯„ä¼°ç»´åº¦æ˜ å°„
    dimensions = {
        "å­¦æœ¯ä¸¥è°¨æ€§": "academic_rigor",
        "å†…å®¹å®Œæ•´æ€§": "content_completeness", 
        "æ–‡çŒ®èåˆåº¦": "literature_integration",
        "å¤šæ¨¡æ€ææ–™å¼•ç”¨": "multimodal_material_citation",
        "å›¾è¡¨åˆ†ææ·±åº¦": "chart_analysis_depth",
        "è®ºè¿°æ·±åº¦": "argument_depth",
        "è¡¨è¾¾è´¨é‡": "expression_quality"
    }
    
    # æå–å„ç»´åº¦è¯„åˆ†
    for chinese_name, english_key in dimensions.items():
        pattern = rf"{chinese_name}:\s*(\d+(?:\.\d+)?)/10"
        match = re.search(pattern, content)
        if match:
            evaluation["scores"][english_key] = float(match.group(1))
    
    # æå–ç»¼åˆè´¨é‡è¯„åˆ†
    overall_pattern = r"ç»¼åˆè´¨é‡:\s*(\d+(?:\.\d+)?)/10"
    match = re.search(overall_pattern, content)
    if match:
        evaluation["scores"]["overall_quality"] = float(match.group(1))
    
    return evaluation

def _extract_difference_analysis(content: str) -> Dict:
    """æå–å¢é‡æ”¹è¿›åˆ†æ"""
    analysis = {}
    
    # æå–å†…å®¹å¢é‡åº¦ç™¾åˆ†æ¯”
    increment_pattern = r"å†…å®¹å¢é‡åº¦:\s*(\d+(?:\.\d+)?)%"
    match = re.search(increment_pattern, content)
    if match:
        analysis["increment_rate"] = float(match.group(1)) / 100
    
    # å…¼å®¹æ—§æ ¼å¼çš„å·®å¼‚åº¦
    diff_pattern = r"å†…å®¹å·®å¼‚åº¦:\s*(\d+(?:\.\d+)?)%"
    match = re.search(diff_pattern, content)
    if match:
        analysis["difference_rate"] = float(match.group(1)) / 100
    
    # æå–ä¸»è¦æ–°å¢ç‚¹
    additions_pattern = r"ä¸»è¦æ–°å¢ç‚¹:\s*([^\n]+)"
    match = re.search(additions_pattern, content)
    if match:
        analysis["main_additions"] = match.group(1).strip()
    
    # å…¼å®¹æ—§æ ¼å¼çš„ä¸»è¦å˜åŒ–ç‚¹
    changes_pattern = r"ä¸»è¦å˜åŒ–ç‚¹:\s*([^\n]+)"
    match = re.search(changes_pattern, content)
    if match:
        analysis["main_changes"] = match.group(1).strip()
    
    # æå–è´¨é‡å¢å€¼åº¦
    value_pattern = r"è´¨é‡å¢å€¼åº¦:\s*([^\n]+)"
    match = re.search(value_pattern, content)
    if match:
        analysis["quality_value_added"] = match.group(1).strip()
    
    # å…¼å®¹æ—§æ ¼å¼çš„è´¨é‡æå‡åº¦
    quality_pattern = r"è´¨é‡æå‡åº¦:\s*([^\n]+)"
    match = re.search(quality_pattern, content)
    if match:
        analysis["quality_improvement"] = match.group(1).strip()
    
    # æå–æ–°å¢å¼•ç”¨ç»Ÿè®¡
    citation_pattern = r"æ–°å¢å¼•ç”¨ç»Ÿè®¡:\s*([^\n]+)"
    match = re.search(citation_pattern, content)
    if match:
        analysis["new_citations"] = match.group(1).strip()
    
    return analysis

def _extract_iteration_decision(content: str) -> bool:
    """æå–è¿­ä»£å†³ç­–"""
    decision_pattern = r"ã€æ˜¯å¦ç»§ç»­è¿­ä»£ã€‘\s*(æ˜¯|å¦)"
    match = re.search(decision_pattern, content)
    if match:
        decision = match.group(1).strip()
        return decision == "æ˜¯"
    return False

def _clean_numeric_content(content: str, threshold: float = 0.1) -> str:
    """
    æ¸…ç†åŒ…å«å¤§é‡æ•°å­—çš„å†…å®¹
    
    Args:
        content: è¦æ£€æŸ¥çš„å†…å®¹
        threshold: æ•°å­—å æ¯”é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼å°†æ¸…ç†æ•°å­—ï¼ˆé»˜è®¤30%ï¼‰
    
    Returns:
        æ¸…ç†åçš„å†…å®¹
    """
    if not content:
        return content
    
    # ç»Ÿè®¡æ•°å­—å­—ç¬¦æ•°é‡
    digit_count = sum(1 for char in content if char.isdigit())
    total_chars = len(content)
    
    if total_chars == 0:
        return content
    
    digit_ratio = digit_count / total_chars
    
    # å¦‚æœæ•°å­—å æ¯”è¶…è¿‡é˜ˆå€¼ï¼Œåˆ™æ™ºèƒ½æ¸…ç†æ•°å­—å’Œç›¸å…³æ— æ„ä¹‰ç¬¦å·
    if digit_ratio > threshold:
        import re
        
        # æ­¥éª¤1: ç§»é™¤æ•°å­—
        cleaned_content = re.sub(r'\d', '', content)
        
        # æ­¥éª¤2: æ¸…ç†ç”±æ•°å­—ç§»é™¤å¯¼è‡´çš„æ— æ„ä¹‰æ ‡ç‚¹ç¬¦å·
        # ç§»é™¤è¿ç»­çš„ç‚¹ã€é€—å·ã€ç ´æŠ˜å·ç­‰æ ‡ç‚¹ç¬¦å·
        cleaned_content = re.sub(r'[.,\-_]+\s*[.,\-_]*', ' ', cleaned_content)
        
        # æ­¥éª¤3: æ¸…ç†å¤šä½™çš„æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼ç»„åˆ
        # ç§»é™¤å•ç‹¬çš„æ ‡ç‚¹ç¬¦å·ï¼ˆè¢«ç©ºæ ¼åŒ…å›´çš„ï¼‰
        cleaned_content = re.sub(r'\s+[.,\-_:;]+\s+', ' ', cleaned_content)
        
        # æ­¥éª¤4: æ¸…ç†è¡Œé¦–è¡Œå°¾çš„æ ‡ç‚¹ç¬¦å·
        lines = cleaned_content.split('\n')
        cleaned_lines = []
        for line in lines:
            # ç§»é™¤è¡Œé¦–è¡Œå°¾çš„æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼
            line = re.sub(r'^[.,\-_:;\s]+|[.,\-_:;\s]+$', '', line.strip())
            # åªä¿ç•™æœ‰å®é™…å†…å®¹çš„è¡Œï¼ˆè‡³å°‘åŒ…å«ä¸€ä¸ªå­—æ¯ï¼‰
            if line and re.search(r'[a-zA-Z]', line):
                cleaned_lines.append(line)
        
        # æ­¥éª¤5: é‡æ–°ç»„åˆå¹¶æœ€ç»ˆæ¸…ç†
        cleaned_content = '\n'.join(cleaned_lines)
        # æ¸…ç†å¤šä½™çš„ç©ºæ ¼å’Œæ¢è¡Œ
        cleaned_content = re.sub(r'\s+', ' ', cleaned_content).strip()
        # ç§»é™¤å‰©ä½™çš„å­¤ç«‹æ ‡ç‚¹ç¬¦å·
        cleaned_content = re.sub(r'\s+[.,\-_:;]\s+', ' ', cleaned_content)
        
        return cleaned_content
    
    return content


def _format_materials_for_writing_prompt(numbered_materials: Dict, iteration: int = 0) -> tuple:
    """
    æ ¼å¼åŒ–ææ–™ç”¨äºå†™ä½œæç¤ºè¯ï¼ŒåŸºäºè¿­ä»£æ¬¡æ•°é‡æ–°ç¼–å·ç¡®ä¿è¿ç»­æ€§
    
    Args:
        numbered_materials: ç¼–å·ææ–™å­—å…¸
        iteration: å½“å‰è¿­ä»£æ¬¡æ•°ï¼ˆ0è¡¨ç¤ºåˆå§‹å†™ä½œï¼Œ1ã€2ã€3è¡¨ç¤ºåç»­è¿­ä»£ï¼‰
        
    Returns:
        tuple: (æ ¼å¼åŒ–çš„ææ–™å­—ç¬¦ä¸², é‡æ–°ç¼–å·çš„ææ–™å­—å…¸)
    """
    
    
    if not numbered_materials:
        return "âš ï¸ å½“å‰æ²¡æœ‰å¯ç”¨çš„ç ”ç©¶ææ–™\n", {}
    
    # ğŸ†• å®šä¹‰æ¯è½®è¿­ä»£çš„ææ–™æ•°é‡é…ç½®
    materials_per_iteration = {
        "text": 60,      # æ¯è½®æ–‡æœ¬ææ–™æ•°é‡
        "equation": 10,  # æ¯è½®å…¬å¼ææ–™æ•°é‡  
        "figure": 20,    # æ¯è½®å›¾ç‰‡ææ–™æ•°é‡
        "table": 20      # æ¯è½®è¡¨æ ¼ææ–™æ•°é‡
    }
    
    # æŒ‰ç±»å‹åˆ†ç»„
    text_materials = []
    equation_materials = []
    figure_materials = []
    table_materials = []
    
    # ğŸ” åˆ†ææ¯ä¸ªç¼–å·ææ–™çš„ç±»å‹
    type_distribution = {}
    
    for material_id, material_info in numbered_materials.items():
        material_type = material_info.get("type", "text")
        type_distribution[material_type] = type_distribution.get(material_type, 0) + 1
        
        if material_type == "text":
            text_materials.append((material_id, material_info))
        elif material_type == "equation":
            equation_materials.append((material_id, material_info))
        elif material_type == "figure":
            figure_materials.append((material_id, material_info))
        elif material_type == "table":
            table_materials.append((material_id, material_info))

    # ğŸ†• è®¡ç®—æ¯ç§ææ–™ç±»å‹çš„èµ·å§‹ç¼–å·
    text_start_num = iteration * materials_per_iteration["text"] + 1
    equation_start_num = iteration * materials_per_iteration["equation"] + 1
    figure_start_num = iteration * materials_per_iteration["figure"] + 1
    table_start_num = iteration * materials_per_iteration["table"] + 1
    
    
    formatted = ""
    # ğŸ†• åˆ›å»ºé‡æ–°ç¼–å·çš„ææ–™å­—å…¸ï¼Œç”¨äºåç»­çš„å¼•ç”¨æ˜ å°„
    renumbered_materials = {}
    
    # æ˜¾ç¤ºæ–‡æœ¬ææ–™ï¼ˆä½¿ç”¨è¿ç»­ç¼–å·ï¼‰
    if text_materials:
        formatted += "\n **æ–‡æœ¬ææ–™**\n"
        for i, (material_id, material_info) in enumerate(text_materials):
            new_material_num = text_start_num + i
            new_material_key = f"æ–‡æœ¬{new_material_num}"
            relevance = material_info.get("relevance_score", 0)
            paper = material_info.get("paper", "æœªçŸ¥æ¥æº")
            content = material_info.get("content", "")
            display_content = content[:2000] + "..." if len(content) > 2000 else content
            formatted += f"{new_material_key} (ç›¸å…³åº¦: {relevance:.2f}, æ¥æºID: {paper[:3]}):\n{display_content}\n\n"
            
            # ğŸ†• æ·»åŠ åˆ°é‡æ–°ç¼–å·çš„å­—å…¸ä¸­
            renumbered_materials[new_material_key] = material_info
    
    # æ˜¾ç¤ºå…¬å¼ææ–™ï¼ˆä½¿ç”¨è¿ç»­ç¼–å·ï¼‰
    if equation_materials:
        formatted += "\n **ç›¸å…³å…¬å¼**\n"
        for i, (material_id, material_info) in enumerate(equation_materials):
            new_material_num = equation_start_num + i
            new_material_key = f"å…¬å¼{new_material_num}"
            relevance = material_info.get("relevance_score", 0)
            paper = material_info.get("paper", "æœªçŸ¥æ¥æº")
            content = material_info.get("content", "")
            formatted += f"{new_material_key} (ç›¸å…³åº¦: {relevance:.2f}, æ¥æºID: {paper[:3]}):\n{content}\n\n"
            
            # ğŸ†• æ·»åŠ åˆ°é‡æ–°ç¼–å·çš„å­—å…¸ä¸­
            renumbered_materials[new_material_key] = material_info
    
    # æ˜¾ç¤ºå›¾ç‰‡ææ–™ï¼ˆä½¿ç”¨è¿ç»­ç¼–å·ï¼‰
    if figure_materials:
        formatted += "\n **å›¾ç‰‡èµ„æ–™**\n"
        for i, (material_id, material_info) in enumerate(figure_materials):
            new_material_num = figure_start_num + i
            new_material_key = f"å›¾ç‰‡{new_material_num}"
            relevance = material_info.get("relevance_score", 0)
            paper = material_info.get("paper", "æœªçŸ¥æ¥æº")
            content = material_info.get("content", "")
            display_content = content[:1000] + "..." if len(content) > 1000 else content
            formatted += f"{new_material_key} (ç›¸å…³åº¦: {relevance:.2f}, æ¥æºID: {paper[:3]}):\n{display_content}\n\n"
            
            # ğŸ†• æ·»åŠ åˆ°é‡æ–°ç¼–å·çš„å­—å…¸ä¸­
            renumbered_materials[new_material_key] = material_info
    
    # æ˜¾ç¤ºè¡¨æ ¼ææ–™ï¼ˆä½¿ç”¨è¿ç»­ç¼–å·ï¼‰
    if table_materials:
        formatted += "\n **è¡¨æ ¼æ•°æ®**\n"
        for i, (material_id, material_info) in enumerate(table_materials):
            new_material_num = table_start_num + i
            new_material_key = f"è¡¨æ ¼{new_material_num}"
            relevance = material_info.get("relevance_score", 0)
            paper = material_info.get("paper", "æœªçŸ¥æ¥æº")
            content = material_info.get("content", "")
            display_content = content[:1000] + "..." if len(content) > 1000 else content
            # æ¸…ç†å¤§é‡æ•°å­—çš„è¡¨æ ¼å†…å®¹
            display_content = _clean_numeric_content(display_content)
            formatted += f"{new_material_key} (ç›¸å…³åº¦: {relevance:.2f}, æ¥æºID: {paper[:3]}):\n{display_content}\n\n"
            
            # ğŸ†• æ·»åŠ åˆ°é‡æ–°ç¼–å·çš„å­—å…¸ä¸­
            renumbered_materials[new_material_key] = material_info
    
    # ææ–™ç»Ÿè®¡ä¿¡æ¯
    total_materials = len(text_materials) + len(equation_materials) + len(figure_materials) + len(table_materials)
    
    # ğŸ†• æ˜¾ç¤ºç¼–å·èŒƒå›´ä¿¡æ¯
    ranges_info = []
    if text_materials:
        ranges_info.append(f"æ–‡æœ¬{text_start_num}-{text_start_num + len(text_materials) - 1}")
    if equation_materials:
        ranges_info.append(f"å…¬å¼{equation_start_num}-{equation_start_num + len(equation_materials) - 1}")
    if figure_materials:
        ranges_info.append(f"å›¾ç‰‡{figure_start_num}-{figure_start_num + len(figure_materials) - 1}")
    if table_materials:
        ranges_info.append(f"è¡¨æ ¼{table_start_num}-{table_start_num + len(table_materials) - 1}")
            
    formatted += f"\nğŸ“Š **ææ–™ç»Ÿè®¡**: å…±{total_materials}æ¡ææ–™ (æ–‡æœ¬:{len(text_materials)}, å…¬å¼:{len(equation_materials)}, å›¾:{len(figure_materials)}, è¡¨æ ¼:{len(table_materials)})\n"
    formatted += f"ğŸ“Š **ç¼–å·èŒƒå›´**: {', '.join(ranges_info)}\n"
    
    return formatted, renumbered_materials

def _format_content_for_analysis( current_content: Dict) -> str:
    """æ ¼å¼åŒ–å½“å‰å†…å®¹ç”¨äºåˆ†æï¼Œç±»ä¼¼Enricherçš„_format_enrichment_for_analysis"""
    if not current_content:
        return "âš ï¸ å½“å‰æ²¡æœ‰å†…å®¹"
    
    content_text = current_content.get("content", "")
    if not content_text:
        return "âš ï¸ å†…å®¹ä¸ºç©º"
    
    # æ˜¾ç¤ºå†…å®¹åŸºæœ¬ä¿¡æ¯
    word_count = len(content_text.split())
    status = current_content.get("status", "unknown")
    materials_used = current_content.get("materials_used", 0)
    iterations = current_content.get("iterations_completed", 0)
    
    formatted = f"""

ã€å½“å‰ç« èŠ‚å†…å®¹ã€‘
{content_text}

ã€è´¨é‡è¯„åˆ†å†å²ã€‘
"""
    
    quality_scores = current_content.get("quality_scores", {})
    if quality_scores:
        for dimension, score in quality_scores.items():
            formatted += f"{dimension}: {score}/10\n"
    else:
        formatted += "æ— å†å²è¯„åˆ†è®°å½•\n"
    
    return formatted

def _format_global_context_for_analysis(main_topic: str, subtopics: List[str] = None, global_outline_summary: Dict = None) -> str:
    """æ ¼å¼åŒ–å…¨å±€ç»¼è¿°æ¡†æ¶ç”¨äºåˆ†æ"""
    formatted = ""
    
    # æ·»åŠ å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯
    if main_topic:
        formatted += f"ç»¼è¿°ä¸»é¢˜: {main_topic}\n"
        if subtopics:
            formatted += f"ç»¼è¿°å­ä¸»é¢˜: {', '.join(subtopics)}\n"
    
    # æ·»åŠ å…¨å±€ç»“æ„æ¦‚è§ˆ
    if global_outline_summary:
        formatted += f"""
ã€å…¨å±€ç»¼è¿°ç»“æ„æ¦‚è§ˆã€‘
è¯·æ³¨æ„ï¼Œä½ æ­£åœ¨ä¼˜åŒ–çš„æ˜¯ä¸€ä¸ªå®Œæ•´ç»¼è¿°çš„ä¸€éƒ¨åˆ†ã€‚ä»¥ä¸‹æ˜¯æ•´ä¸ªç»¼è¿°çš„ç»“æ„æ¦‚è§ˆï¼Œè¯·ç¡®ä¿ä½ çš„å†…å®¹ä¸æ•´ä½“ç»“æ„ä¿æŒä¸€è‡´ï¼š

ç»¼è¿°æ€»ä½“æ¡†æ¶ï¼š
{_format_global_outline_for_prompt(global_outline_summary)}

ã€ç»“æ„ä¸€è‡´æ€§è¦æ±‚ã€‘
1. é¿å…ä¸å…¶ä»–ç« èŠ‚å†…å®¹é‡å¤ï¼Œä¿æŒå†…å®¹è¾¹ç•Œæ¸…æ™°
2. é€‚å½“æåŠä¸å…¶ä»–ç« èŠ‚çš„é€»è¾‘å…³ç³»
3. ç¡®ä¿å†…å®¹æ·±åº¦å’Œè¯¦ç»†ç¨‹åº¦ä¸æ•´ä¸ªç»¼è¿°çš„å­¦æœ¯æ°´å¹³ä¿æŒä¸€è‡´
4. æ³¨æ„å½“å‰ç« èŠ‚åœ¨æ•´ä½“ç»“æ„ä¸­çš„ä½ç½®å’Œä½œç”¨
"""
    else:
        formatted += "æ— å…¨å±€ç»“æ„ä¿¡æ¯\n"
    
    return formatted

def _parse_writing_refinement_response(response: str, iteration: int, current_content: Dict, renumbered_materials: Dict = None) -> Dict:
    """è§£æå†™ä½œä¼˜åŒ–å“åº”ï¼Œæå–ä¼˜åŒ–åå†…å®¹å’Œå†³ç­–ä¿¡æ¯"""
    try:
        # æŸ¥æ‰¾ä¼˜åŒ–ç»“æœéƒ¨åˆ†
        start_marker = "===å†™ä½œä¼˜åŒ–ç»“æœå¼€å§‹==="
        end_marker = "===å†™ä½œä¼˜åŒ–ç»“æœç»“æŸ==="
        
        start_idx = response.find(start_marker)
        if start_idx != -1:
            end_idx = response.find(end_marker)
            if end_idx != -1:
                result_section = response[start_idx + len(start_marker):end_idx].strip()
                print(f"âœ… ç¬¬{iteration}è½®ï¼šæ‰¾åˆ°å®Œæ•´çš„ä¼˜åŒ–ç»“æœæ ‡è®°")
            else:
                result_section = response[start_idx + len(start_marker):].strip()
                print(f"âš ï¸ ç¬¬{iteration}è½®ï¼šåªæ‰¾åˆ°å¼€å§‹æ ‡è®°ï¼Œä»æ­¤å¤„å¼€å§‹è§£æ")
        else:
            print(f"â„¹ï¸ ç¬¬{iteration}è½®ï¼šæœªæ‰¾åˆ°ä¼˜åŒ–ç»“æœæ ‡è®°ï¼Œç›´æ¥è§£ææ•´ä¸ªå“åº”")
            result_section = response
        
        # æå–ä¼˜åŒ–åçš„ç« èŠ‚å†…å®¹
        content_start = "===ç« èŠ‚å†…å®¹å¼€å§‹==="
        content_end = "===ç« èŠ‚å†…å®¹ç»“æŸ==="
        
        content_start_idx = result_section.find(content_start)
        if content_start_idx != -1:
            content_end_idx = result_section.find(content_end)
            if content_end_idx != -1:
                optimized_content = result_section[content_start_idx + len(content_start):content_end_idx].strip()
                print(f"âœ… ç¬¬{iteration}è½®ï¼šæˆåŠŸæå–ä¼˜åŒ–åå†…å®¹")
            else:
                print(f"âš ï¸ ç¬¬{iteration}è½®ï¼šç¼ºå°‘å†…å®¹ç»“æŸæ ‡è®°")
                optimized_content = current_content.get("content", "")
        else:
            print(f"âš ï¸ ç¬¬{iteration}è½®ï¼šç¼ºå°‘å†…å®¹å¼€å§‹æ ‡è®°")
            optimized_content = current_content.get("content", "")
        
        # æå–æ˜¯å¦ç»§ç»­è¿­ä»£çš„å†³ç­–
        should_continue = _extract_iteration_decision(result_section)
        
        # æå–å·®å¼‚åº¦åˆ†æ
        difference_analysis = _extract_difference_analysis(result_section)
        
        # æå–è´¨é‡è¯„ä¼°
        quality_evaluation = _extract_quality_evaluation_writing(result_section)
        
        # ğŸ†• å¦‚æœæä¾›äº†é‡æ–°ç¼–å·çš„ææ–™å­—å…¸ï¼Œè¿›è¡Œå¼•ç”¨æ˜ å°„
        citation_mapping = {}
        if renumbered_materials and optimized_content:
            from utils import extract_citation_mapping
            citation_mapping = extract_citation_mapping(optimized_content, renumbered_materials)
        
        # æ„å»ºç»“æœ
        result = {
            "content": {
                "content": optimized_content,
                "status": "success",
                "materials_used": current_content.get("materials_used", 0) + 100,  # å¢åŠ 100ä¸ªæ–°ææ–™
                "iterations_completed": iteration,
                "quality_scores": quality_evaluation.get("scores", current_content.get("quality_scores", {})),
                "citation_mapping": citation_mapping  # ğŸ†• æ·»åŠ å¼•ç”¨æ˜ å°„ä¿¡æ¯
            },
            "should_continue": should_continue,
            "difference_analysis": difference_analysis,
            "quality_evaluation": quality_evaluation
        }
        
        return result
        
    except Exception as e:
        print(f"âŒ ç¬¬{iteration}è½®ä¼˜åŒ–ç»“æœè§£æå¤±è´¥: {str(e)}")
        return None


def _extract_scientific_enrichment_decision(response: str, iteration: int) -> Dict:
    """æå–ç§‘å­¦å†³ç­–ä¾æ®"""
    decision_info = {}
    try:
        # æå–æ˜¯å¦ç»§ç»­è¿­ä»£çš„å†³ç­–
        decision_match = re.search(r'ã€æ˜¯å¦ç»§ç»­è¿­ä»£ã€‘\s*\n?(.+?)(?=\nã€|\n\n|$)', response, re.DOTALL)
        should_continue = False
        if decision_match:
            decision_text = decision_match.group(1).strip()
            should_continue = "æ˜¯" in decision_text
            decision_info["decision_text"] = decision_text
            pass
        
        # æå–ç§‘å­¦å†³ç­–ä¾æ®
        basis_match = re.search(r'ã€ç§‘å­¦å†³ç­–ä¾æ®ã€‘\s*(.*?)(?=\n===|$)', response, re.DOTALL)
        if basis_match:
            basis_content = basis_match.group(1).strip()
            print(f"âœ“ æ‰¾åˆ°ç§‘å­¦å†³ç­–ä¾æ®éƒ¨åˆ†ï¼Œé•¿åº¦: {len(basis_content)}å­—ç¬¦")
            
            # æå–é‡åŒ–æŒ‡æ ‡
            metrics_section = re.search(r'å†³ç­–é‡åŒ–æŒ‡æ ‡:\s*(.*?)(?=å†³ç­–é€»è¾‘:|$)', basis_content, re.DOTALL)
            if metrics_section:
                metrics_content = metrics_section.group(1).strip()
                print(f"âœ“ æ‰¾åˆ°é‡åŒ–æŒ‡æ ‡éƒ¨åˆ†: {metrics_content[:100]}...")
                
                # æå–å…·ä½“æŒ‡æ ‡ - ğŸ”§ ä¿®å¤ï¼šæ”¯æŒå°æ•°è¯„åˆ†
                improvement_score = re.search(r'ç»¼åˆæ”¹è¿›è¯„åˆ†:\s*(\d+\.?\d*)/10', metrics_content)
                if improvement_score:
                    decision_info["improvement_score"] = float(improvement_score.group(1))
                    print(f"âœ“ æˆåŠŸæå–ç»¼åˆæ”¹è¿›è¯„åˆ†: {improvement_score.group(1)}")
                else:
                    print(f"âŒ åœ¨é‡åŒ–æŒ‡æ ‡ä¸­æœªæ‰¾åˆ°ç»¼åˆæ”¹è¿›è¯„åˆ†æ¨¡å¼")
                    print(f"   é‡åŒ–æŒ‡æ ‡å†…å®¹: {metrics_content}")
                
                guidance_ratio = re.search(r'æ–°å¢æŒ‡å¯¼ä»·å€¼å æ¯”:\s*(\d+\.?\d*)%', metrics_content)
                if guidance_ratio:
                    decision_info["guidance_ratio"] = float(guidance_ratio.group(1))
                    print(f"âœ“ æˆåŠŸæå–æŒ‡å¯¼ä»·å€¼å æ¯”: {guidance_ratio.group(1)}%")
            else:
                print(f"âŒ æœªæ‰¾åˆ°'å†³ç­–é‡åŒ–æŒ‡æ ‡:'éƒ¨åˆ†")
                print(f"   å†³ç­–ä¾æ®å†…å®¹: {basis_content[:200]}...")
            
            # æå–å†³ç­–é€»è¾‘
            logic_match = re.search(r'å†³ç­–é€»è¾‘:\s*(.*?)(?=ä¸»è¦æ”¹è¿›ç‚¹:|$)', basis_content, re.DOTALL)
            if logic_match:
                decision_info["logic"] = logic_match.group(1).strip()
        else:
            print(f"âŒ æœªæ‰¾åˆ°ã€ç§‘å­¦å†³ç­–ä¾æ®ã€‘æ ‡è®°")
            print(f"   å“åº”é•¿åº¦: {len(response)}å­—ç¬¦")
            print(f"   å“åº”å†…å®¹é¢„è§ˆ: {response[:300]}...")
        
        decision_info["should_continue"] = should_continue
        
    except Exception as e:
        print(f"âš ï¸ ç¬¬{iteration}è½®ï¼šè§£æç§‘å­¦å†³ç­–æ—¶å‡ºé”™: {e}")
        # å›é€€åˆ°ç®€å•çš„å†³ç­–æå–
        if "ã€æ˜¯å¦ç»§ç»­è¿­ä»£ã€‘" in response:
            decision_info["should_continue"] = "æ˜¯" in response[response.find("ã€æ˜¯å¦ç»§ç»­è¿­ä»£ã€‘"):response.find("ã€æ˜¯å¦ç»§ç»­è¿­ä»£ã€‘")+20]
    
    return decision_info



def _format_materials_for_enrichment(materials: List) -> str:
    """æ ¼å¼åŒ–ææ–™ç”¨äºä¸°å¯Œåˆ†æ"""
    if not materials:
        return "æ— æ–°ææ–™"
    
    # ğŸ†• ä½¿ç”¨max(100, len(materials))æ¥å†³å®šè¿”å›çš„ææ–™æ•°é‡ï¼Œå°½å¯èƒ½å¤šè¿”å›å‚è€ƒèµ„æ–™
    material_count = max(100, len(materials))
    actual_materials = materials[:material_count]
    
    formatted = f"å…±{len(actual_materials)}æ¡æ–°ææ–™:\n"
    formatted += "âš ï¸ æ³¨æ„ï¼šææ–™ç¼–å·ä»…ç”¨äºå±•ç¤ºï¼Œåœ¨åˆ†æå’Œä¼˜åŒ–å†…å®¹æ—¶è¯·ä¸è¦å¼•ç”¨å…·ä½“çš„ææ–™ç¼–å·ï¼\n\n"
    
    # ğŸ†• ä¸åˆå§‹å¤§çº²ç”Ÿæˆä¿æŒä¸€è‡´çš„æ ¼å¼åŒ–æ–¹å¼
    for i, material in enumerate(actual_materials, 1):
        content = material.get('content', '')
        relevance_score = material.get('relevance_score', 0)
        
        # ğŸ†• ä¸ä¹‹å‰ä¿æŒä¸€è‡´ï¼šé•¿åº¦è¶…è¿‡1000åˆ™åªå–å‰1000ï¼Œä¸è¶…è¿‡åˆ™ç›´æ¥ä½¿ç”¨
        if len(content) > 2000:
            formatted += f"ææ–™{i} (ç›¸å…³åº¦: {relevance_score:.2f}): {content[:2000]}...\n\n"
        else:
            formatted += f"ææ–™{i} (ç›¸å…³åº¦: {relevance_score:.2f}): {content}\n\n"
    
    return formatted

def _extract_content_quality_evaluation(response: str, iteration: int) -> Dict:
    """æå–å¤šç»´åº¦å†…å®¹è´¨é‡è¯„ä¼°ä¿¡æ¯"""
    evaluation = {}
    try:
        # æŸ¥æ‰¾è´¨é‡è¯„ä¼°éƒ¨åˆ†
        eval_match = re.search(r'ã€å¤šç»´åº¦å†…å®¹è´¨é‡è¯„ä¼°ã€‘\s*(.*?)(?=\nã€|$)', response, re.DOTALL)
        if eval_match:
            eval_content = eval_match.group(1).strip()
            
            # æå–å„ç»´åº¦è¯„åˆ†
            dimensions = ["å†™ä½œæŒ‡å¯¼å®Œæ•´æ€§", "å…³é”®è¯æ£€ç´¢ç²¾å‡†æ€§", "å­¦æœ¯æ·±åº¦é€‚å®œæ€§", "ç»“æ„é€»è¾‘åˆç†æ€§", "å®ç”¨æŒ‡å¯¼ä»·å€¼"]
            for dim in dimensions:
                score_match = re.search(rf'{dim}:\s*(\d+)/10', eval_content)
                if score_match:
                    evaluation[dim] = int(score_match.group(1))
            
            # æå–ç»¼åˆè¯„åˆ†
            overall_match = re.search(r'ç»¼åˆè´¨é‡è¯„åˆ†:\s*(\d+\.?\d*)/10', eval_content)
            if overall_match:
                evaluation["overall_score"] = float(overall_match.group(1))
        
    except Exception as e:
        print(f"âš ï¸ ç¬¬{iteration}è½®ï¼šè§£æå†…å®¹è´¨é‡è¯„ä¼°æ—¶å‡ºé”™: {e}")
    
    return evaluation

def _extract_material_value_analysis(response: str, iteration: int) -> Dict:
    """æå–æ–°ææ–™ä»·å€¼åˆ†æ"""
    analysis = {}
    try:
        # æŸ¥æ‰¾æ–°ææ–™ä»·å€¼åˆ†æéƒ¨åˆ†
        analysis_match = re.search(r'ã€æ–°ææ–™ä»·å€¼åˆ†æã€‘\s*(.*?)(?=\nã€|$)', response, re.DOTALL)
        if analysis_match:
            analysis_content = analysis_match.group(1).strip()
            
            # æå–å„ä¸ªåˆ†æç»´åº¦
            fields = ["æŠ€æœ¯è´¡çŒ®è¯†åˆ«", "åº”ç”¨åœºæ™¯æ‰©å±•", "ç ”ç©¶çƒ­ç‚¹å‘ç°", "è¯„ä¼°æ ‡å‡†æ›´æ–°"]
            for field in fields:
                field_match = re.search(rf'{field}:\s*(.*?)(?=\n[^\s]|\n{field}|\næŠ€æœ¯|\nåº”ç”¨|\nç ”ç©¶|\nè¯„ä¼°|$)', analysis_content, re.DOTALL)
                if field_match:
                    analysis[field] = field_match.group(1).strip()
        
    except Exception as e:
        print(f"âš ï¸ ç¬¬{iteration}è½®ï¼šè§£ææ–°ææ–™ä»·å€¼åˆ†ææ—¶å‡ºé”™: {e}")
    
    return analysis

def _extract_keyword_optimization_analysis( response: str, iteration: int) -> Dict:
    """æå–å…³é”®è¯ä¼˜åŒ–æ½œåŠ›è¯„ä¼°"""
    optimization = {}
    try:
        # æŸ¥æ‰¾å…³é”®è¯ä¼˜åŒ–æ½œåŠ›è¯„ä¼°éƒ¨åˆ†
        opt_match = re.search(r'ã€å…³é”®è¯ä¼˜åŒ–æ½œåŠ›è¯„ä¼°ã€‘\s*(.*?)(?=\nã€|$)', response, re.DOTALL)
        if opt_match:
            opt_content = opt_match.group(1).strip()
            
            # æå–å„ä¸ªä¼˜åŒ–ç»´åº¦
            fields = ["æ£€ç´¢æ•ˆæœåˆ†æ", "æœ¯è¯­æ›´æ–°éœ€æ±‚", "è¦†ç›–åº¦ä¼˜åŒ–", "å·®å¼‚åŒ–ç¨‹åº¦"]
            for field in fields:
                field_match = re.search(rf'{field}:\s*(.*?)(?=\n[^\s]|\n{field}|\næ£€ç´¢|\næœ¯è¯­|\nè¦†ç›–|\nå·®å¼‚|$)', opt_content, re.DOTALL)
                if field_match:
                    optimization[field] = field_match.group(1).strip()
        
    except Exception as e:
        print(f"âš ï¸ ç¬¬{iteration}è½®ï¼šè§£æå…³é”®è¯ä¼˜åŒ–åˆ†ææ—¶å‡ºé”™: {e}")
    
    return optimization

def _extract_improvement_opportunities(response: str, iteration: int) -> Dict:
    """æå–å†…å®¹ç¼ºé™·ä¸æ”¹è¿›æœºä¼šè¯†åˆ«"""
    opportunities = {}
    try:
        # æŸ¥æ‰¾æ”¹è¿›æœºä¼šè¯†åˆ«éƒ¨åˆ†
        opp_match = re.search(r'ã€å†…å®¹ç¼ºé™·ä¸æ”¹è¿›æœºä¼šè¯†åˆ«ã€‘\s*(.*?)(?=\nã€|$)', response, re.DOTALL)
        if opp_match:
            opp_content = opp_match.group(1).strip()
            
            # æå–å„ä¸ªæ”¹è¿›ç»´åº¦
            fields = ["å†™ä½œæŒ‡å¯¼ç¼ºé™·", "å†…å®¹è§„åˆ’é—æ¼", "å…³é”®è¯ç­–ç•¥ä¸è¶³", "å­¦æœ¯ä»·å€¼æå‡"]
            for field in fields:
                field_match = re.search(rf'{field}:\s*(.*?)(?=\n[^\s]|\n{field}|\nå†™ä½œ|\nå†…å®¹|\nå…³é”®|\nå­¦æœ¯|$)', opp_content, re.DOTALL)
                if field_match:
                    opportunities[field] = field_match.group(1).strip()
        
    except Exception as e:
        print(f"âš ï¸ ç¬¬{iteration}è½®ï¼šè§£ææ”¹è¿›æœºä¼šæ—¶å‡ºé”™: {e}")
    
    return opportunities

def _extract_enrichment_improvement_assessment(response: str, iteration: int) -> Dict:
    """æå–æ”¹è¿›æ•ˆæœé‡åŒ–è¯„ä¼°"""
    assessment = {}

    return assessment

def _count_actual_citations(topic: str) -> int:
    """
    ä»å¼•ç”¨JSONæ–‡ä»¶ä¸­ç»Ÿè®¡å®é™…ä½¿ç”¨çš„å¼•ç”¨æ•°é‡
    
    Args:
        topic: ä¸»é¢˜åç§°ï¼Œç”¨äºå®šä½å¼•ç”¨æ–‡ä»¶
        
    Returns:
        å®é™…å¼•ç”¨çš„æ€»æ•°é‡
    """
    import json
    import os
    from datetime import datetime
    
    try:
        # ğŸ†• æŸ¥æ‰¾å·²å­˜åœ¨çš„å¼•ç”¨æ–‡ä»¶
        citations_dir = "./chapter_citations"
        safe_topic = "".join(c for c in topic if c.isalnum() or c in [' ', '_', '-']).rstrip()
        safe_topic = safe_topic.replace(' ', '_') if safe_topic else "ç»¼è¿°"
        
        # ğŸ†• æ™ºèƒ½æŸ¥æ‰¾å¼•ç”¨æ–‡ä»¶ï¼ˆä¼˜å…ˆä½¿ç”¨æ­£å¼æ–‡ä»¶ï¼Œé¿å…ä¸´æ—¶æ–‡ä»¶ï¼‰
        import glob
        
        # å…ˆæŸ¥æ‰¾æ­£å¼çš„å¼•ç”¨æ–‡ä»¶
        formal_pattern = f"{safe_topic}_citations_[0-9]*_[0-9]*.json"
        formal_path = os.path.join(citations_dir, formal_pattern)
        formal_files = glob.glob(formal_path)
        
        # æŸ¥æ‰¾æ‰€æœ‰å¼•ç”¨æ–‡ä»¶
        all_pattern = f"{safe_topic}_citations*.json"
        all_path = os.path.join(citations_dir, all_pattern)
        all_files = glob.glob(all_path)
        
        if formal_files:
            # ä¼˜å…ˆä½¿ç”¨æ­£å¼æ–‡ä»¶
            filepath = max(formal_files, key=os.path.getmtime)
        elif all_files:
            # å¦‚æœæ²¡æœ‰æ­£å¼æ–‡ä»¶ï¼Œä½¿ç”¨å…¶ä»–æ–‡ä»¶
            filepath = max(all_files, key=os.path.getmtime)
        else:
            return 0
        
        # è¯»å–å¼•ç”¨æ•°æ®å¹¶ç»Ÿè®¡
        with open(filepath, 'r', encoding='utf-8') as f:
            citation_data = json.load(f)
        
        total_citations = 0
        sections = citation_data.get("sections", {})
        
        for section_key, section_data in sections.items():
            detailed_citations = section_data.get("detailed_citations", {})
            total_citations += len(detailed_citations)
        
        return total_citations
        
    except Exception as e:
        print(f"âš ï¸ ç»Ÿè®¡å¼•ç”¨æ•°é‡æ—¶å‡ºé”™: {e}")
        return 0

def _debug_response_structure(response: str, iteration: int):
    """è°ƒè¯•å“åº”ç»“æ„ï¼Œè¾“å‡ºå…³é”®æ ‡è®°çš„ä½ç½®ä¿¡æ¯"""
    # markers = [
    #     "===ä¼˜åŒ–ç»“æœå¼€å§‹===",
    #     "===ä¼˜åŒ–ç»“æœç»“æŸ===", 
    #     "===å¤§çº²å¼€å§‹===",
    #     "===å¤§çº²ç»“æŸ===",
    #     "ã€å¤šç»´åº¦è´¨é‡è¯„ä¼°ã€‘",
    #     "ã€æ–°ææ–™å†…å®¹åˆ†æã€‘",
    #     "ã€æ”¹è¿›æ•ˆæœé‡åŒ–è¯„ä¼°ã€‘",
    #     "ã€æ˜¯å¦ç»§ç»­è¿­ä»£ã€‘",
    #     "ã€ç§‘å­¦å†³ç­–ä¾æ®ã€‘"
    # ]
    
    # ç®€åŒ–è°ƒè¯•è¾“å‡ºï¼Œåªåœ¨éœ€è¦æ—¶å¯ç”¨
    # print(f"ğŸ” ç¬¬{iteration}è½®ï¼šå“åº”ç»“æ„è°ƒè¯•")
    # print(f"   å“åº”æ€»é•¿åº¦: {len(response)} å­—ç¬¦")
    
    # for marker in markers:
    #     pos = response.find(marker)
    #     if pos != -1:
    #         print(f"   âœ… '{marker}' ä½ç½®: {pos}")
    #     else:
    #         print(f"   âŒ '{marker}' æœªæ‰¾åˆ°")
    return {}


def _extract_scientific_decision(response: str, iteration: int) -> Dict:
    """æå–ç§‘å­¦å†³ç­–ä¾æ®"""
    decision_info = {}
    try:
        # æå–æ˜¯å¦ç»§ç»­è¿­ä»£çš„å†³ç­–
        decision_match = re.search(r'ã€æ˜¯å¦ç»§ç»­è¿­ä»£ã€‘\s*\n?(.+?)(?=\nã€|\n\n|$)', response, re.DOTALL)
        should_continue = False
        if decision_match:
            decision_text = decision_match.group(1).strip()
            should_continue = "æ˜¯" in decision_text
            decision_info["decision_text"] = decision_text
        
        # æå–ç§‘å­¦å†³ç­–ä¾æ®
        basis_match = re.search(r'ã€ç§‘å­¦å†³ç­–ä¾æ®ã€‘\s*(.*?)(?=\n===|$)', response, re.DOTALL)
        if basis_match:
            basis_content = basis_match.group(1).strip()
            
            # æå–é‡åŒ–æŒ‡æ ‡
            metrics_section = re.search(r'å†³ç­–é‡åŒ–æŒ‡æ ‡:\s*(.*?)(?=å†³ç­–é€»è¾‘:|$)', basis_content, re.DOTALL)
            if metrics_section:
                metrics_content = metrics_section.group(1).strip()
                
                # æå–å…·ä½“æŒ‡æ ‡ - ğŸ”§ ä¿®å¤ï¼šæ”¯æŒå°æ•°è¯„åˆ†
                improvement_score = re.search(r'ç»¼åˆæ”¹è¿›è¯„åˆ†:\s*(\d+\.?\d*)/10', metrics_content)
                if improvement_score:
                    decision_info["improvement_score"] = float(improvement_score.group(1))
                
                content_ratio = re.search(r'æ–°å¢é‡è¦å†…å®¹å æ¯”:\s*(\d+\.?\d*)%', metrics_content)
                if content_ratio:
                    decision_info["content_ratio"] = float(content_ratio.group(1))
            
            # æå–å†³ç­–é€»è¾‘
            logic_match = re.search(r'å†³ç­–é€»è¾‘:\s*(.*?)(?=ä¸»è¦æ”¹è¿›ç‚¹:|$)', basis_content, re.DOTALL)
            if logic_match:
                decision_info["logic"] = logic_match.group(1).strip()
        
        decision_info["should_continue"] = should_continue
        
    except Exception as e:
        print(f"âš ï¸ ç¬¬{iteration}è½®ï¼šè§£æç§‘å­¦å†³ç­–æ—¶å‡ºé”™: {e}")
        # å›é€€åˆ°ç®€å•çš„å†³ç­–æå–
        if "ã€æ˜¯å¦ç»§ç»­è¿­ä»£ã€‘" in response:
            decision_info["should_continue"] = "æ˜¯" in response[response.find("ã€æ˜¯å¦ç»§ç»­è¿­ä»£ã€‘"):response.find("ã€æ˜¯å¦ç»§ç»­è¿­ä»£ã€‘")+20]
    
    return decision_info
    

def _extract_material_analysis(response: str, iteration: int) -> Dict:
    """æå–æ–°ææ–™å†…å®¹åˆ†æ"""
    analysis = {}
    try:
        # æŸ¥æ‰¾æ–°ææ–™åˆ†æéƒ¨åˆ†
        analysis_match = re.search(r'ã€æ–°ææ–™å†…å®¹åˆ†æã€‘\s*(.*?)(?=\nã€|$)', response, re.DOTALL)
        if analysis_match:
            analysis_content = analysis_match.group(1).strip()
            
            # æå–å„ä¸ªåˆ†æç»´åº¦
            fields = ["æ ¸å¿ƒæ¦‚å¿µæå–", "ç ”ç©¶çƒ­ç‚¹è¯†åˆ«", "æ–¹æ³•æŠ€æœ¯å½’ç±»", "åº”ç”¨åœºæ™¯æ‰©å±•"]
            for field in fields:
                field_match = re.search(rf'{field}:\s*(.*?)(?=\n[^\s]|\n{field}|\næ ¸å¿ƒ|\nç ”ç©¶|\næ–¹æ³•|\nåº”ç”¨|$)', analysis_content, re.DOTALL)
                if field_match:
                    analysis[field] = field_match.group(1).strip()
        
    except Exception as e:
        print(f"âš ï¸ ç¬¬{iteration}è½®ï¼šè§£æææ–™åˆ†ææ—¶å‡ºé”™: {e}")
    
    return analysis

def _extract_improvement_assessment(response: str, iteration: int) -> Dict:
    """æå–æ”¹è¿›æ•ˆæœé‡åŒ–è¯„ä¼°"""
    assessment = {}
    try:
        # æŸ¥æ‰¾æ”¹è¿›æ•ˆæœè¯„ä¼°éƒ¨åˆ†
        improve_match = re.search(r'ã€æ”¹è¿›æ•ˆæœé‡åŒ–è¯„ä¼°ã€‘\s*(.*?)(?=\nã€|$)', response, re.DOTALL)
        if improve_match:
            improve_content = improve_match.group(1).strip()
            
            # æå–æ–°å¢å†…å®¹å æ¯”
            content_match = re.search(r'æ–°å¢é‡è¦å†…å®¹å æ¯”:\s*(\d+\.?\d*)%', improve_content)
            if content_match:
                assessment["new_content_ratio"] = float(content_match.group(1))
            
            # æå–ç»¼åˆæ”¹è¿›è¯„åˆ† - ğŸ”§ ä¿®å¤ï¼šæ”¯æŒå°æ•°è¯„åˆ†
            overall_match = re.search(r'ç»¼åˆæ”¹è¿›è¯„åˆ†:\s*(\d+\.?\d*)/10', improve_content)
            if overall_match:
                assessment["overall_improvement"] = float(overall_match.group(1))
                
            # æå–å…¶ä»–æ”¹è¿›æè¿°
            fields = ["ç»“æ„ä¼˜åŒ–ç¨‹åº¦", "ä¸“ä¸šæ€§æå‡åº¦", "å®Œæ•´æ€§æ”¹å–„ç‡"]
            for field in fields:
                field_match = re.search(rf'{field}:\s*(.*?)(?=\n[^\s]|$)', improve_content, re.DOTALL)
                if field_match:
                    assessment[field] = field_match.group(1).strip()
        
    except Exception as e:
        print(f"âš ï¸ ç¬¬{iteration}è½®ï¼šè§£ææ”¹è¿›è¯„ä¼°æ—¶å‡ºé”™: {e}")
    
    return assessment
    
def _extract_quality_evaluation(response: str, iteration: int) -> Dict:
    """æå–å¤šç»´åº¦è´¨é‡è¯„ä¼°ä¿¡æ¯"""
    evaluation = {}
    try:
        # æŸ¥æ‰¾è´¨é‡è¯„ä¼°éƒ¨åˆ†
        eval_match = re.search(r'ã€å¤šç»´åº¦è´¨é‡è¯„ä¼°ã€‘\s*(.*?)(?=\nã€|$)', response, re.DOTALL)
        if eval_match:
            eval_content = eval_match.group(1).strip()
            
            # æå–å„ç»´åº¦è¯„åˆ†
            dimensions = ["å­¦æœ¯å®Œæ•´æ€§", "ç»“æ„é€»è¾‘æ€§", "æœ¯è¯­ä¸“ä¸šæ€§", "å†…å®¹å¹³è¡¡æ€§", "å›½é™…è§„èŒƒæ€§"]
            for dim in dimensions:
                score_match = re.search(rf'{dim}:\s*(\d+)/10', eval_content)
                if score_match:
                    evaluation[dim] = int(score_match.group(1))
            
            # æå–ç»¼åˆè¯„åˆ†
            overall_match = re.search(r'ç»¼åˆè´¨é‡è¯„åˆ†:\s*(\d+\.?\d*)/10', eval_content)
            if overall_match:
                evaluation["overall_score"] = float(overall_match.group(1))
        
    except Exception as e:
        print(f"âš ï¸ ç¬¬{iteration}è½®ï¼šè§£æè´¨é‡è¯„ä¼°æ—¶å‡ºé”™: {e}")
    
    return evaluation
    
def _format_materials_for_refinement(materials: List) -> str:
    """æ ¼å¼åŒ–ææ–™ç”¨äºä¼˜åŒ–åˆ†æ"""
    if not materials:
        return "æ— æ–°ææ–™"
    
    # ğŸ†• ä½¿ç”¨max(100, len(materials))æ¥å†³å®šè¿”å›çš„ææ–™æ•°é‡ï¼Œå°½å¯èƒ½å¤šè¿”å›å‚è€ƒèµ„æ–™
    material_count = max(100, len(materials))
    actual_materials = materials[:material_count]
    
    formatted = f"å…±{len(actual_materials)}æ¡æ–°ææ–™:\n\n"
    
    # ğŸ†• ä¸åˆå§‹å¤§çº²ç”Ÿæˆä¿æŒä¸€è‡´çš„æ ¼å¼åŒ–æ–¹å¼
    for i, material in enumerate(actual_materials, 1):
        content = material.get('content', '')
        relevance_score = material.get('relevance_score', 0)
        
        # ğŸ†• ä¸ä¹‹å‰ä¿æŒä¸€è‡´ï¼šé•¿åº¦è¶…è¿‡1000åˆ™åªå–å‰1000ï¼Œä¸è¶…è¿‡åˆ™ç›´æ¥ä½¿ç”¨
        if len(content) > 2000:
            formatted += f"ææ–™{i} (ç›¸å…³åº¦: {relevance_score:.2f}): {content[:2000]}...\n\n"
        else:
            formatted += f"ææ–™{i} (ç›¸å…³åº¦: {relevance_score:.2f}): {content}\n\n"
    
    return formatted

def _parse_refinement_response( response: str, topic: str, iteration: int) -> Dict:
    """è§£æç§‘å­¦ä¼˜åŒ–å“åº”ï¼Œæå–è¯„ä¼°ä¿¡æ¯ã€å¤§çº²å’Œå†³ç­–"""
    try:
        from utils import _debug_response_structure
        # ğŸ†• è¾“å‡ºè°ƒè¯•ä¿¡æ¯
        _debug_response_structure(response, iteration)
        
        # ğŸ†• æ›´é²æ£’çš„è§£æé€»è¾‘ï¼šé¦–å…ˆå°è¯•å®Œæ•´çš„æ ‡è®°åŒ¹é…
        result_section = response
        
        # æŸ¥æ‰¾ä¼˜åŒ–ç»“æœéƒ¨åˆ†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        start_marker = "===ä¼˜åŒ–ç»“æœå¼€å§‹==="
        end_marker = "===ä¼˜åŒ–ç»“æœç»“æŸ==="
        
        start_idx = response.find(start_marker)
        if start_idx != -1:
            end_idx = response.find(end_marker)
            if end_idx != -1:
                result_section = response[start_idx + len(start_marker):end_idx].strip()
                print(f"âœ… ç¬¬{iteration}è½®ï¼šæ‰¾åˆ°å®Œæ•´çš„ä¼˜åŒ–ç»“æœæ ‡è®°")
            else:
                # å¦‚æœåªæœ‰å¼€å§‹æ ‡è®°ï¼Œä»å¼€å§‹æ ‡è®°åé¢å¼€å§‹è§£æ
                result_section = response[start_idx + len(start_marker):].strip()
                print(f"âš ï¸ ç¬¬{iteration}è½®ï¼šåªæ‰¾åˆ°å¼€å§‹æ ‡è®°ï¼Œä»æ­¤å¤„å¼€å§‹è§£æ")
        else:
            print(f"â„¹ï¸ ç¬¬{iteration}è½®ï¼šæœªæ‰¾åˆ°ä¼˜åŒ–ç»“æœæ ‡è®°ï¼Œç›´æ¥è§£ææ•´ä¸ªå“åº”")
        
        from utils import _extract_quality_evaluation, _extract_material_analysis, _extract_improvement_assessment

        # ğŸ†• è§£æå¤šç»´åº¦è´¨é‡è¯„ä¼°
        evaluation_info = _extract_quality_evaluation(result_section, iteration)
        
        # ğŸ†• è§£ææ–°ææ–™åˆ†æ
        material_analysis = _extract_material_analysis(result_section, iteration)
        
        # ğŸ†• è§£ææ”¹è¿›æ•ˆæœè¯„ä¼°
        improvement_assessment = _extract_improvement_assessment(result_section, iteration)
        
        # ğŸ†• æå–ä¼˜åŒ–åçš„å¤§çº² - æ”¾å®½å¼€å¤´æ ‡è®°é™åˆ¶
        # æ”¯æŒå¤šç§å¼€å¤´æ ‡è®°ï¼šã€ä¼˜åŒ–åå¤§çº²ã€‘ã€===å¤§çº²å¼€å§‹===ã€ã€ç»¼è¿°æ¦‚è¿°ã€‘ã€===ä¼˜åŒ–ç»“æœå¼€å§‹===
        start_markers = ["ã€ä¼˜åŒ–åå¤§çº²ã€‘", "===å¤§çº²å¼€å§‹===", "ã€ç»¼è¿°æ¦‚è¿°ã€‘", "===ä¼˜åŒ–ç»“æœå¼€å§‹==="]
        outline_start = -1
        found_marker = None
        
        for marker in start_markers:
            pos = result_section.find(marker)
            if pos != -1:
                outline_start = pos
                found_marker = marker
                break
        
        if outline_start == -1:
            print(f"âŒ ç¬¬{iteration}è½®ï¼šæœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„å¼€å¤´æ ‡è®°ï¼Œå“åº”æ ¼å¼å¯èƒ½ä¸æ­£ç¡®")
            print(f"è°ƒè¯•ä¿¡æ¯ï¼šå“åº”å¼€å¤´300å­—ç¬¦: {result_section[:300]}")
            return None
        
        # æŸ¥æ‰¾ç»“æŸæ ‡è®°ï¼ˆå¯é€‰ï¼‰
        outline_end = result_section.find("===å¤§çº²ç»“æŸ===")
        
        if outline_end != -1:
            # æ‰¾åˆ°äº†ç»“æŸæ ‡è®°
            outline_content = result_section[outline_start:outline_end + len("===å¤§çº²ç»“æŸ===")]
        else:
            # æ²¡æœ‰æ‰¾åˆ°ç»“æŸæ ‡è®°ï¼Œä½¿ç”¨ä»å¼€å¤´æ ‡è®°åˆ°å“åº”ç»“å°¾çš„å†…å®¹
            outline_content = result_section[outline_start:]
            print(f"âš ï¸ ç¬¬{iteration}è½®ï¼šåªæ‰¾åˆ°å¼€å¤´æ ‡è®° ({found_marker})ï¼Œä½¿ç”¨åˆ°ç»“å°¾çš„å†…å®¹")
        
        # è§£æå¤§çº² - ä¼ é€’æ‰¾åˆ°çš„å¼€å¤´æ ‡è®°ä¿¡æ¯
        from utils import parse_outline_response
        outline = parse_outline_response(outline_content, topic, [], found_start_marker=found_marker)
        
        from utils import _extract_scientific_decision

        # ğŸ†• æ›´ç§‘å­¦çš„å†³ç­–æå–ï¼šè§£æç§‘å­¦å†³ç­–ä¾æ®
        decision_info = _extract_scientific_decision(response, iteration)
        should_continue = decision_info.get("should_continue", False)
        
        # ğŸ†• æ·»åŠ è§£ææˆåŠŸçš„è¯¦ç»†æ—¥å¿—
        print(f"âœ… ç¬¬{iteration}è½®ï¼šPlannerä¼˜åŒ–è§£ææˆåŠŸ,è´¨é‡è¯„ä¼°: ç»¼åˆè¯„åˆ† {evaluation_info.get('overall_score', 'N/A')}/10")
        # ğŸ”§ ä¿®å¤ï¼šä»å†³ç­–ä¿¡æ¯ä¸­è·å–ç»¼åˆæ”¹è¿›è¯„åˆ†ï¼Œä¼˜å…ˆä½¿ç”¨decision_infoä¸­çš„æ•°æ®
        improvement_score = decision_info.get('improvement_score', improvement_assessment.get('overall_improvement', 'N/A'))
        print(f"   æ”¹è¿›è¯„ä¼°: ç»¼åˆæ”¹è¿›è¯„åˆ† {improvement_score}/10, å†³ç­–ç»“æœ: {'ç»§ç»­' if should_continue else 'åœæ­¢'}")
        
        return {
            "outline": outline,
            "should_continue": should_continue,
            "evaluation_info": evaluation_info,
            "material_analysis": material_analysis,
            "improvement_assessment": improvement_assessment,
            "decision_info": decision_info
        }
        
    except Exception as e:
        print(f"âŒ ç¬¬{iteration}è½®ï¼šè§£æä¼˜åŒ–å“åº”æ—¶å‡ºé”™: {e}")
        print(f"è°ƒè¯•ä¿¡æ¯ï¼šå“åº”é•¿åº¦: {len(response)}, å¼€å¤´200å­—ç¬¦: {response[:200]}")
        return None


def _format_outline_for_refinement(outline: Dict) -> str:
    """æ ¼å¼åŒ–å¤§çº²ç”¨äºä¼˜åŒ–åˆ†æ"""
    if not outline:
        return "å½“å‰å¤§çº²ä¸ºç©º"
    
    formatted = f"ä¸»é¢˜: {outline.get('topic', 'æœªçŸ¥')}\n"
    formatted += f"æ¦‚è¿°: {outline.get('overview', 'æ— æ¦‚è¿°')}\n\n"
    
    chapters = outline.get("chapters", [])
    
    # ğŸ”§ ä¿®æ”¹ï¼šåŒæ—¶æ”¯æŒåˆ—è¡¨å’Œå­—å…¸æ ¼å¼çš„ç« èŠ‚æ•°æ®
    if isinstance(chapters, list):
        # å¤„ç†åˆ—è¡¨æ ¼å¼çš„ç« èŠ‚ (parse_outline_response è¿”å›çš„æ ¼å¼)
        for chapter in chapters:
            chapter_id = chapter.get('id', '?')
            chapter_title = chapter.get('title', 'æœªå‘½åç« èŠ‚')
            chapter_desc = chapter.get('description', 'æ— æè¿°')
            
            formatted += f"{chapter_id}. {chapter_title}\n"
            formatted += f"æè¿°: {chapter_desc}\n"
            
            # å¤„ç†å­ç« èŠ‚ï¼ˆä¹Ÿæ˜¯åˆ—è¡¨æ ¼å¼ï¼‰
            subsections = chapter.get("subsections", [])
            if isinstance(subsections, list):
                for subsection in subsections:
                    sub_id = subsection.get('id', '?')
                    sub_title = subsection.get('title', 'æœªå‘½åå­ç« èŠ‚')
                    sub_desc = subsection.get('description', '')
                    
                    formatted += f"  {sub_id} {sub_title}\n"
                    if sub_desc:
                        formatted += f"    æè¿°: {sub_desc}\n"
            formatted += "\n"
            
    elif isinstance(chapters, dict):
        # ä¿æŒåŸæœ‰çš„å­—å…¸æ ¼å¼å¤„ç†é€»è¾‘ï¼ˆå‘åå…¼å®¹ï¼‰
        for chapter_id, chapter in chapters.items():
            formatted += f"{chapter_id}. {chapter.get('title', 'æœªå‘½åç« èŠ‚')}\n"
            formatted += f"æè¿°: {chapter.get('description', 'æ— æè¿°')}\n"
            
            subsections = chapter.get("subsections", {})
            if isinstance(subsections, dict):
                for sub_id, subsection in subsections.items():
                    formatted += f"  {sub_id} {subsection.get('title', 'æœªå‘½åå­ç« èŠ‚')}\n"
                    if subsection.get('description'):
                        formatted += f"    æè¿°: {subsection['description']}\n"
            elif isinstance(subsections, list):
                # å¤„ç†å­—å…¸æ ¼å¼ç« èŠ‚ä¸­çš„åˆ—è¡¨æ ¼å¼å­ç« èŠ‚
                for subsection in subsections:
                    sub_id = subsection.get('id', '?')
                    sub_title = subsection.get('title', 'æœªå‘½åå­ç« èŠ‚')
                    sub_desc = subsection.get('description', '')
                    
                    formatted += f"  {sub_id} {sub_title}\n"
                    if sub_desc:
                        formatted += f"    æè¿°: {sub_desc}\n"
            formatted += "\n"
    else:
        formatted += "ç« èŠ‚ä¿¡æ¯æ ¼å¼å¼‚å¸¸\n"
    
    return formatted


def _parse_enrichment_refinement_response( response: str, iteration: int, current_enriched: Dict = None) -> Dict:
    """è§£æç§‘å­¦ä¼˜åŒ–å“åº”ï¼Œæå–è¯„ä¼°ä¿¡æ¯ã€ä¸°å¯Œå†…å®¹å’Œå†³ç­–"""
    try:
        # ğŸ†• æ›´é²æ£’çš„è§£æé€»è¾‘ï¼šé¦–å…ˆå°è¯•å®Œæ•´çš„æ ‡è®°åŒ¹é…
        result_section = response
        
        # æŸ¥æ‰¾ä¼˜åŒ–ç»“æœéƒ¨åˆ†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        start_marker = "===ä¼˜åŒ–ç»“æœå¼€å§‹==="
        end_marker = "===ä¼˜åŒ–ç»“æœç»“æŸ==="
        
        start_idx = response.find(start_marker)
        if start_idx != -1:
            end_idx = response.find(end_marker)
            if end_idx != -1:
                result_section = response[start_idx + len(start_marker):end_idx].strip()
                print(f"âœ… ç¬¬{iteration}è½®ï¼šæ‰¾åˆ°å®Œæ•´çš„ä¼˜åŒ–ç»“æœæ ‡è®°")
            else:
                result_section = response[start_idx + len(start_marker):].strip()
                print(f"âš ï¸ ç¬¬{iteration}è½®ï¼šåªæ‰¾åˆ°å¼€å§‹æ ‡è®°ï¼Œä»æ­¤å¤„å¼€å§‹è§£æ")
        else:
            print(f"â„¹ï¸ ç¬¬{iteration}è½®ï¼šæœªæ‰¾åˆ°ä¼˜åŒ–ç»“æœæ ‡è®°ï¼Œç›´æ¥è§£ææ•´ä¸ªå“åº”")
        
        from utils import _extract_content_quality_evaluation, _extract_material_value_analysis, _extract_keyword_optimization_analysis, _extract_improvement_opportunities, _extract_enrichment_improvement_assessment
        # ğŸ†• è§£æå¤šç»´åº¦å†…å®¹è´¨é‡è¯„ä¼°
        quality_evaluation = _extract_content_quality_evaluation(result_section, iteration)
        
        # ğŸ†• è§£ææ–°ææ–™ä»·å€¼åˆ†æ
        material_value_analysis = _extract_material_value_analysis(result_section, iteration)
        
        # ğŸ†• è§£æå…³é”®è¯ä¼˜åŒ–æ½œåŠ›è¯„ä¼°
        keyword_optimization = _extract_keyword_optimization_analysis(result_section, iteration)
        
        # ğŸ†• è§£æå†…å®¹ç¼ºé™·ä¸æ”¹è¿›æœºä¼šè¯†åˆ«
        improvement_opportunities = _extract_improvement_opportunities(result_section, iteration)
        
        # ğŸ†• è§£ææ”¹è¿›æ•ˆæœé‡åŒ–è¯„ä¼°
        improvement_assessment = _extract_enrichment_improvement_assessment(result_section, iteration)
        
        # ğŸ†• æå–ä¼˜åŒ–åçš„ä¸°å¯Œå†…å®¹ - ä¿ç•™å®Œæ•´æ ‡è®°ä¾›parse_full_enrichmentä½¿ç”¨
        enrichment_start = result_section.find("===å†…å®¹è§„åˆ’å¼€å§‹===")
        enrichment_end = result_section.find("===å†…å®¹è§„åˆ’ç»“æŸ===")
        
        if enrichment_start == -1:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¼€å§‹æ ‡è®°ï¼Œä½¿ç”¨æ•´ä¸ªresult_section
            enrichment_content = result_section
            print(f"âš ï¸ ç¬¬{iteration}è½®ï¼šæœªæ‰¾åˆ°===å†…å®¹è§„åˆ’å¼€å§‹===æ ‡è®°ï¼Œä½¿ç”¨å…¨éƒ¨å†…å®¹è§£æ")
        elif enrichment_end == -1:
            # åªæœ‰å¼€å§‹æ ‡è®°ï¼Œæ²¡æœ‰ç»“æŸæ ‡è®°ï¼Œä¿ç•™æ ‡è®°
            enrichment_content = result_section[enrichment_start:]
            print(f"âš ï¸ ç¬¬{iteration}è½®ï¼šåªæ‰¾åˆ°å¼€å§‹æ ‡è®°ï¼Œä½¿ç”¨åˆ°ç»“å°¾çš„å†…å®¹")
        else:
            # ğŸ”§ ä¿®å¤ï¼šæ‰¾åˆ°äº†å®Œæ•´çš„æ ‡è®°å¯¹ï¼Œä¿ç•™å®Œæ•´çš„æ ‡è®°ä¾›parse_full_enrichmentè§£æ
            enrichment_content = result_section[enrichment_start:enrichment_end + len("===å†…å®¹è§„åˆ’ç»“æŸ===")]
            print(f"âœ… ç¬¬{iteration}è½®ï¼šæ‰¾åˆ°å®Œæ•´çš„å†…å®¹è§„åˆ’æ ‡è®°")
        
        # è§£æä¸°å¯Œå†…å®¹ï¼ˆå¤ç”¨ç°æœ‰çš„è§£æå‡½æ•°ï¼‰
        from utils import parse_full_enrichment
        # ğŸ”§ ä¿®æ”¹ï¼šä½¿ç”¨å½“å‰çš„ä¸°å¯Œå†…å®¹ä½œä¸ºæ¨¡æ¿ï¼Œä¿ç•™topicå’Œoverviewä¿¡æ¯
        if current_enriched and isinstance(current_enriched, dict):
            template_outline = {
                "topic": current_enriched.get("topic", "æœªçŸ¥"),
                "overview": current_enriched.get("overview", "æ— æ¦‚è¿°"),
                "chapters": current_enriched.get("chapters", {})
            }
            print(f"âœ“ ä½¿ç”¨å½“å‰ä¸°å¯Œå†…å®¹ä½œä¸ºæ¨¡æ¿: {current_enriched.get('topic', 'æœªçŸ¥')}")
        else:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šåˆ›å»ºä¸´æ—¶å¤§çº²
            template_outline = {"topic": "temp", "chapters": {}}
            print(f"âš ï¸ current_enrichedç±»å‹å¼‚å¸¸ï¼Œä½¿ç”¨ä¸´æ—¶æ¨¡æ¿ã€‚ç±»å‹: {type(current_enriched)}, å€¼: {str(current_enriched)[:100] if current_enriched else 'None'}")
        
        enriched = parse_full_enrichment(enrichment_content, template_outline)
        
        # ğŸ†• æ›´ç§‘å­¦çš„å†³ç­–æå–ï¼šè§£æç§‘å­¦å†³ç­–ä¾æ®
        decision_info = _extract_scientific_enrichment_decision(response, iteration)
        should_continue = decision_info.get("should_continue", False)
        
        # ğŸ†• æ·»åŠ è§£ææˆåŠŸçš„è¯¦ç»†æ—¥å¿—
        print(f"âœ… ç¬¬{iteration}è½®ï¼šEnricherä¼˜åŒ–è§£ææˆåŠŸ,è´¨é‡è¯„ä¼°: ç»¼åˆè¯„åˆ† {quality_evaluation.get('overall_score', 'N/A')}/10")
        # ğŸ”§ ä¿®å¤ï¼šä»å†³ç­–ä¿¡æ¯ä¸­è·å–ç»¼åˆæ”¹è¿›è¯„åˆ†ï¼Œä¼˜å…ˆä½¿ç”¨decision_infoä¸­çš„æ•°æ®
        improvement_score = decision_info.get('improvement_score', improvement_assessment.get('overall_improvement', 'N/A'))
        print(f"   æ”¹è¿›è¯„ä¼°: ç»¼åˆæ”¹è¿›è¯„åˆ† {improvement_score}/10, å…³é”®è¯ä¼˜åŒ–: {len(keyword_optimization.get('æœ¯è¯­æ›´æ–°éœ€æ±‚', ''))} ä¸ªæœ¯è¯­éœ€è¦æ›´æ–°, å†³ç­–ç»“æœ: {'ç»§ç»­' if should_continue else 'åœæ­¢'}")
        
        # ğŸ”§ æ·»åŠ ç±»å‹æ£€æŸ¥ï¼Œç¡®ä¿è¿”å›çš„enrichedæ˜¯å­—å…¸ç±»å‹
        if not isinstance(enriched, dict):
            print(f"âš ï¸ è­¦å‘Šï¼šè§£æç»“æœä¸æ˜¯å­—å…¸ç±»å‹ï¼Œç±»å‹: {type(enriched)}, å€¼: {str(enriched)[:200] if enriched else 'None'}")
            return None
        
        return {
            "enrichment": enriched,
            "should_continue": should_continue,
            "quality_evaluation": quality_evaluation,
            "material_value_analysis": material_value_analysis,
            "keyword_optimization": keyword_optimization,
            "improvement_opportunities": improvement_opportunities,
            "improvement_assessment": improvement_assessment,
            "decision_info": decision_info
        }
        
    except Exception as e:
        print(f"âŒ ç¬¬{iteration}è½®ï¼šè§£æä¸°å¯Œä¼˜åŒ–å“åº”æ—¶å‡ºé”™: {e}")
        import traceback
        print(f"å®Œæ•´é”™è¯¯å †æ ˆï¼š\n{traceback.format_exc()}")
        return None


def _select_materials_proportionally( categorized_materials: Dict, 
                                    target_texts: int = 75, target_equations: int = 8, 
                                    target_figures: int = 10, target_tables: int = 12,
                                    skip_count: int = 0) -> List:
    """æŒ‰æ¯”ä¾‹ä»å„ç±»å‹ææ–™ä¸­é€‰æ‹©ææ–™"""

    # ğŸ” è¯¦ç»†æ£€æŸ¥è¾“å…¥çš„åˆ†ç±»ææ–™
    for material_type, materials in categorized_materials.items():
        if len(materials) > 0:
            # æ£€æŸ¥ç¬¬ä¸€ä¸ªææ–™çš„ç»“æ„
            first_material = materials[0]
            content_type_from_metadata = first_material.get("metadata", {}).get("content_type", "unknown")
            content_type_direct = first_material.get("content_type", "unknown")
    
    selected_materials = []
    
    # æŒ‰ç›¸å…³åº¦æ’åºå„ç±»å‹ææ–™
    sorted_texts = sorted(categorized_materials.get("texts", []), 
                            key=lambda x: x.get("relevance_score", 0), reverse=True)
    sorted_equations = sorted(categorized_materials.get("equations", []), 
                                key=lambda x: x.get("relevance_score", 0), reverse=True)
    sorted_figures = sorted(categorized_materials.get("figures", []), 
                            key=lambda x: x.get("relevance_score", 0), reverse=True)
    sorted_tables = sorted(categorized_materials.get("tables", []), 
                            key=lambda x: x.get("relevance_score", 0), reverse=True)
    
    
    # ğŸ†• æŒ‰ç›®æ ‡æ•°é‡é€‰æ‹©ææ–™ï¼Œæ”¯æŒè·³è¿‡å‰é¢å·²é€‰çš„ææ–™
    selected_texts = sorted_texts[skip_count:skip_count + target_texts]
    selected_equations = sorted_equations[skip_count//4:skip_count//4 + target_equations]  # å…¬å¼ææ–™è¾ƒå°‘ï¼ŒæŒ‰æ¯”ä¾‹è·³è¿‡
    selected_figures = sorted_figures[skip_count//3:skip_count//3 + target_figures]      # å›¾ç‰‡ææ–™ä¸­ç­‰ï¼ŒæŒ‰æ¯”ä¾‹è·³è¿‡
    selected_tables = sorted_tables[skip_count//3:skip_count//3 + target_tables]        # è¡¨æ ¼ææ–™ä¸­ç­‰ï¼ŒæŒ‰æ¯”ä¾‹è·³è¿‡
    
    
    # åˆå¹¶æ‰€æœ‰é€‰ä¸­çš„ææ–™ï¼Œå¹¶ä¸ºæ¯ä¸ªææ–™æ ‡è®°æ­£ç¡®çš„ç±»å‹
    initial_count = len(selected_materials)
    
    # æ·»åŠ æ–‡æœ¬ææ–™å¹¶æ ‡è®°ç±»å‹
    for material in selected_texts:
        material['content_type'] = 'text'  # ğŸ”§ ä¸ºææ–™è®¾ç½®æ­£ç¡®çš„ç±»å‹æ ‡è¯†
    selected_materials.extend(selected_texts)
    
    # æ·»åŠ å…¬å¼ææ–™å¹¶æ ‡è®°ç±»å‹
    initial_count = len(selected_materials)
    for material in selected_equations:
        material['content_type'] = 'equation'  # ğŸ”§ ä¸ºææ–™è®¾ç½®æ­£ç¡®çš„ç±»å‹æ ‡è¯†
    selected_materials.extend(selected_equations)
    
    # æ·»åŠ å›¾ç‰‡ææ–™å¹¶æ ‡è®°ç±»å‹
    initial_count = len(selected_materials)
    for material in selected_figures:
        material['content_type'] = 'figure'  # ğŸ”§ ä¸ºææ–™è®¾ç½®æ­£ç¡®çš„ç±»å‹æ ‡è¯†
    selected_materials.extend(selected_figures)
    
    # æ·»åŠ è¡¨æ ¼ææ–™å¹¶æ ‡è®°ç±»å‹
    initial_count = len(selected_materials)
    for material in selected_tables:
        material['content_type'] = 'table'  # ğŸ”§ ä¸ºææ–™è®¾ç½®æ­£ç¡®çš„ç±»å‹æ ‡è¯†
    selected_materials.extend(selected_tables)
    
    # ğŸ” åŸºäºå®é™…é€‰æ‹©æ•°é‡ç»Ÿè®¡æœ€ç»ˆçš„ææ–™ç±»å‹åˆ†å¸ƒ
    final_type_counts = {
        "text": len(selected_texts),
        "equation": len(selected_equations), 
        "figure": len(selected_figures),
        "table": len(selected_tables)
    }
    
    # è®°å½•å®é™…é€‰æ‹©çš„æ•°é‡
    actual_counts = {
        "texts": len(selected_texts),
        "equations": len(selected_equations), 
        "figures": len(selected_figures),
        "tables": len(selected_tables)
    }
    
    
    return selected_materials


def _select_next_batch_materials(categorized_materials: Dict, used_materials_count: Dict, 
                                target_counts: Dict) -> List:
    """ä»å‰©ä½™ææ–™ä¸­é€‰æ‹©ä¸‹ä¸€æ‰¹ææ–™"""
    selected_materials = []
    
    for material_type, target_count in target_counts.items():
        materials = categorized_materials.get(material_type, [])
        used_count = used_materials_count.get(material_type, 0)
        
        # æŒ‰ç›¸å…³åº¦æ’åºææ–™
        sorted_materials = sorted(materials, key=lambda x: x.get("relevance_score", 0), reverse=True)
        
        # è·å–ä¸‹ä¸€æ‰¹ææ–™
        start_idx = used_count
        end_idx = min(start_idx + target_count, len(sorted_materials))
        
        if start_idx < len(sorted_materials):
            next_batch = sorted_materials[start_idx:end_idx]
            
            # ğŸ”§ ä¸ºé€‰ä¸­çš„ææ–™è®¾ç½®æ­£ç¡®çš„ç±»å‹æ ‡è¯†
            content_type_mapping = {
                "texts": "text",
                "equations": "equation", 
                "figures": "figure",
                "tables": "table"
            }
            content_type = content_type_mapping.get(material_type, "text")
            for material in next_batch:
                material['content_type'] = content_type
                
            selected_materials.extend(next_batch)
            
    # è®°å½•å®é™…é€‰æ‹©çš„æ•°é‡ç»Ÿè®¡
    actual_counts = {}
    for material_type in target_counts.keys():
        materials = categorized_materials.get(material_type, [])
        used_count = used_materials_count.get(material_type, 0)
        available = len(materials) - used_count
        target = target_counts[material_type]
        actual = min(available, target)
        actual_counts[material_type] = actual
        
    print(f"ğŸ“Š é€‰æ‹©ä¸‹ä¸€æ‰¹ææ–™: æ–‡æœ¬{actual_counts.get('texts', 0)}, å…¬å¼{actual_counts.get('equations', 0)}, å›¾ç‰‡{actual_counts.get('figures', 0)}, è¡¨æ ¼{actual_counts.get('tables', 0)}, æ€»è®¡{len(selected_materials)}æ¡")
    
    return selected_materials
