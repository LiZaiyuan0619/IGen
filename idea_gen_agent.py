"""
Idea Generation å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆéª¨æ¶å®šä¹‰ï¼‰

è„šæœ¬ç›®æ ‡:
    - åœ¨ä¸æ”¹åŠ¨ç°æœ‰ Survey Gen ä»£ç çš„å‰æä¸‹ï¼ŒåŸºäºå…¶äº§ç‰©(final_result ä¸ enriched_outline)ä¸å¯æ£€ç´¢æ•°æ®åº“ï¼Œ
      æä¾›â€œæœºä¼šå›¾è°±â†’æƒ³æ³•ç”Ÿæˆâ†’æ–°é¢–æ€§/å¯è¡Œæ€§è¯„å®¡â†’ç²¾ç‚¼è¿­ä»£â€çš„å¤šæ™ºèƒ½ä½“æ¶æ„éª¨æ¶ã€‚

ä¸Šä¸‹æ–‡:
    - ä¸Šæ¸¸: `ma_gen.generate_survey` å·²äº§å‡º `final_result`(Markdownæ•´åˆæ­£æ–‡ç­‰) ä¸ `enriched_outline`(JSONå¤§çº²)ã€‚
    - ä¸‹æ¸¸: æœ¬æ–‡ä»¶åªæä¾›ç±»ä¸å‡½æ•°ç­¾ååŠå®ç°æŒ‡å¼•ï¼Œä¾¿äºå·¥ç¨‹å¸ˆé€æ­¥å¡«å……å…·ä½“å®ç°ã€‚
    - è¿è¡Œç¯å¢ƒ: å¤ç”¨ `multi_agent.py` ä¸­çš„ LLMFactoryã€AcademicPaperDatabase ç­‰åŸºç¡€è®¾æ–½ã€‚

è¾“å…¥:
    - final_result: Dictï¼Œè‡³å°‘åŒ…å« `full_document`(Markdownæ­£æ–‡å®Œæ•´å­—ç¬¦ä¸²)ã€å¯é€‰ `bibliography/figures/tables/equations/statistics` ç­‰ã€‚
    - enriched_outline: Dictï¼Œç« èŠ‚çº§ `keywords/research_focus/content_guide/subsections` ç­‰å†™ä½œæŒ‡å¼•ã€‚
    - db: AcademicPaperDatabaseï¼Œå¯é€šè¿‡ `db.search_content(query, content_type="texts", n_results=K)` è¿›è¡ŒRAGæ£€ç´¢ã€‚

æ‰§è¡Œæ­¥éª¤(é«˜å±‚):
    1) IdeaMiner æ„å»ºè¯­ä¹‰æœºä¼šå›¾è°± SemanticOpportunityGraphï¼ˆæŠ½å–å®ä½“ä¸å…³ç³»ï¼Œå®šä½ç©ºæ´/å†²çªï¼‰ã€‚
    2) IdeaGenerator åœ¨å›¾è°±ä¸Šè§¦å‘æ¨¡å¼å¹¶åº”ç”¨ç­–ç•¥æ¨¡æ¿ï¼Œç”Ÿæˆ CandidateIdea åˆ—è¡¨ï¼ˆé»˜è®¤å¹¶å‘/æ•°é‡=6ï¼‰ã€‚
    3) NoveltyCritic åŸºäº RAG çš„ä¸¤é˜¶æ®µæ£€ç´¢ä¸é‡æ’ï¼Œç»™å‡ºæ–°é¢–æ€§è¯„å®¡ã€‚
    4) FeasibilityCritic ç»“åˆå›¾è°±èµ„æºçº¦æŸï¼Œç»™å‡ºå¯è¡Œæ€§è¯„å®¡ã€‚
    5) IdeaRefiner æ±‡æ€»æ‰¹åˆ¤å¹¶ä¸‹è¾¾å¯æ‰§è¡Œçš„ç²¾ç‚¼æŒ‡ä»¤ï¼Œè¿›å…¥è¿­ä»£è¾©è®ºé—­ç¯ï¼Œç›´è‡³æ”¶æ•›æˆ–è¾¾åˆ°ä¸Šé™ã€‚

è¾“å‡º:
    - ç»“æ„åŒ–çš„ç»“æœå¯¹è±¡ï¼ŒåŒ…æ‹¬ï¼šæœºä¼šå›¾è°±ã€å€™é€‰æƒ³æ³•ã€è¯„å®¡è®°å½•ã€ç²¾ç‚¼æŒ‡ä»¤ã€è¿­ä»£åçš„æœ€ç»ˆæƒ³æ³•é›†ä¸è½¨è¿¹ã€‚

æ³¨æ„äº‹é¡¹:
    - æœ¬æ–‡ä»¶ä»…ä¸ºâ€œéª¨æ¶â€ï¼Œä¸åŒ…å«å®é™…ç®—æ³•ä¸LLMæç¤ºè¯ï¼›è¯·åœ¨å„æ–¹æ³•å†…æŒ‰docstringæä¾›çš„å®ç°æ€è·¯è¡¥å……ã€‚
    - ä¿æŒæ•°æ®å¯è¿½æº¯ï¼šæ‰€æœ‰ç»“è®ºå°½å¯èƒ½ç»‘å®š `provenance` è¯æ®é”šç‚¹ï¼ˆsource/loc/paper_id/quoteï¼‰ã€‚
    - å¹¶å‘å»ºè®®: é»˜è®¤å¹¶å‘=6ï¼ˆä¸æ—¢æœ‰Writerå¹¶å‘ä¹ æƒ¯ä¸€è‡´ï¼‰ã€‚
"""

from __future__ import annotations

import asyncio
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple
import networkx as nx
from datetime import datetime

# å¤ç”¨æ—¢æœ‰åŸºç¡€è®¾æ–½
from multi_agent import LLMFactory, AcademicPaperDatabase, AgentConfig, ModelType


# =========================
# æ•°æ®ç»“æ„å®šä¹‰ï¼ˆéª¨æ¶ï¼‰
# =========================

class GraphNodeType(Enum):
    """è¯­ä¹‰æœºä¼šå›¾è°±ä¸­çš„èŠ‚ç‚¹ç±»å‹ã€‚

    - Method: æ–¹æ³•/æ¨¡å‹/ç®—æ³•
    - Task: ä»»åŠ¡/é—®é¢˜å®šä¹‰
    - Dataset: æ•°æ®é›†
    - Metric: è¯„ä»·æŒ‡æ ‡
    - Paper: è®ºæ–‡/è¯æ®æº
    - Problem: ç ”ç©¶éš¾é¢˜/æŒ‘æˆ˜
    - Domain: é¢†åŸŸ/å­é¢†åŸŸ
    """

    METHOD = "Method"
    TASK = "Task"
    DATASET = "Dataset"
    METRIC = "Metric"
    PAPER = "Paper"
    PROBLEM = "Problem"
    DOMAIN = "Domain"


@dataclass
class GraphNode:
    """å›¾è°±èŠ‚ç‚¹ã€‚

    å­—æ®µ:
        id: å”¯ä¸€IDï¼Œå»ºè®®å‰ç¼€ç¼–ç ï¼Œå¦‚ "M:Transformer"ã€"T:MT"ã€‚
        type: èŠ‚ç‚¹ç±»å‹ GraphNodeTypeã€‚
        name: è§„èŒƒåŒ–ä¸»åã€‚
        aliases: åˆ«å/ç¼©å†™åˆ—è¡¨ï¼Œä¾¿äºæ¶ˆæ­§ä¸æ£€ç´¢ã€‚
        evidence: è¯æ®åˆ—è¡¨ï¼Œå…ƒç´ å½¢å¦‚ {source, loc, quote, paper_id}ã€‚
        salience: èŠ‚ç‚¹é‡è¦æ€§(0~1)ï¼Œå¯ç”±é¢‘æ¬¡/ä¸­å¿ƒæ€§/å¼•ç”¨å¯†åº¦ç»¼åˆä¼°è®¡ã€‚
    """

    id: str
    type: GraphNodeType
    name: str
    aliases: List[str] = field(default_factory=list)
    evidence: List[Dict[str, Any]] = field(default_factory=list)
    salience: float = 0.0


class GraphEdgeRelation(Enum):
    """å›¾è°±è¯­ä¹‰å…³ç³»ç±»å‹ã€‚"""

    SOLVES = "solves"
    IMPROVES_ON = "improves_on"
    USES_DATASET = "uses_dataset"
    EVALUATED_BY = "evaluated_by"
    CRITIQUES = "critiques"
    CONTRADICTS = "contradicts"
    SIMILAR_TO = "similar_to"
    EXTENSION_OF = "extension_of"
    CAUSES = "causes"
    MITIGATES = "mitigates"


@dataclass
class GraphEdge:
    """å›¾è°±è¾¹ã€‚"""

    src: str
    dst: str
    relation: GraphEdgeRelation
    weight: float = 0.5
    confidence: float = 0.5
    evidence: List[Dict[str, Any]] = field(default_factory=list)
    notes: Optional[str] = None


# ä½¿ç”¨ NetworkX ä½œä¸ºè¯­ä¹‰æœºä¼šå›¾è°±çš„åº•å±‚å®ç°
# èŠ‚ç‚¹å±æ€§åŒ…å«: type, name, aliases, evidence, salience
# è¾¹å±æ€§åŒ…å«: relation, weight, confidence, evidence, notes
# å›¾å±æ€§åŒ…å«: gaps, provenance, indices
SemanticOpportunityGraph = nx.DiGraph


def create_semantic_graph() -> SemanticOpportunityGraph:
    """åˆ›å»ºç©ºçš„è¯­ä¹‰æœºä¼šå›¾è°±ã€‚
    
    è¿”å›:
        SemanticOpportunityGraph: åˆå§‹åŒ–çš„æœ‰å‘å›¾ï¼ŒåŒ…å«ç©ºçš„å…¨å±€å±æ€§ã€‚
    """
    graph = nx.DiGraph()
    # åˆå§‹åŒ–å›¾çš„å…¨å±€å±æ€§
    graph.graph['gaps'] = []
    graph.graph['provenance'] = {
        'created_at': datetime.now().isoformat(),
        'source': 'IdeaMinerAgent',
        'version': '1.0'
    }
    graph.graph['indices'] = {
        'by_type': {},
        'by_token': {}
    }
    return graph


def add_graph_node(graph: SemanticOpportunityGraph, node: GraphNode) -> None:
    """å‘å›¾ä¸­æ·»åŠ èŠ‚ç‚¹ã€‚
    
    å‚æ•°:
        graph: ç›®æ ‡å›¾è°±ã€‚
        node: è¦æ·»åŠ çš„èŠ‚ç‚¹å¯¹è±¡ã€‚
    """
    graph.add_node(
        node.id,
        type=node.type.value,
        name=node.name,
        aliases=node.aliases,
        evidence=node.evidence,
        salience=node.salience
    )
    
    # æ›´æ–°ç±»å‹ç´¢å¼•
    node_type = node.type.value
    if node_type not in graph.graph['indices']['by_type']:
        graph.graph['indices']['by_type'][node_type] = []
    graph.graph['indices']['by_type'][node_type].append(node.id)
    
    # æ›´æ–°tokenç´¢å¼•
    tokens = [node.name.lower()] + [alias.lower() for alias in node.aliases]
    for token in tokens:
        if token not in graph.graph['indices']['by_token']:
            graph.graph['indices']['by_token'][token] = []
        if node.id not in graph.graph['indices']['by_token'][token]:
            graph.graph['indices']['by_token'][token].append(node.id)


def add_graph_edge(graph: SemanticOpportunityGraph, edge: GraphEdge) -> None:
    """å‘å›¾ä¸­æ·»åŠ è¾¹ã€‚
    
    å‚æ•°:
        graph: ç›®æ ‡å›¾è°±ã€‚
        edge: è¦æ·»åŠ çš„è¾¹å¯¹è±¡ã€‚
    """
    graph.add_edge(
        edge.src,
        edge.dst,
        relation=edge.relation.value,
        weight=edge.weight,
        confidence=edge.confidence,
        evidence=edge.evidence,
        notes=edge.notes
    )


def find_nodes_by_type(graph: SemanticOpportunityGraph, node_type: GraphNodeType) -> List[str]:
    """æ ¹æ®ç±»å‹æŸ¥æ‰¾èŠ‚ç‚¹ã€‚
    
    å‚æ•°:
        graph: ç›®æ ‡å›¾è°±ã€‚
        node_type: èŠ‚ç‚¹ç±»å‹ã€‚
        
    è¿”å›:
        List[str]: åŒ¹é…çš„èŠ‚ç‚¹IDåˆ—è¡¨ã€‚
    """
    return graph.graph['indices']['by_type'].get(node_type.value, [])


def find_nodes_by_token(graph: SemanticOpportunityGraph, token: str) -> List[str]:
    """æ ¹æ®åç§°/åˆ«åtokenæŸ¥æ‰¾èŠ‚ç‚¹ã€‚
    
    å‚æ•°:
        graph: ç›®æ ‡å›¾è°±ã€‚
        token: æœç´¢tokenï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰ã€‚
        
    è¿”å›:
        List[str]: åŒ¹é…çš„èŠ‚ç‚¹IDåˆ—è¡¨ã€‚
    """
    return graph.graph['indices']['by_token'].get(token.lower(), [])


def add_opportunity_gap(graph: SemanticOpportunityGraph, gap: Dict[str, Any]) -> None:
    """å‘å›¾ä¸­æ·»åŠ æœºä¼šç¼ºå£ã€‚
    
    å‚æ•°:
        graph: ç›®æ ‡å›¾è°±ã€‚
        gap: æœºä¼šç¼ºå£æè¿°å­—å…¸ã€‚
    """
    graph.graph['gaps'].append(gap)


@dataclass
class CandidateIdea:
    """å€™é€‰æƒ³æ³•å¯¹è±¡ã€‚"""

    id: str
    title: str
    core_hypothesis: str
    initial_innovation_points: List[str] = field(default_factory=list)
    source_trigger_nodes: List[str] = field(default_factory=list)
    expected_contribution: List[str] = field(default_factory=list)
    required_assets: List[Dict[str, Any]] = field(default_factory=list)
    preliminary_experiments: List[Dict[str, Any]] = field(default_factory=list)
    risks: List[str] = field(default_factory=list)
    provenance: Dict[str, Any] = field(default_factory=dict)
    version: int = 1
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸ã€‚"""
        return {
            'id': self.id,
            'title': self.title,
            'core_hypothesis': self.core_hypothesis,
            'initial_innovation_points': self.initial_innovation_points,
            'source_trigger_nodes': self.source_trigger_nodes,
            'expected_contribution': self.expected_contribution,
            'required_assets': self.required_assets,
            'preliminary_experiments': self.preliminary_experiments,
            'risks': self.risks,
            'provenance': self.provenance,
            'version': self.version
        }


@dataclass
class NoveltyCritique:
    """æ–°é¢–æ€§è¯„å®¡ç»“æœã€‚"""

    idea_id: str
    novelty_score: float
    facet_scores: Dict[str, float] = field(default_factory=dict)
    similar_works: List[Dict[str, Any]] = field(default_factory=list)
    difference_claims: List[Dict[str, Any]] = field(default_factory=list)
    method: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸ã€‚"""
        return {
            'idea_id': self.idea_id,
            'novelty_score': self.novelty_score,
            'facet_scores': self.facet_scores,
            'similar_works': self.similar_works,
            'difference_claims': self.difference_claims,
            'method': self.method
        }


@dataclass
class FeasibilityCritique:
    """å¯è¡Œæ€§è¯„å®¡ç»“æœã€‚"""

    idea_id: str
    feasibility_score: float
    relevance: str = ""
    required_assets: List[Dict[str, Any]] = field(default_factory=list)
    potential_risks: List[Dict[str, Any]] = field(default_factory=list)
    graph_checks: Dict[str, Any] = field(default_factory=dict)
    dimension_scores: Dict[str, float] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸ã€‚"""
        return {
            'idea_id': self.idea_id,
            'feasibility_score': self.feasibility_score,
            'relevance': self.relevance,
            'required_assets': self.required_assets,
            'potential_risks': self.potential_risks,
            'graph_checks': self.graph_checks,
            'dimension_scores': self.dimension_scores
        }


@dataclass
class RefinementPrompt:
    """ç²¾ç‚¼æŒ‡ä»¤ã€‚"""

    idea_id: str
    decision: str  # revise | split | merge | discard | accept
    instructions: List[str] = field(default_factory=list)
    rationale: str = ""
    acceptance_criteria: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸ã€‚"""
        return {
            'idea_id': self.idea_id,
            'decision': self.decision,
            'instructions': self.instructions,
            'rationale': self.rationale,
            'acceptance_criteria': self.acceptance_criteria
        }


# =========================
# åŸºç±»ä¸æ™ºèƒ½ä½“ï¼ˆéª¨æ¶ï¼‰
# =========================


class BaseIdeaAgent:
    """Ideaç”Ÿæˆå­ç³»ç»Ÿçš„æ™ºèƒ½ä½“åŸºç±»ã€‚

    ç›®æ ‡:
        - ç»Ÿä¸€æ³¨å…¥ LLMFactory ä¸ AcademicPaperDatabaseï¼Œæä¾›åŸºæœ¬å·¥å…·/é…ç½®çš„æŒæœ‰ã€‚

    è¾“å…¥:
        - llm_factory: ç»Ÿä¸€çš„LLMè°ƒç”¨å·¥å‚ã€‚
        - db: å­¦æœ¯æ•°æ®åº“å®ä¾‹ã€‚
        - config: æ™ºèƒ½ä½“æœ¬åœ°é…ç½®ï¼ˆæ¸©åº¦ã€æ£€ç´¢Kã€å¹¶å‘ç­‰ï¼‰ã€‚

    æ³¨æ„äº‹é¡¹:
        - åªå®šä¹‰æ¥å£ä¸ä¾èµ–ï¼Œä¸å®ç°ç‰¹å®šä¸šåŠ¡é€»è¾‘ã€‚
    """

    def __init__(self, name: str, llm_factory: LLMFactory, db: AcademicPaperDatabase, config: Optional[AgentConfig] = None):
        self.name = name
        self.llm = llm_factory
        self.db = db
        self.config = config


class IdeaMinerAgent(BaseIdeaAgent):
    """ç¬¬ä¸€é˜¶æ®µï¼šæœºä¼šå›¾è°±æ„å»ºã€‚

    æ ¸å¿ƒèŒè´£:
        - ä» `final_result.full_document` ä¸ `enriched_outline` æŠ½å–å®ä½“ä¸å…³ç³»ï¼Œæ„å»º `SemanticOpportunityGraph`ã€‚
        - è¯†åˆ«"ç»“æ„ç©ºæ´/å†²çª/æ–­è£‚ç¾¤é›†"ç­‰æœºä¼šä¿¡å·ï¼Œå†™å…¥ `gaps`ã€‚

    å…³é”®å®ç°æ€è·¯:
        - å®ä½“æŠ½å–: ç»“åˆè§„åˆ™/LLMå¯¹æ­£æ–‡ä¸å…³é”®è¯è¿›è¡Œæ ‡æ³¨ï¼›å¯¹åŒä¹‰/åˆ«åè¿›è¡Œè§„èŒƒåŒ–ã€‚
        - å…³ç³»æŠ½å–: æŒ‰æ¨¡æ¿è¯†åˆ« solves/improves_on/uses_dataset/evaluated_by/critiques/...ï¼›è®°å½•è¯æ®é”šç‚¹ä¸ç½®ä¿¡åº¦ã€‚
        - å›¾è°±æ•´ç†: å»é‡åˆå¹¶ã€æ„é€ ç´¢å¼•(byType/byToken)ã€è®¡ç®—salience(é¢‘æ¬¡/ä¸­å¿ƒæ€§/å¼•ç”¨å¯†åº¦)ã€‚
        - æœºä¼šæ£€æµ‹: åŸºäºç›¸ä¼¼åº¦ä¸ç»“æ„æœç´¢å®šä½è¿ç§»/ç»„åˆ/åè½¬ç­‰è§¦å‘æ¨¡å¼ç¼ºå£ã€‚

    æ³¨æ„äº‹é¡¹:
        - ä¸¥æ ¼ä¿ç•™ `provenance` ä»¥ä¾¿è¿½æº¯ï¼›å†²çªè¾¹ä¸åº”è¢«æ¸…æ´—ã€‚
        - å¯æŒ‰ç« èŠ‚/å­ç« èŠ‚æ‰¹é‡åŒ–å¤„ç†ï¼Œé™ä½ä¸Šä¸‹æ–‡çª—å£å‹åŠ›ã€‚
    """

    async def build_opportunity_graph(self, final_result: Dict[str, Any], enriched_outline: Dict[str, Any]) -> SemanticOpportunityGraph:
        """æ„å»ºè¯­ä¹‰æœºä¼šå›¾è°±ã€‚

        è¾“å…¥:
            - final_result: ä¸Šæ¸¸æ•´åˆç»“æœï¼Œè‡³å°‘åŒ…å« `full_document`ã€‚
            - enriched_outline: ä¸°å¯Œå¤§çº²ï¼ˆå«ç« èŠ‚å…³é”®è¯ã€ç ”ç©¶é‡ç‚¹ç­‰ï¼‰ã€‚

        è¾“å‡º:
            - SemanticOpportunityGraph: å« nodes/edges/indices/gaps/provenance çš„å®Œæ•´å›¾è°±å¯¹è±¡ã€‚

        å®ç°æ­¥éª¤å»ºè®®:
            1) è§£æ `full_document`ï¼Œåˆ†ç« èŠ‚/æ®µè½åˆ‡ç‰‡ï¼Œè¿›è¡Œå®ä½“å€™é€‰æŠ½å–ã€‚
            2) ç»“åˆ `enriched_outline` çš„å…³é”®è¯/ç ”ç©¶é‡ç‚¹ï¼Œåšå®ä½“è§„èŒƒåŒ–ä¸ç±»å‹åˆ¤å®šã€‚
            3) åŸºäºå¥æ³•/æ¨¡æ¿/LLMåˆ¤å®šå…³ç³»ï¼Œå¹¶èšåˆè¯æ®å½¢æˆè¾¹ï¼Œä¼°è®¡ weight/confidenceã€‚
            4) æ„å»º indices ä¸ provenanceï¼Œæ‰§è¡Œæœºä¼šæ£€æµ‹ï¼Œå¡«å…… gapsã€‚
        """
        # åˆ›å»ºç©ºå›¾è°±
        graph = create_semantic_graph()
        
        # åˆ†æ­¥éª¤æ„å»º
        await self._extract_entities(graph, final_result, enriched_outline)
        await self._extract_relations(graph, final_result, enriched_outline)
        await self._compute_salience(graph)
        await self._detect_opportunities(graph)
        
        return graph

    async def _extract_entities(self, graph: SemanticOpportunityGraph, final_result: Dict[str, Any], enriched_outline: Dict[str, Any]) -> None:
        """ç¬¬ä¸€æ­¥ï¼šå®ä½“æŠ½å–ä¸è§„èŒƒåŒ–ã€‚
        
        è¾“å…¥:
            - graph: ç›®æ ‡å›¾è°±ï¼ˆå°†è¢«å°±åœ°ä¿®æ”¹ï¼‰ã€‚
            - final_result: ç»¼è¿°æ­£æ–‡ç­‰å†…å®¹ã€‚
            - enriched_outline: ä¸°å¯Œå¤§çº²ã€‚
            
        å®ç°æ€è·¯:
            1) ä» `final_document` æŒ‰ç« èŠ‚åˆ‡ç‰‡ï¼Œä½¿ç”¨ LLM è¯†åˆ«æ–¹æ³•/ä»»åŠ¡/æ•°æ®é›†/æŒ‡æ ‡/é—®é¢˜ç­‰å®ä½“ã€‚
            2) ç»“åˆ `enriched_outline` çš„å…³é”®è¯è¿›è¡Œç§å­æ‰©å±•ä¸ç±»å‹æ ‡æ³¨ã€‚
            3) å®ä½“è§„èŒƒåŒ–ï¼šå¤„ç†åˆ«åã€ç¼©å†™ã€åŒä¹‰è¯åˆå¹¶ï¼Œé¿å…é‡å¤èŠ‚ç‚¹ã€‚
            4) è°ƒç”¨ `add_graph_node` å°†æ ‡å‡†åŒ–å®ä½“åŠ å…¥å›¾è°±ã€‚
        
        æ³¨æ„äº‹é¡¹:
            - ä¿ç•™å®ä½“å‡ºç°çš„æ–‡æ¡£ä½ç½®ä½œä¸º `evidence`ã€‚
            - é¢„è®¾å®ä½“ç±»å‹ä¼˜å…ˆçº§ï¼šMethod > Task > Dataset > Metric > Problemã€‚
        """
        print("  ğŸ” å¼€å§‹å®ä½“æŠ½å–ä¸è§„èŒƒåŒ–")
        
        # æ­¥éª¤1ï¼šæ”¶é›†ç§å­å…³é”®è¯
        seed_keywords = self._collect_seed_keywords(enriched_outline)
        print(f"    ğŸ“‹ æ”¶é›†åˆ° {len(seed_keywords)} ä¸ªç§å­å…³é”®è¯")
        
        # æ­¥éª¤2ï¼šä»ç»¼è¿°æ­£æ–‡ä¸­åˆ‡åˆ†ç« èŠ‚å¹¶æŠ½å–å®ä½“
        full_document = final_result.get("full_document", "")
        chapters = self._split_document_by_chapters(full_document)
        print(f"    ğŸ“„ å°†æ–‡æ¡£åˆ‡åˆ†ä¸º {len(chapters)} ä¸ªç« èŠ‚")
        
        # æ­¥éª¤3ï¼šå¹¶è¡Œç« èŠ‚å®ä½“æŠ½å–
        import asyncio
        all_entities = []
        
        # åˆ›å»ºæ‰€æœ‰ç« èŠ‚çš„å¹¶è¡Œä»»åŠ¡
        tasks = []
        for chapter_num, chapter_content in chapters.items():
            task = self._extract_entities_from_chapter(
                chapter_num, chapter_content, seed_keywords
            )
            tasks.append(task)
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰LLMè°ƒç”¨
        if tasks:
            print(f"    ğŸš€ å¯åŠ¨ {len(tasks)} ä¸ªç« èŠ‚çš„å¹¶è¡Œå®ä½“æŠ½å–")
            chapter_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            for i, result in enumerate(chapter_results):
                if isinstance(result, Exception):
                    print(f"      âŒ ç« èŠ‚ {list(chapters.keys())[i]} æŠ½å–å¤±è´¥: {str(result)}")
                else:
                    all_entities.extend(result)
        
        print(f"    ğŸ¯ å…±æŠ½å– {len(all_entities)} ä¸ªåŸå§‹å®ä½“")
        
        # æ­¥éª¤4ï¼šå®ä½“è§„èŒƒåŒ–ä¸å»é‡
        normalized_entities = await self._normalize_and_deduplicate_entities(all_entities)
        print(f"    âœ¨ è§„èŒƒåŒ–åä¿ç•™ {len(normalized_entities)} ä¸ªå®ä½“")
        
        # æ­¥éª¤5ï¼šæ·»åŠ åˆ°å›¾è°±
        for entity in normalized_entities:
            # å­—ç¬¦ä¸²åˆ°æšä¸¾çš„æ˜ å°„
            type_mapping = {
                "Method": GraphNodeType.METHOD,
                "Task": GraphNodeType.TASK,
                "Dataset": GraphNodeType.DATASET,
                "Metric": GraphNodeType.METRIC,
                "Paper": GraphNodeType.PAPER,
                "Problem": GraphNodeType.PROBLEM,
                "Domain": GraphNodeType.DOMAIN
            }
            
            entity_type_str = entity["type"]
            entity_type = type_mapping.get(entity_type_str, GraphNodeType.METHOD)  # é»˜è®¤ä¸ºMETHOD
            
            node = GraphNode(
                id=entity["id"],
                type=entity_type,
                name=entity["name"],
                aliases=entity.get("aliases", []),
                evidence=entity.get("evidence", []),
                salience=entity.get("salience", 0.0)
            )
            add_graph_node(graph, node)
        
        print(f"    âœ… æˆåŠŸæ·»åŠ  {len(normalized_entities)} ä¸ªèŠ‚ç‚¹åˆ°å›¾è°±")

    async def _extract_relations(self, graph: SemanticOpportunityGraph, final_result: Dict[str, Any], enriched_outline: Dict[str, Any]) -> None:
        """ç¬¬äºŒæ­¥ï¼šå…³ç³»æŠ½å–ä¸è¯æ®ç»‘å®šã€‚
        
        è¾“å…¥:
            - graph: å·²æœ‰å®ä½“çš„å›¾è°±ã€‚
            - final_result: ç»¼è¿°æ­£æ–‡ç­‰å†…å®¹ã€‚
            - enriched_outline: ä¸°å¯Œå¤§çº²ã€‚
            
        å®ç°æ€è·¯:
            1) ä¸ºæ¯å¯¹å®ä½“ç”Ÿæˆå…³ç³»å€™é€‰ï¼ŒåŸºäºè·ç¦»ã€å…±ç°ã€å¥æ³•æ¨¡å¼ç­‰å¯å‘å¼è¿‡æ»¤ã€‚
            2) ä½¿ç”¨ LLM æˆ–è§„åˆ™æ¨¡æ¿åˆ¤å®šå…³ç³»ç±»å‹ï¼ˆsolves/improves_on/uses_dataset/...ï¼‰ã€‚
            3) ä¼°è®¡å…³ç³»æƒé‡ï¼ˆåŸºäºè¯æ®å¼ºåº¦/é¢‘æ¬¡ï¼‰ä¸ç½®ä¿¡åº¦ï¼ˆåŸºäºæ¨¡å‹è¾“å‡ºï¼‰ã€‚
            4) è°ƒç”¨ `add_graph_edge` å°†å…³ç³»è¾¹åŠ å…¥å›¾è°±ã€‚
        
        æ³¨æ„äº‹é¡¹:
            - è®°å½•æ¯æ¡è¾¹çš„æ–‡æ¡£è¯æ®ï¼ˆquote/locï¼‰ï¼›å†²çªè¾¹ï¼ˆcontradicts/critiquesï¼‰éœ€ç‰¹åˆ«ä¿ç•™ã€‚
            - å¯æŒ‰ç« èŠ‚å¹¶è¡Œå¤„ç†ï¼Œé¿å…ä¸Šä¸‹æ–‡çª—å£è¶…é™ã€‚
        """
        print("  ğŸ”— å¼€å§‹å…³ç³»æŠ½å–ä¸è¯æ®ç»‘å®š")
        
        # è·å–å·²æœ‰çš„èŠ‚ç‚¹
        nodes = list(graph.nodes())
        print(f"    ğŸ“Š å›¾è°±ä¸­å…±æœ‰ {len(nodes)} ä¸ªèŠ‚ç‚¹")
        
        # æ­¥éª¤2ï¼šåŸºäºLLMçš„å¹¶è¡Œå…³ç³»æŠ½å–
        pattern_relations = await self._extract_pattern_relations_parallel(graph, final_result)
        print(f"    ğŸ“ åŸºäºLLMæŠ½å– {len(pattern_relations)} ä¸ªå…³ç³»")
        
        # æ­¥éª¤3ï¼šåŸºäºæ•°æ®åº“æ£€ç´¢çš„å…³ç³»éªŒè¯
        verified_relations = await self._verify_relations_with_database(pattern_relations)
        print(f"    âœ… éªŒè¯åä¿ç•™ {len(verified_relations)} ä¸ªå…³ç³»")
        
        # æ­¥éª¤4ï¼šæ·»åŠ å…³ç³»åˆ°å›¾è°±
        for relation in verified_relations:
            # å­—ç¬¦ä¸²åˆ°æšä¸¾çš„å®‰å…¨è½¬æ¢
            relation_str = relation["relation"]
            relation_mapping = {
                "improves_on": GraphEdgeRelation.IMPROVES_ON,
                "uses_dataset": GraphEdgeRelation.USES_DATASET,
                "evaluated_by": GraphEdgeRelation.EVALUATED_BY,
                "applies_to": GraphEdgeRelation.SOLVES,  # æ˜ å°„åˆ°ç°æœ‰çš„solves
                "requires": GraphEdgeRelation.SIMILAR_TO,  # æ˜ å°„åˆ°ç›¸ä¼¼å…³ç³»
                "enables": GraphEdgeRelation.EXTENSION_OF,  # æ˜ å°„åˆ°æ‰©å±•å…³ç³»
                "addresses": GraphEdgeRelation.SOLVES,  # æ˜ å°„åˆ°è§£å†³å…³ç³»
                "similar_to": GraphEdgeRelation.SIMILAR_TO
            }
            
            relation_enum = relation_mapping.get(relation_str, GraphEdgeRelation.SIMILAR_TO)  # é»˜è®¤ç›¸ä¼¼å…³ç³»
            
            edge = GraphEdge(
                src=relation["src"],
                dst=relation["dst"], 
                relation=relation_enum,
                weight=relation.get("weight", 0.5),
                confidence=relation.get("confidence", 0.5),
                evidence=relation.get("evidence", []),
                notes=relation.get("notes", "")
            )
            add_graph_edge(graph, edge)
        
        print(f"    âœ… æˆåŠŸæ·»åŠ  {len(verified_relations)} æ¡è¾¹åˆ°å›¾è°±")

    async def _compute_salience(self, graph: SemanticOpportunityGraph) -> None:
        """ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—èŠ‚ç‚¹é‡è¦æ€§ã€‚
        
        è¾“å…¥:
            - graph: å·²æœ‰èŠ‚ç‚¹å’Œè¾¹çš„å›¾è°±ã€‚
            
        å®ç°æ€è·¯:
            1) åˆ©ç”¨ NetworkX è®¡ç®—ä¸­å¿ƒæ€§æŒ‡æ ‡ï¼šåº¦ä¸­å¿ƒæ€§ã€æ¥è¿‘ä¸­å¿ƒæ€§ã€ä»‹æ•°ä¸­å¿ƒæ€§ã€PageRankç­‰ã€‚
            2) ç»“åˆèŠ‚ç‚¹é¢‘æ¬¡ï¼ˆevidenceæ•°é‡ï¼‰ã€å¼•ç”¨å¯†åº¦ç­‰å¯å‘å¼æŒ‡æ ‡ã€‚
            3) åŠ æƒèåˆå¾—åˆ° `salience` åˆ†æ•°ï¼Œæ›´æ–°èŠ‚ç‚¹å±æ€§ã€‚
        
        æ³¨æ„äº‹é¡¹:
            - ä¸åŒç±»å‹èŠ‚ç‚¹å¯ä½¿ç”¨ä¸åŒæƒé‡ï¼šMethod/Task åå‘ä¸­å¿ƒæ€§ï¼ŒDataset/Metric åå‘é¢‘æ¬¡ã€‚
        """
        print("  ğŸ“Š å¼€å§‹è®¡ç®—èŠ‚ç‚¹é‡è¦æ€§")
        
        if graph.number_of_nodes() == 0:
            print("    âš ï¸ å›¾è°±ä¸ºç©ºï¼Œè·³è¿‡é‡è¦æ€§è®¡ç®—")
            return
        
        # è®¡ç®—å„ç§ä¸­å¿ƒæ€§æŒ‡æ ‡
        centrality_scores = {}
        
        if graph.number_of_edges() > 0:
            # åº¦ä¸­å¿ƒæ€§
            degree_centrality = nx.degree_centrality(graph)
            centrality_scores['degree'] = degree_centrality
            
            # PageRank
            try:
                pagerank = nx.pagerank(graph, weight='weight')
                centrality_scores['pagerank'] = pagerank
            except:
                centrality_scores['pagerank'] = {node: 0.0 for node in graph.nodes()}
            
            # ä»‹æ•°ä¸­å¿ƒæ€§ï¼ˆå¯¹äºå¤§å›¾å¯èƒ½å¾ˆæ…¢ï¼Œè¿™é‡Œç®€åŒ–ï¼‰
            if graph.number_of_nodes() < 100:
                betweenness = nx.betweenness_centrality(graph, weight='weight')
                centrality_scores['betweenness'] = betweenness
            else:
                centrality_scores['betweenness'] = {node: 0.0 for node in graph.nodes()}
        else:
            # æ²¡æœ‰è¾¹çš„æƒ…å†µï¼Œæ‰€æœ‰ä¸­å¿ƒæ€§ä¸º0
            for metric in ['degree', 'pagerank', 'betweenness']:
                centrality_scores[metric] = {node: 0.0 for node in graph.nodes()}
        
        # æ›´æ–°æ¯ä¸ªèŠ‚ç‚¹çš„salience
        for node_id in graph.nodes():
            node_data = graph.nodes[node_id]
            node_type = node_data.get('type', 'Method')
            evidence_count = len(node_data.get('evidence', []))
            
            # åŸºç¡€åˆ†æ•°ï¼šè¯æ®æ•°é‡
            evidence_score = min(evidence_count * 0.1, 1.0)
            
            # ä¸­å¿ƒæ€§åˆ†æ•°
            degree_score = centrality_scores['degree'].get(node_id, 0.0)
            pagerank_score = centrality_scores['pagerank'].get(node_id, 0.0) * 10  # æ”¾å¤§PageRank
            betweenness_score = centrality_scores['betweenness'].get(node_id, 0.0)
            
            # æ ¹æ®èŠ‚ç‚¹ç±»å‹è°ƒæ•´æƒé‡
            if node_type in ['Method', 'Task']:
                # æ–¹æ³•å’Œä»»åŠ¡åå‘ä¸­å¿ƒæ€§
                salience = (
                    0.3 * evidence_score + 
                    0.3 * degree_score + 
                    0.3 * pagerank_score + 
                    0.1 * betweenness_score
                )
            elif node_type in ['Dataset', 'Metric']:
                # æ•°æ®é›†å’ŒæŒ‡æ ‡åå‘é¢‘æ¬¡
                salience = (
                    0.6 * evidence_score + 
                    0.2 * degree_score + 
                    0.2 * pagerank_score
                )
            else:
                # å…¶ä»–ç±»å‹ä½¿ç”¨å¹³å‡æƒé‡
                salience = (
                    0.4 * evidence_score + 
                    0.3 * degree_score + 
                    0.3 * pagerank_score
                )
            
            # æ›´æ–°èŠ‚ç‚¹å±æ€§
            graph.nodes[node_id]['salience'] = min(salience, 1.0)
        
        print(f"    âœ… å®Œæˆ {graph.number_of_nodes()} ä¸ªèŠ‚ç‚¹çš„é‡è¦æ€§è®¡ç®—")

    async def _detect_opportunities(self, graph: SemanticOpportunityGraph) -> None:
        """ç¬¬å››æ­¥ï¼šæœºä¼šæ£€æµ‹ä¸gapè¯†åˆ«ã€‚
        
        è¾“å…¥:
            - graph: å®Œæ•´æ„å»ºçš„å›¾è°±ã€‚
            
        å®ç°æ€è·¯:
            1) è¿ç§»æœºä¼šï¼šé«˜ç›¸ä¼¼Taské—´ç¼ºå°‘Methodè¿ç§»è¾¹ã€‚
            2) ç»„åˆæœºä¼šï¼šå¤šä¸ªMethodè§£å†³åŒTaskçš„ä¸åŒå­é—®é¢˜ã€‚
            3) åè½¬æœºä¼šï¼šå­˜åœ¨critiqueå…³ç³»ï¼Œä½†ç¼ºå°‘æ”¹è¿›æ–¹æ¡ˆã€‚
            4) è¯„æµ‹ç©ºç¼ºï¼šTaskç¼ºå°‘åˆé€‚Metricï¼Œæˆ–Metricè¿‡æ—¶ã€‚
            5) æ•°æ®å¢å¼ºï¼šTaskæœ‰Methodä½†ç¼ºå°‘è¶³å¤ŸDatasetã€‚
        
        æ³¨æ„äº‹é¡¹:
            - åˆ©ç”¨ NetworkX çš„è·¯å¾„æŸ¥æ‰¾ã€å­å›¾æ£€æµ‹ç­‰ç®—æ³•è¾…åŠ©gapå‘ç°ã€‚
            - æ¯ä¸ªgapè®°å½•è§¦å‘æ¨¡å¼ã€ç›¸å…³èŠ‚ç‚¹ã€ç½®ä¿¡åº¦è¯„ä¼°ã€‚
        """
        print("  ğŸ” å¼€å§‹æœºä¼šæ£€æµ‹ä¸gapè¯†åˆ«")
        
        gaps = []
        
        # è·å–ä¸åŒç±»å‹çš„èŠ‚ç‚¹
        methods = find_nodes_by_type(graph, GraphNodeType.METHOD)
        tasks = find_nodes_by_type(graph, GraphNodeType.TASK)
        datasets = find_nodes_by_type(graph, GraphNodeType.DATASET)
        metrics = find_nodes_by_type(graph, GraphNodeType.METRIC)
        
        print(f"    ğŸ“Š èŠ‚ç‚¹ç»Ÿè®¡: {len(methods)}ä¸ªæ–¹æ³•, {len(tasks)}ä¸ªä»»åŠ¡, {len(datasets)}ä¸ªæ•°æ®é›†, {len(metrics)}ä¸ªæŒ‡æ ‡")
        
        # æœºä¼š1ï¼šè¿ç§»æœºä¼šæ£€æµ‹
        transfer_gaps = self._detect_transfer_opportunities(graph, methods, tasks)
        gaps.extend(transfer_gaps)
        print(f"    ğŸ”„ å‘ç° {len(transfer_gaps)} ä¸ªè¿ç§»æœºä¼š")
        
        # æœºä¼š2ï¼šç»„åˆæœºä¼šæ£€æµ‹
        composition_gaps = self._detect_composition_opportunities(graph, methods, tasks)
        gaps.extend(composition_gaps)
        print(f"    ğŸ”— å‘ç° {len(composition_gaps)} ä¸ªç»„åˆæœºä¼š")
        
        # æœºä¼š3ï¼šåè½¬æœºä¼šæ£€æµ‹
        reverse_gaps = self._detect_reverse_opportunities(graph)
        gaps.extend(reverse_gaps)
        print(f"    ğŸ”„ å‘ç° {len(reverse_gaps)} ä¸ªåè½¬æœºä¼š")
        
        # æœºä¼š4ï¼šè¯„æµ‹ç©ºç¼ºæ£€æµ‹
        evaluation_gaps = self._detect_evaluation_gaps(graph, tasks, metrics)
        gaps.extend(evaluation_gaps)
        print(f"    ğŸ“ å‘ç° {len(evaluation_gaps)} ä¸ªè¯„æµ‹ç©ºç¼º")
        
        # æœºä¼š5ï¼šæ•°æ®å¢å¼ºæœºä¼šæ£€æµ‹
        data_gaps = self._detect_data_enhancement_opportunities(graph, tasks, datasets)
        gaps.extend(data_gaps)
        print(f"    ğŸ“Š å‘ç° {len(data_gaps)} ä¸ªæ•°æ®å¢å¼ºæœºä¼š")
        
        # å°†gapsæ·»åŠ åˆ°å›¾è°±
        for gap in gaps:
            add_opportunity_gap(graph, gap)
        
        print(f"    âœ… æ€»å…±è¯†åˆ«å‡º {len(gaps)} ä¸ªæœºä¼šgap")

    def _collect_seed_keywords(self, enriched_outline: Dict[str, Any]) -> List[str]:
        """ä»ä¸°å¯Œå¤§çº²ä¸­æ”¶é›†ç§å­å…³é”®è¯ã€‚"""
        keywords = []
        
        # ä»ç« èŠ‚ç»“æ„ä¸­æå–å…³é”®è¯
        # enriched_outlineæœ¬èº«å°±æ˜¯parsed_structureï¼Œä¸éœ€è¦å†åµŒå¥—è·å–
        chapters = enriched_outline.get("chapters", {})
        
        for chapter_id, chapter in chapters.items():
            # ç« èŠ‚å…³é”®è¯
            chapter_keywords = chapter.get("keywords", [])
            keywords.extend(chapter_keywords)
            
            # ç ”ç©¶é‡ç‚¹ä¸­çš„å…³é”®æ¦‚å¿µ
            research_focus = chapter.get("research_focus", [])
            for focus in research_focus:
                # ç®€å•æå–ï¼šä½¿ç”¨å¸¸è§åˆ†éš”ç¬¦åˆ†å‰²
                focus_keywords = self._extract_keywords_from_text(focus)
                keywords.extend(focus_keywords)
            
            # å­ç« èŠ‚å…³é”®è¯
            subsections = chapter.get("subsections", {})
            for subsection_id, subsection in subsections.items():
                sub_keywords = subsection.get("key_points", [])
                for point in sub_keywords:
                    point_keywords = self._extract_keywords_from_text(point)
                    keywords.extend(point_keywords)
        
        # å»é‡å¹¶è¿‡æ»¤
        unique_keywords = list(set(keywords))
        filtered_keywords = [kw for kw in unique_keywords if len(kw.strip()) > 2]
        
        return filtered_keywords[:100]  # é™åˆ¶æ•°é‡
    
    def _extract_keywords_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®æœ¯è¯­ã€‚"""
        import re
        
        # ç®€å•çš„å…³é”®è¯æå–è§„åˆ™
        keywords = []
        
        # æå–ä¸“æœ‰åè¯å’ŒæŠ€æœ¯æœ¯è¯­ï¼ˆå¤§å†™å­—æ¯å¼€å¤´çš„è¯ç»„ï¼‰
        patterns = [
            r'\b[A-Z][a-z]*(?:\s+[A-Z][a-z]*)*\b',  # ä¸“æœ‰åè¯
            r'\b[A-Z]{2,}\b',  # ç¼©å†™
            r'\b\w*(?:Model|Network|Transformer|BERT|GPT|LLM)\w*\b',  # æ¨¡å‹ç›¸å…³
            r'\b\w*(?:Dataset|Benchmark|Task|Metric)\w*\b',  # æ•°æ®ç›¸å…³
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            keywords.extend(matches)
        
        return [kw.strip() for kw in keywords if len(kw.strip()) > 2]
    
    def _split_document_by_chapters(self, full_document: str) -> Dict[str, str]:
        """å°†å®Œæ•´æ–‡æ¡£æŒ‰ç« èŠ‚åˆ‡åˆ†ã€‚"""
        import re
        
        chapters = {}
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ç« èŠ‚æ ‡é¢˜
        chapter_pattern = r'^#+\s*(\d+(?:\.\d+)*)\s+(.+)$'
        lines = full_document.split('\n')
        
        current_chapter = None
        current_content = []
        
        for line in lines:
            match = re.match(chapter_pattern, line.strip())
            if match:
                # ä¿å­˜å‰ä¸€ç« èŠ‚
                if current_chapter and current_content:
                    chapters[current_chapter] = '\n'.join(current_content)
                
                # å¼€å§‹æ–°ç« èŠ‚
                chapter_num = match.group(1)
                if '.' not in chapter_num:  # åªå¤„ç†ä¸€çº§ç« èŠ‚
                    current_chapter = chapter_num
                    current_content = [line]
                else:
                    if current_chapter:
                        current_content.append(line)
            else:
                if current_chapter:
                    current_content.append(line)
        
        # ä¿å­˜æœ€åä¸€ç« 
        if current_chapter and current_content:
            chapters[current_chapter] = '\n'.join(current_content)
        
        return chapters
    
    async def _extract_entities_from_chapter(self, chapter_num: str, chapter_content: str, seed_keywords: List[str]) -> List[Dict[str, Any]]:
        """ä»å•ä¸ªç« èŠ‚ä¸­æŠ½å–å®ä½“ã€‚"""
        entities = []
        
        # ä½¿ç”¨è§„åˆ™ + LLM çš„æ–¹å¼æŠ½å–å®ä½“
        
        # è§„åˆ™1ï¼šä»ç§å­å…³é”®è¯ä¸­åŒ¹é…
        for keyword in seed_keywords:
            if keyword.lower() in chapter_content.lower():
                entity_type = self._classify_entity_type(keyword)
                entities.append({
                    "name": keyword,
                    "type": entity_type,
                    "source": "seed_keyword",
                    "chapter": chapter_num,
                    "confidence": 0.8,
                    "evidence": [{
                        "source": "survey",
                        "loc": f"ch{chapter_num}",
                        "quote": self._extract_context_around_keyword(chapter_content, keyword)
                    }]
                })
        
        # è§„åˆ™2ï¼šæ¨¡å¼åŒ¹é…å¸¸è§å®ä½“ç±»å‹
        pattern_entities = self._extract_entities_by_patterns(chapter_content, chapter_num)
        entities.extend(pattern_entities)
        
        # è§„åˆ™3ï¼šä½¿ç”¨ LLM è¿›ä¸€æ­¥æŠ½å–ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…å¯è°ƒç”¨ self.llmï¼‰
        # TODO: è¿™é‡Œå¯ä»¥è°ƒç”¨ LLM è¿›è¡Œæ›´ç²¾ç¡®çš„å®ä½“è¯†åˆ«
        llm_entities = await self._extract_entities_with_llm(chapter_content, chapter_num)
        entities.extend(llm_entities)
        
        return entities
    
    def _classify_entity_type(self, keyword: str) -> str:
        """æ ¹æ®å…³é”®è¯åˆ†ç±»å®ä½“ç±»å‹ã€‚"""
        keyword_lower = keyword.lower()
        
        # ä»»åŠ¡ç›¸å…³ - æ‰©å±•å…³é”®è¯
        if any(term in keyword_lower for term in [
            'task', 'generation', 'translation', 'classification', 'qa', 'question answering',
            'summarization', 'reasoning', 'understanding', 'inference', 'prediction',
            'fine-tuning', 'training', 'learning', 'alignment', 'evaluation',
            'processing', 'analysis', 'synthesis', 'optimization', 'improvement',
            'enhancement', 'adaptation', 'personalization', 'customization'
        ]):
            return "Task"
        
        # è¯„ä»·æŒ‡æ ‡ç›¸å…³ - æ‰©å±•å…³é”®è¯
        if any(term in keyword_lower for term in [
            'metric', 'score', 'bleu', 'rouge', 'accuracy', 'perplexity',
            'precision', 'recall', 'f1', 'performance', 'quality', 'effectiveness',
            'efficiency', 'benchmarking', 'evaluation', 'assessment', 'measurement'
        ]):
            return "Metric"
        
        # æ•°æ®é›†ç›¸å…³
        if any(term in keyword_lower for term in [
            'dataset', 'corpus', 'benchmark', 'data', 'collection', 'training data'
        ]):
            return "Dataset"
        
        # é—®é¢˜/æŒ‘æˆ˜ç›¸å…³
        if any(term in keyword_lower for term in [
            'problem', 'challenge', 'issue', 'bias', 'hallucination',
            'limitation', 'difficulty', 'barrier', 'obstacle'
        ]):
            return "Problem"
        
        # æ–¹æ³•/æ¨¡å‹ç›¸å…³ - æ”¾åœ¨æœ€åï¼Œä½œä¸ºæ›´å…·ä½“çš„åˆ†ç±»
        if any(term in keyword_lower for term in [
            'model', 'transformer', 'bert', 'gpt', 'network', 'algorithm', 
            'attention', 'embedding', 'architecture', 'framework', 'approach',
            'technique', 'method', 'mechanism', 'strategy', 'procedure'
        ]):
            return "Method"
        
        # æ£€æŸ¥æ˜¯å¦å¯èƒ½æ˜¯ä»»åŠ¡ï¼ˆåŸºäºåŠ¨è¯æ¨¡å¼ï¼‰
        if any(keyword_lower.endswith(suffix) for suffix in ['ing', 'ion', 'ment', 'ance', 'ence']):
            return "Task"
        
        # é»˜è®¤ä¸ºæ–¹æ³•ç±»å‹
        return "Method"
    
    def _extract_context_around_keyword(self, text: str, keyword: str, context_size: int = 100) -> str:
        """æå–å…³é”®è¯å‘¨å›´çš„ä¸Šä¸‹æ–‡ã€‚"""
        import re
        
        # æ‰¾åˆ°å…³é”®è¯çš„ä½ç½®
        pattern = re.compile(re.escape(keyword), re.IGNORECASE)
        match = pattern.search(text)
        
        if match:
            start = max(0, match.start() - context_size)
            end = min(len(text), match.end() + context_size)
            context = text[start:end].strip()
            return f"...{context}..."
        
        return keyword
    
    def _extract_entities_by_patterns(self, chapter_content: str, chapter_num: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨æ¨¡å¼åŒ¹é…æŠ½å–å®ä½“ã€‚"""
        import re
        entities = []
        
        # æ¨¡å¼1ï¼šæ¨¡å‹åç§°ï¼ˆé€šå¸¸æ˜¯å¤§å†™æˆ–é¦–å­—æ¯å¤§å†™ï¼‰
        model_patterns = [
            r'\b(?:GPT|BERT|T5|RoBERTa|ELECTRA|DeBERTa|ALBERT|DistilBERT|XLNet|Transformer|LLaMA|PaLM|ChatGPT|GPT-\d+|BERT-\w+)(?:-\w+)*\b',
            r'\b[A-Z][a-zA-Z]*(?:Model|Net|Former|Bert|GPT)\b'
        ]
        
        for pattern in model_patterns:
            matches = re.finditer(pattern, chapter_content, re.IGNORECASE)
            for match in matches:
                name = match.group()
                entities.append({
                    "name": name,
                    "type": "Method",
                    "source": "pattern_match",
                    "chapter": chapter_num,
                    "confidence": 0.7,
                    "evidence": [{
                        "source": "survey",
                        "loc": f"ch{chapter_num}",
                        "quote": self._extract_context_around_keyword(chapter_content, name)
                    }]
                })
        
        # æ¨¡å¼2ï¼šæ•°æ®é›†åç§°
        dataset_patterns = [
            r'\b(?:GLUE|SuperGLUE|SQuAD|MNLI|CoLA|SST|QNLI|RTE|WNLI|MultiNLI|ImageNet|COCO|WikiText|BookCorpus|Common Crawl)\b',
            r'\b[A-Z][a-zA-Z]*(?:Dataset|Corpus|Benchmark|DB)\b'
        ]
        
        for pattern in dataset_patterns:
            matches = re.finditer(pattern, chapter_content, re.IGNORECASE)
            for match in matches:
                name = match.group()
                entities.append({
                    "name": name,
                    "type": "Dataset",
                    "source": "pattern_match",
                    "chapter": chapter_num,
                    "confidence": 0.7,
                    "evidence": [{
                        "source": "survey",
                        "loc": f"ch{chapter_num}",
                        "quote": self._extract_context_around_keyword(chapter_content, name)
                    }]
                })
        
        return entities
    
    async def _extract_entities_with_llm(self, chapter_content: str, chapter_num: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨ LLM æŠ½å–å®ä½“ã€‚"""
        try:
            # æ„é€ å®ä½“æŠ½å–æç¤ºè¯
            prompt = f"""
ä»ä»¥ä¸‹å­¦æœ¯æ–‡æœ¬ä¸­è¯†åˆ«å’ŒæŠ½å–å…³é”®å®ä½“ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹åˆ†ç±»æ ‡å‡†ï¼š

**å®ä½“ç±»å‹**ï¼š
- Method: æ–¹æ³•ã€æ¨¡å‹ã€ç®—æ³•ã€æŠ€æœ¯ï¼ˆå¦‚GPTã€Transformerã€BERTç­‰ï¼‰
- Task: ä»»åŠ¡ã€åº”ç”¨ã€é—®é¢˜ï¼ˆå¦‚æ–‡æœ¬ç”Ÿæˆã€æœºå™¨ç¿»è¯‘ã€æƒ…æ„Ÿåˆ†æç­‰ï¼‰
- Dataset: æ•°æ®é›†ã€è¯­æ–™åº“ï¼ˆå¦‚GLUEã€SQuADã€ImageNetç­‰ï¼‰
- Metric: è¯„ä»·æŒ‡æ ‡ï¼ˆå¦‚BLEUã€ROUGEã€å‡†ç¡®ç‡ã€å›°æƒ‘åº¦ç­‰ï¼‰
- Problem: æŒ‘æˆ˜ã€é—®é¢˜ã€é™åˆ¶ï¼ˆå¦‚åè§ã€å¹»è§‰ã€æ³›åŒ–ç­‰ï¼‰

**æ–‡æœ¬å†…å®¹**ï¼š
{chapter_content[:30000]}  

è¯·ä»¥JSONæ ¼å¼è¿”å›æŠ½å–çš„å®ä½“ï¼Œæ¯ä¸ªå®ä½“åŒ…å«ï¼š
- name: å®ä½“åç§°
- type: å®ä½“ç±»å‹ï¼ˆMethod/Task/Dataset/Metric/Problemï¼‰
- context: å®ä½“åœ¨æ–‡æœ¬ä¸­çš„ä¸Šä¸‹æ–‡ï¼ˆç®€çŸ­ï¼‰
- confidence: ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰

ç¤ºä¾‹æ ¼å¼ï¼š
{{
  "entities": [
    {{
      "name": "BERT",
      "type": "Method",
      "context": "BERTæ¨¡å‹åœ¨å¤šä¸ªNLPä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²",
      "confidence": 0.9
    }}
  ]
}}
"""

            # è°ƒç”¨LLM
            response_data = await self.llm.generate(
                model_name=self.config.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=30000,
                agent_name=self.name,
                task_type="entity_extraction"
            )
            response = response_data.get("content", "")
            
            # è§£æLLMå“åº”
            import json
            import re
            try:
                # æå–JSONä»£ç å—
                if "```json" in response:
                    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–JSONå†…å®¹
                    json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
                    if json_match:
                        json_content = json_match.group(1).strip()
                    else:
                        # å¦‚æœæ²¡æ‰¾åˆ°å®Œæ•´çš„ä»£ç å—ï¼Œå°è¯•ä»```jsonå¼€å§‹åˆ°æœ€å
                        json_start = response.find("```json") + 7
                        json_content = response[json_start:].strip()
                        if json_content.endswith("```"):
                            json_content = json_content[:-3].strip()
                else:
                    json_content = response.strip()
                
                result = json.loads(json_content)
                llm_entities = []
                
                for entity_data in result.get("entities", []):
                    entity = {
                        "name": entity_data.get("name", "").strip(),
                        "type": entity_data.get("type", "Method"),
                        "confidence": entity_data.get("confidence", 0.5),
                        "evidence": [{
                            "source": "llm_extraction",
                            "loc": f"ch{chapter_num}",
                            "quote": entity_data.get("context", ""),
                            "confidence": entity_data.get("confidence", 0.5)
                        }]
                    }
                    
                    if entity["name"] and len(entity["name"]) > 2:
                        llm_entities.append(entity)
                
                print(f"      ğŸ¤– LLMæŠ½å–åˆ° {len(llm_entities)} ä¸ªå®ä½“")
                return llm_entities
                
            except json.JSONDecodeError:
                print(f"      âš ï¸ LLMå“åº”è§£æå¤±è´¥ï¼Œè·³è¿‡ç« èŠ‚ {chapter_num}")
                return []
                
        except Exception as e:
            print(f"      âŒ LLMå®ä½“æŠ½å–å¤±è´¥: {str(e)}")
            return []
    
    async def _normalize_and_deduplicate_entities(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å®ä½“è§„èŒƒåŒ–ä¸å»é‡ã€‚"""
        # æŒ‰åç§°åˆ†ç»„
        entity_groups = {}
        for entity in entities:
            name = entity["name"].strip()
            normalized_name = self._normalize_entity_name(name)
            
            if normalized_name not in entity_groups:
                entity_groups[normalized_name] = []
            entity_groups[normalized_name].append(entity)
        
        # åˆå¹¶åŒç±»å®ä½“
        normalized_entities = []
        entity_id_counter = 1
        
        for normalized_name, group in entity_groups.items():
            if not group:
                continue
            
            # é€‰æ‹©æœ€ä½³ä»£è¡¨å®ä½“
            representative = max(group, key=lambda x: x.get("confidence", 0))
            
            # æ”¶é›†æ‰€æœ‰åˆ«å
            aliases = set()
            all_evidence = []
            
            for entity in group:
                aliases.add(entity["name"])
                all_evidence.extend(entity.get("evidence", []))
            
            aliases.discard(normalized_name)  # ç§»é™¤ä¸»åç§°
            
            # ç¡®å®šå®ä½“ç±»å‹ï¼ˆå–æœ€å¸¸è§çš„ç±»å‹ï¼‰
            type_counts = {}
            for entity in group:
                entity_type = entity["type"]
                type_counts[entity_type] = type_counts.get(entity_type, 0) + 1
            
            most_common_type = max(type_counts.items(), key=lambda x: x[1])[0]
            
            # æ„é€ è§„èŒƒåŒ–å®ä½“
            type_prefix = most_common_type[0]  # Method -> M, Task -> T, etc.
            entity_id = f"{type_prefix}:{normalized_name.replace(' ', '_')}"
            
            normalized_entity = {
                "id": entity_id,
                "name": normalized_name,
                "type": most_common_type,
                "aliases": list(aliases),
                "evidence": all_evidence,
                "salience": len(group) * 0.1,  # ç®€å•çš„é‡è¦æ€§è®¡ç®—
                "source_count": len(group)
            }
            
            normalized_entities.append(normalized_entity)
            entity_id_counter += 1
        
        return normalized_entities
    
    def _normalize_entity_name(self, name: str) -> str:
        """è§„èŒƒåŒ–å®ä½“åç§°ã€‚"""
        # åŸºæœ¬æ¸…ç†
        name = name.strip()
        
        # å¤„ç†å¸¸è§çš„ç¼©å†™å’Œå˜ä½“
        normalizations = {
            "GPT-3": "GPT-3",
            "GPT-4": "GPT-4", 
            "BERT": "BERT",
            "Transformer": "Transformer",
            "Large Language Model": "Large Language Model",
            "LLM": "Large Language Model",
            "Natural Language Processing": "Natural Language Processing",
            "NLP": "Natural Language Processing",
        }
        
        return normalizations.get(name, name)

    # ====== å…³ç³»æŠ½å–ç›¸å…³æ–¹æ³• ======
    
    async def _extract_cooccurrence_relations(self, graph: SemanticOpportunityGraph, full_document: str) -> List[Dict[str, Any]]:
        """åŸºäºå…±ç°æ¨¡å¼æŠ½å–å…³ç³»ã€‚"""
        relations = []
        nodes = list(graph.nodes())
        
        # ç®€åŒ–å®ç°ï¼šæ£€æŸ¥èŠ‚ç‚¹å¯¹åœ¨æ–‡æ¡£ä¸­çš„å…±ç°
        for i, node1 in enumerate(nodes):
            for node2 in nodes[i+1:]:
                node1_name = graph.nodes[node1]['name']
                node2_name = graph.nodes[node2]['name']
                
                # æ£€æŸ¥ä¸¤ä¸ªå®ä½“æ˜¯å¦åœ¨åŒä¸€æ®µè½ä¸­å…±ç°
                if self._check_cooccurrence(full_document, node1_name, node2_name):
                    relation_type = self._infer_relation_type(
                        graph.nodes[node1]['type'], 
                        graph.nodes[node2]['type']
                    )
                    
                    if relation_type:
                        relations.append({
                            "src": node1,
                            "dst": node2,
                            "relation": relation_type,
                            "weight": 0.5,
                            "confidence": 0.6,
                            "evidence": [{
                                "source": "survey",
                                "loc": "cooccurrence",
                                "quote": f"{node1_name} and {node2_name} co-occur"
                            }]
                        })
        
        return relations
    
    def _check_cooccurrence(self, document: str, entity1: str, entity2: str, window_size: int = 200) -> bool:
        """æ£€æŸ¥ä¸¤ä¸ªå®ä½“æ˜¯å¦åœ¨æŒ‡å®šçª—å£å†…å…±ç°ã€‚"""
        import re
        
        # å¿½ç•¥å¤§å°å†™æŸ¥æ‰¾å®ä½“ä½ç½®
        pattern1 = re.compile(re.escape(entity1), re.IGNORECASE)
        pattern2 = re.compile(re.escape(entity2), re.IGNORECASE)
        
        matches1 = list(pattern1.finditer(document))
        matches2 = list(pattern2.finditer(document))
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åŒ¹é…åœ¨çª—å£å†…
        for match1 in matches1:
            for match2 in matches2:
                if abs(match1.start() - match2.start()) <= window_size:
                    return True
        
        return False
    
    def _infer_relation_type(self, type1: str, type2: str) -> Optional[str]:
        """æ ¹æ®å®ä½“ç±»å‹æ¨æ–­å¯èƒ½çš„å…³ç³»ç±»å‹ã€‚"""
        # æ–¹æ³• -> ä»»åŠ¡
        if type1 == "Method" and type2 == "Task":
            return "solves"
        
        # ä»»åŠ¡ -> æ–¹æ³• (åå‘)
        if type1 == "Task" and type2 == "Method":
            return "solves"
        
        # æ–¹æ³• -> æ•°æ®é›†
        if type1 == "Method" and type2 == "Dataset":
            return "uses_dataset"
        
        # ä»»åŠ¡ -> æŒ‡æ ‡
        if type1 == "Task" and type2 == "Metric":
            return "evaluated_by"
        
        # æ–¹æ³• -> æ–¹æ³•
        if type1 == "Method" and type2 == "Method":
            return "similar_to"
        
        return None
    
    async def _extract_pattern_relations(self, graph: SemanticOpportunityGraph, full_document: str) -> List[Dict[str, Any]]:
        """åŸºäºLLMçš„å…³ç³»æŠ½å–ã€‚"""
        relations = []
        
        try:
            # è·å–å›¾è°±ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹
            nodes = list(graph.nodes())
            if len(nodes) < 2:
                return relations
            
            # é™åˆ¶å¤„ç†çš„èŠ‚ç‚¹æ•°é‡ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
            node_names = [graph.nodes[node_id].get('name', node_id) for node_id in nodes[:50]]
            
            # æ„é€ å…³ç³»æŠ½å–æç¤ºè¯
            prompt = f"""
ä»ä»¥ä¸‹å­¦æœ¯æ–‡æœ¬ä¸­è¯†åˆ«å®ä½“é—´çš„å…³ç³»ã€‚å·²çŸ¥å®ä½“åˆ—è¡¨ï¼š
{', '.join(node_names[:30])}

**å…³ç³»ç±»å‹**ï¼š
- improves_on: Aæ”¹è¿›äº†Bã€Aä¼˜äºB
- uses_dataset: Aä½¿ç”¨äº†æ•°æ®é›†Bè¿›è¡Œè®­ç»ƒ/è¯„ä¼°  
- evaluated_by: Aé€šè¿‡æŒ‡æ ‡Bè¿›è¡Œè¯„ä¼°
- applies_to: Aåº”ç”¨äºä»»åŠ¡B
- requires: Aéœ€è¦/ä¾èµ–B
- enables: Aä½¿å¾—Bæˆä¸ºå¯èƒ½
- addresses: Aè§£å†³äº†é—®é¢˜B

**æ–‡æœ¬ç‰‡æ®µ**ï¼ˆå‰5000å­—ç¬¦ï¼‰ï¼š
{full_document[:30000]}

è¯·ä»¥JSONæ ¼å¼è¿”å›è¯†åˆ«çš„å…³ç³»ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "relations": [
    {{
      "src": "å®ä½“Aåç§°", 
      "dst": "å®ä½“Båç§°",
      "relation": "å…³ç³»ç±»å‹",
      "evidence": "æ”¯æ’‘è¯¥å…³ç³»çš„æ–‡æœ¬è¯æ®",
      "confidence": 0.8
    }}
  ]
}}

åªè¿”å›ç½®ä¿¡åº¦>0.6ä¸”åœ¨å·²çŸ¥å®ä½“åˆ—è¡¨ä¸­çš„å…³ç³»ã€‚
"""

            # è°ƒç”¨LLM
            response_data = await self.llm.generate(
                model_name=self.config.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=15000,
                agent_name=self.name,
                task_type="relation_extraction"
            )
            response = response_data.get("content", "")
            
            # è§£æLLMå“åº”
            import json
            import re
            try:
                # æå–JSONä»£ç å—
                if "```json" in response:
                    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–JSONå†…å®¹
                    json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
                    if json_match:
                        json_content = json_match.group(1).strip()
                    else:
                        # å¦‚æœæ²¡æ‰¾åˆ°å®Œæ•´çš„ä»£ç å—ï¼Œå°è¯•ä»```jsonå¼€å§‹åˆ°æœ€å
                        json_start = response.find("```json") + 7
                        json_content = response[json_start:].strip()
                        if json_content.endswith("```"):
                            json_content = json_content[:-3].strip()
                else:
                    json_content = response.strip()
                
                result = json.loads(json_content)
                
                # åˆ›å»ºèŠ‚ç‚¹åç§°åˆ°IDçš„æ˜ å°„
                name_to_id = {}
                for node_id in nodes:
                    node_data = graph.nodes[node_id]
                    node_name = node_data.get('name', node_id)
                    name_to_id[node_name.lower()] = node_id
                    
                    # ä¹Ÿæ˜ å°„åˆ«å
                    aliases = node_data.get('aliases', [])
                    for alias in aliases:
                        name_to_id[alias.lower()] = node_id
                
                for rel_data in result.get("relations", []):
                    src_name = rel_data.get("src", "").strip().lower()
                    dst_name = rel_data.get("dst", "").strip().lower()
                    relation_type = rel_data.get("relation", "")
                    confidence = rel_data.get("confidence", 0.0)
                    evidence = rel_data.get("evidence", "")
                    
                    # æ£€æŸ¥å®ä½“æ˜¯å¦åœ¨å›¾è°±ä¸­
                    src_id = name_to_id.get(src_name)
                    dst_id = name_to_id.get(dst_name)
                    
                    if src_id and dst_id and src_id != dst_id and confidence > 0.6:
                        relation = {
                            "src": src_id,
                            "dst": dst_id,
                            "relation": relation_type,
                            "confidence": confidence,
                            "evidence": [{
                                "source": "llm_extraction",
                                "quote": evidence,
                                "confidence": confidence
                            }]
                        }
                        relations.append(relation)
                
                print(f"      ğŸ¤– LLMæŠ½å–åˆ° {len(relations)} ä¸ªå…³ç³»")
                return relations
                
            except json.JSONDecodeError:
                print(f"      âš ï¸ LLMå…³ç³»æŠ½å–å“åº”è§£æå¤±è´¥")
                return []
                
        except Exception as e:
            print(f"      âŒ LLMå…³ç³»æŠ½å–å¤±è´¥: {str(e)}")
            return []
    
    async def _extract_pattern_relations_parallel(self, graph: SemanticOpportunityGraph, final_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åŸºäºLLMçš„å¹¶è¡Œå…³ç³»æŠ½å–ã€‚æŒ‰ç« èŠ‚åˆ†åˆ«å¤„ç†ï¼Œæé«˜è¦†ç›–ç‡å’Œå¹¶è¡Œåº¦ã€‚"""
        import asyncio
        
        # è·å–æ–‡æ¡£ç« èŠ‚
        full_document = final_result.get("full_document", "")
        chapters = self._split_document_by_chapters(full_document)
        
        if not chapters:
            return []
        
        # åˆ›å»ºæ‰€æœ‰ç« èŠ‚çš„å¹¶è¡Œä»»åŠ¡
        tasks = []
        for chapter_num, chapter_content in chapters.items():
            task = self._extract_relations_from_chapter(graph, chapter_num, chapter_content)
            tasks.append(task)
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰LLMè°ƒç”¨
        all_relations = []
        if tasks:
            print(f"    ğŸš€ å¯åŠ¨ {len(tasks)} ä¸ªç« èŠ‚çš„å¹¶è¡Œå…³ç³»æŠ½å–")
            chapter_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            for i, result in enumerate(chapter_results):
                if isinstance(result, Exception):
                    print(f"      âŒ ç« èŠ‚ {list(chapters.keys())[i]} å…³ç³»æŠ½å–å¤±è´¥: {str(result)}")
                else:
                    all_relations.extend(result)
        
        return all_relations
    
    async def _extract_relations_from_chapter(self, graph: SemanticOpportunityGraph, chapter_num: str, chapter_content: str) -> List[Dict[str, Any]]:
        """ä»å•ä¸ªç« èŠ‚æŠ½å–å…³ç³»ã€‚"""
        relations = []
        
        try:
            # è·å–å›¾è°±ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹
            nodes = list(graph.nodes())
            if len(nodes) < 2:
                return relations
            
            # å¤„ç†æ›´å¤šèŠ‚ç‚¹ï¼Œä¸å†é™åˆ¶ä¸º50ä¸ª
            node_names = [graph.nodes[node_id].get('name', node_id) for node_id in nodes[:100]]
            
            # æ„é€ å…³ç³»æŠ½å–æç¤ºè¯
            prompt = f"""
ä»ä»¥ä¸‹å­¦æœ¯æ–‡æœ¬ç« èŠ‚ä¸­è¯†åˆ«å®ä½“é—´çš„å…³ç³»ã€‚å·²çŸ¥å®ä½“åˆ—è¡¨ï¼š
{', '.join(node_names[:100])}

**å…³ç³»ç±»å‹**ï¼š
- improves_on: Aæ”¹è¿›äº†Bã€Aä¼˜äºB
- uses_dataset: Aä½¿ç”¨äº†æ•°æ®é›†Bè¿›è¡Œè®­ç»ƒ/è¯„ä¼°  
- evaluated_by: Aé€šè¿‡æŒ‡æ ‡Bè¿›è¡Œè¯„ä¼°
- applies_to: Aåº”ç”¨äºä»»åŠ¡B
- requires: Aéœ€è¦/ä¾èµ–B
- enables: Aä½¿å¾—Bæˆä¸ºå¯èƒ½
- addresses: Aè§£å†³äº†é—®é¢˜B

**ç« èŠ‚ {chapter_num} å†…å®¹**ï¼š
{chapter_content[:20000]}

è¯·ä»¥JSONæ ¼å¼è¿”å›è¯†åˆ«çš„å…³ç³»ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "relations": [
    {{
      "src": "å®ä½“Aåç§°", 
      "dst": "å®ä½“Båç§°",
      "relation": "å…³ç³»ç±»å‹",
      "evidence": "æ”¯æ’‘è¯¥å…³ç³»çš„æ–‡æœ¬è¯æ®",
      "confidence": 0.8
    }}
  ]
}}

åªè¿”å›ç½®ä¿¡åº¦>0.6ä¸”åœ¨å·²çŸ¥å®ä½“åˆ—è¡¨ä¸­çš„å…³ç³»ã€‚
"""

            # è°ƒç”¨LLM
            response_data = await self.llm.generate(
                model_name=self.config.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=15000,
                agent_name=self.name,
                task_type="relation_extraction"
            )
            response = response_data.get("content", "")
            
            # è§£æLLMå“åº”
            import json
            import re
            try:
                # æå–JSONä»£ç å—
                if "```json" in response:
                    json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
                    if json_match:
                        json_content = json_match.group(1).strip()
                    else:
                        json_start = response.find("```json") + 7
                        json_content = response[json_start:].strip()
                        if json_content.endswith("```"):
                            json_content = json_content[:-3].strip()
                else:
                    json_content = response.strip()
                
                result = json.loads(json_content)
                
                # åˆ›å»ºèŠ‚ç‚¹åç§°åˆ°IDçš„æ˜ å°„
                name_to_id = {}
                for node_id in nodes:
                    node_data = graph.nodes[node_id]
                    node_name = node_data.get('name', node_id)
                    name_to_id[node_name.lower()] = node_id
                    
                    # ä¹Ÿæ˜ å°„åˆ«å
                    aliases = node_data.get('aliases', [])
                    for alias in aliases:
                        name_to_id[alias.lower()] = node_id
                
                for rel_data in result.get("relations", []):
                    src_name = rel_data.get("src", "").strip().lower()
                    dst_name = rel_data.get("dst", "").strip().lower()
                    relation_type = rel_data.get("relation", "")
                    confidence = rel_data.get("confidence", 0.0)
                    evidence = rel_data.get("evidence", "")
                    
                    # æ£€æŸ¥å®ä½“æ˜¯å¦åœ¨å›¾è°±ä¸­
                    src_id = name_to_id.get(src_name)
                    dst_id = name_to_id.get(dst_name)
                    
                    if src_id and dst_id and src_id != dst_id and confidence > 0.6:
                        relation = {
                            "src": src_id,
                            "dst": dst_id,
                            "relation": relation_type,
                            "confidence": confidence,
                            "evidence": [{
                                "source": "llm_extraction",
                                "quote": evidence,
                                "confidence": confidence
                            }]
                        }
                        relations.append(relation)
                
                return relations
                
            except json.JSONDecodeError:
                print(f"      âš ï¸ ç« èŠ‚ {chapter_num} LLMå…³ç³»æŠ½å–å“åº”è§£æå¤±è´¥")
                return []
                
        except Exception as e:
            print(f"      âŒ ç« èŠ‚ {chapter_num} LLMå…³ç³»æŠ½å–å¤±è´¥: {str(e)}")
            return []
    
    async def _verify_relations_with_database(self, relations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨æ•°æ®åº“éªŒè¯å…³ç³»ã€‚"""
        verified_relations = []
        
        for relation in relations:
            # ç®€åŒ–å®ç°ï¼šç›®å‰åªè¿›è¡ŒåŸºæœ¬è¿‡æ»¤
            if relation.get("confidence", 0) > 0.3:
                verified_relations.append(relation)
        
        return verified_relations
    
    # ====== æœºä¼šæ£€æµ‹ç›¸å…³æ–¹æ³• ======
    
    def _detect_transfer_opportunities(self, graph: SemanticOpportunityGraph, methods: List[str], tasks: List[str]) -> List[Dict[str, Any]]:
        """æ£€æµ‹è¿ç§»æœºä¼šã€‚"""
        gaps = []
        
        # ç®€åŒ–å®ç°ï¼šå¯»æ‰¾æ–¹æ³•ä¸ä»»åŠ¡ä¹‹é—´ç¼ºå¤±çš„è¿æ¥
        for method in methods:
            connected_tasks = []
            for task in tasks:
                if graph.has_edge(method, task) or graph.has_edge(task, method):
                    connected_tasks.append(task)
            
            # å¦‚æœæ–¹æ³•è¿æ¥çš„ä»»åŠ¡å°‘äºæ€»ä»»åŠ¡çš„ä¸€åŠï¼Œå¯èƒ½å­˜åœ¨è¿ç§»æœºä¼š
            if len(connected_tasks) < len(tasks) / 2 and len(connected_tasks) > 0:
                unconnected_tasks = [t for t in tasks if t not in connected_tasks]
                for unconnected_task in unconnected_tasks[:2]:  # é™åˆ¶æ•°é‡
                    gaps.append({
                        "pattern": "transfer",
                        "from_method": method,
                        "to_task": unconnected_task,
                        "explanation": f"æ–¹æ³•{graph.nodes[method]['name']}å¯èƒ½é€‚ç”¨äºä»»åŠ¡{graph.nodes[unconnected_task]['name']}",
                        "confidence": 0.7,
                        "related_nodes": [method, unconnected_task]
                    })
        
        return gaps
    
    def _detect_composition_opportunities(self, graph: SemanticOpportunityGraph, methods: List[str], tasks: List[str]) -> List[Dict[str, Any]]:
        """æ£€æµ‹ç»„åˆæœºä¼šã€‚"""
        gaps = []
        
        # å¯»æ‰¾è§£å†³åŒä¸€ä»»åŠ¡çš„å¤šä¸ªæ–¹æ³•ï¼Œå¯èƒ½å¯ä»¥ç»„åˆ
        for task in tasks:
            connected_methods = []
            for method in methods:
                if graph.has_edge(method, task) or graph.has_edge(task, method):
                    connected_methods.append(method)
            
            # å¦‚æœæœ‰å¤šä¸ªæ–¹æ³•è§£å†³åŒä¸€ä»»åŠ¡ï¼Œè€ƒè™‘ç»„åˆæœºä¼š
            if len(connected_methods) >= 2:
                for i, method1 in enumerate(connected_methods):
                    for method2 in connected_methods[i+1:]:
                        gaps.append({
                            "pattern": "composition",
                            "method1": method1,
                            "method2": method2,
                            "target_task": task,
                            "explanation": f"ç»„åˆ{graph.nodes[method1]['name']}å’Œ{graph.nodes[method2]['name']}å¯èƒ½æ”¹è¿›{graph.nodes[task]['name']}",
                            "confidence": 0.6,
                            "related_nodes": [method1, method2, task]
                        })
                        break  # é™åˆ¶æ¯ä¸ªä»»åŠ¡åªç”Ÿæˆä¸€ä¸ªç»„åˆæœºä¼š
        
        return gaps
    
    def _detect_reverse_opportunities(self, graph: SemanticOpportunityGraph) -> List[Dict[str, Any]]:
        """æ£€æµ‹åè½¬æœºä¼šï¼ˆåŸºäºcritiqueå…³ç³»ï¼‰ã€‚"""
        gaps = []
        
        # å¯»æ‰¾critiqueè¾¹ï¼Œç”Ÿæˆæ”¹è¿›æœºä¼š
        for edge in graph.edges(data=True):
            src, dst, edge_data = edge
            if edge_data.get('relation') == 'critiques':
                gaps.append({
                    "pattern": "reverse",
                    "critiqued_method": dst,
                    "critique_source": src,
                    "explanation": f"é’ˆå¯¹{graph.nodes[dst]['name']}çš„æ‰¹è¯„ï¼Œå¯»æ‰¾æ”¹è¿›æ–¹æ¡ˆ",
                    "confidence": 0.8,
                    "related_nodes": [src, dst]
                })
        
        return gaps
    
    def _detect_evaluation_gaps(self, graph: SemanticOpportunityGraph, tasks: List[str], metrics: List[str]) -> List[Dict[str, Any]]:
        """æ£€æµ‹è¯„æµ‹ç©ºç¼ºã€‚"""
        gaps = []
        
        # å¯»æ‰¾ç¼ºå°‘è¯„ä»·æŒ‡æ ‡çš„ä»»åŠ¡
        for task in tasks:
            connected_metrics = []
            for metric in metrics:
                if graph.has_edge(task, metric) or graph.has_edge(metric, task):
                    connected_metrics.append(metric)
            
            # å¦‚æœä»»åŠ¡æ²¡æœ‰æˆ–å¾ˆå°‘è¯„ä»·æŒ‡æ ‡
            if len(connected_metrics) == 0:
                gaps.append({
                    "pattern": "evaluation",
                    "task": task,
                    "explanation": f"ä»»åŠ¡{graph.nodes[task]['name']}ç¼ºå°‘åˆé€‚çš„è¯„ä»·æŒ‡æ ‡",
                    "confidence": 0.7,
                    "related_nodes": [task]
                })
        
        return gaps
    
    def _detect_data_enhancement_opportunities(self, graph: SemanticOpportunityGraph, tasks: List[str], datasets: List[str]) -> List[Dict[str, Any]]:
        """æ£€æµ‹æ•°æ®å¢å¼ºæœºä¼šã€‚"""
        gaps = []
        
        # å¯»æ‰¾æ•°æ®é›†ä¸è¶³çš„ä»»åŠ¡
        for task in tasks:
            connected_datasets = []
            for dataset in datasets:
                if graph.has_edge(task, dataset) or graph.has_edge(dataset, task):
                    connected_datasets.append(dataset)
            
            # å¦‚æœä»»åŠ¡ç¼ºå°‘æ•°æ®é›†
            if len(connected_datasets) <= 1:
                gaps.append({
                    "pattern": "data_enhancement",
                    "task": task,
                    "explanation": f"ä»»åŠ¡{graph.nodes[task]['name']}å¯èƒ½éœ€è¦æ›´å¤šæ•°æ®é›†æ”¯æŒ",
                    "confidence": 0.6,
                    "related_nodes": [task]
                })
        
        return gaps


class IdeaGeneratorAgent(BaseIdeaAgent):
    """ç¬¬äºŒé˜¶æ®µï¼šæƒ³æ³•èŒç”Ÿä¸ç­–ç•¥æ€§ç”Ÿæˆã€‚

    æ ¸å¿ƒèŒè´£:
        - åœ¨ `SemanticOpportunityGraph` ä¸Šæ‰«æè§¦å‘æ¨¡å¼ï¼ˆè¿ç§»/ç»„åˆ/åè½¬/è¯„æµ‹é‡æ„/æ•°æ®å¢å¼ºç­‰ï¼‰ã€‚
        - åº”ç”¨å¯é…ç½®çš„ç­–ç•¥æ¨¡æ¿ä¸ Chain-of-Ideas æ€ç»´é“¾ï¼Œç”Ÿæˆç»“æ„åŒ– `CandidateIdea` åˆ—è¡¨ã€‚

    æ³¨æ„äº‹é¡¹:
        - æ”¯æŒæ‰¹é‡ç”Ÿæˆï¼šæ¯æ‰¹10ä¸ªï¼Œæœ€å¤š5æ‰¹æ¬¡ï¼Œæ€»å…±50ä¸ªidea
        - æ¯ä¸ªæƒ³æ³•éœ€ç»‘å®šè§¦å‘èŠ‚ç‚¹æ¥æº `source_trigger_nodes` ä¸ `provenance`ã€‚
    """

    def __init__(self, name: str, llm_factory: LLMFactory, db: AcademicPaperDatabase, config: Optional[AgentConfig] = None):
        super().__init__(name, llm_factory, db, config)
        self.strategy_templates = self._load_strategy_templates()

    def _load_strategy_templates(self) -> Dict[str, Dict[str, Any]]:
        """åŠ è½½ç­–ç•¥æ¨¡æ¿åº“ã€‚
        
        è¿”å›:
            Dict[str, Dict]: ç­–ç•¥ID -> ç­–ç•¥é…ç½®çš„æ˜ å°„ã€‚
            
        å®ç°æ€è·¯:
            1) é¢„å®šä¹‰å¸¸è§ç­–ç•¥æ¨¡æ¿ï¼štransfer_across_tasksã€compose_methodsã€reverse_critiqueç­‰ã€‚
            2) æ¯ä¸ªç­–ç•¥åŒ…å«ï¼šè§¦å‘æ¡ä»¶ã€æç¤ºè¯æ§½ä½ã€æ£€æŸ¥é¡¹ã€ç¤ºä¾‹ç­‰ã€‚
        """
        return {
            "transfer_across_tasks": {
                "trigger_pattern": "Method->Task_A strong; Task_B similar_to Task_A; no edge Method->Task_B",
                "prompt_slots": ["Method", "Task_A", "Task_B"],
                "checks": ["æ•°æ®è§„æ¨¡å¯¹é½", "è¯„ä»·æŒ‡æ ‡å¯æ¯”", "å½’çº³åç½®åˆç†æ€§"],
                "example": "å°†Transformerä»æœºå™¨ç¿»è¯‘è¿ç§»åˆ°ä»£ç ç”Ÿæˆ"
            },
            "compose_methods": {
                "trigger_pattern": "Method_X solves SubZ1; Method_Y solves SubZ2; both under Task_Z",
                "prompt_slots": ["Method_X", "Method_Y", "Task_Z"],
                "checks": ["æ¥å£/è¡¨å¾å¯å…¼å®¹", "è®­ç»ƒ/æ¨ç†å¤æ‚åº¦"],
                "example": "ç»“åˆæ£€ç´¢ä¸ç”Ÿæˆæ”¹è¿›é—®ç­”ç³»ç»Ÿ"
            },
            "reverse_critique": {
                "trigger_pattern": "Paper critiques Method w/ facet=inefficiency",
                "prompt_slots": ["Method", "Critique_Facet"],
                "checks": ["ç­‰ä»·æ›¿æ¢æˆ–è¿‘ä¼¼æ¨æ–­", "å‹ç¼©/è’¸é¦/å‰ªæ"],
                "example": "é’ˆå¯¹Transformerè®¡ç®—å¤æ‚åº¦æ‰¹è¯„æå‡ºé«˜æ•ˆå˜ç§"
            }
        }

    async def generate_candidates(self, graph: SemanticOpportunityGraph, 
                                 max_ideas: int = 50, 
                                 ideas_per_generation: int = 10, 
                                 num_generations: int = 5) -> Dict[str, Any]:
        """ç­–ç•¥æ€§ç”Ÿæˆå€™é€‰æƒ³æ³•ï¼šä»å¤§é‡æœºä¼šä¸­æ™ºèƒ½ç­›é€‰ã€‚

        è¾“å…¥:
            - graph: å·²æ„å»ºçš„ `SemanticOpportunityGraph`ã€‚
            - max_ideas: ç”Ÿæˆæ•°é‡ä¸Šé™ï¼ˆé»˜è®¤50ï¼‰ã€‚
            - ideas_per_generation: æ¯æ¬¡ç”Ÿæˆçš„æƒ³æ³•æ•°é‡ï¼ˆé»˜è®¤10ï¼‰ã€‚
            - num_generations: ç”Ÿæˆæ¬¡æ•°ï¼ˆé»˜è®¤5æ¬¡ï¼‰ã€‚

        è¾“å‡º:
            - Dict: {
                'all_candidates': List[CandidateIdea],  # æ‰€æœ‰å€™é€‰æƒ³æ³•
                'batches': List[List[CandidateIdea]],   # æŒ‰ç”Ÿæˆæ‰¹æ¬¡ç»„ç»‡çš„æƒ³æ³•
                'generation_info': Dict                  # ç”Ÿæˆè¿‡ç¨‹ä¿¡æ¯
              }

        å®ç°æ­¥éª¤:
            1) è¯†åˆ«å¤§é‡æœºä¼šè§¦å‘ç‚¹ï¼ˆç›®æ ‡50+ä¸ªæœºä¼šç‚¹ï¼‰ã€‚
            2) ç­–ç•¥æ€§ç”Ÿæˆï¼š5æ¬¡LLMè°ƒç”¨ï¼Œæ¯æ¬¡ä»50ä¸ªæœºä¼šä¸­ç­›é€‰10ä¸ªæœ€ä½³æƒ³æ³•ã€‚
            3) æ¯æ¬¡LLMè°ƒç”¨éƒ½åŒ…å«å®Œæ•´çš„æœºä¼šåˆ†æå’Œæ™ºèƒ½ç­›é€‰è¿‡ç¨‹ã€‚
            4) ä¿æŒæ‰¹æ¬¡ç»“æ„ï¼Œä¾›åç»­æ‰¹é‡è¯„å®¡ä½¿ç”¨ã€‚
        """
        print(f"ğŸ§  å¼€å§‹ç­–ç•¥æ€§æƒ³æ³•ç”Ÿæˆï¼šç›®æ ‡{max_ideas}ä¸ªæƒ³æ³•ï¼Œ{num_generations}æ¬¡ç”Ÿæˆï¼Œæ¯æ¬¡æŒ‘é€‰{ideas_per_generation}ä¸ª")
        
        # æ­¥éª¤1ï¼šæ”¶é›†å¤§é‡æœºä¼šè§¦å‘ç‚¹
        all_opportunities = await self._collect_comprehensive_opportunities(graph)
        print(f"ğŸ¯ è¯†åˆ«å‡º {len(all_opportunities)} ä¸ªæœºä¼šè§¦å‘ç‚¹")
        
        if len(all_opportunities) < 30:
            print(f"âš ï¸ æœºä¼šç‚¹æ•°é‡åå°‘({len(all_opportunities)})ï¼Œå¯èƒ½å½±å“ç­–ç•¥æ€§é€‰æ‹©çš„è´¨é‡")
        
        # æ­¥éª¤2ï¼šå°†æœºä¼šåˆ†æ‰¹åˆ†é…ç»™ä¸åŒè½®æ¬¡çš„ç”Ÿæˆ
        opportunity_batches = self._allocate_opportunities_to_batches(all_opportunities, num_generations)
        print(f"ğŸ¯ æœºä¼šåˆ†é…ï¼š{len(all_opportunities)}ä¸ªæœºä¼šåˆ†ä¸º{len(opportunity_batches)}æ‰¹ï¼Œæ¯æ‰¹å¹³å‡{len(all_opportunities)//num_generations}ä¸ª")
        
        all_candidates = []
        idea_batches = []  # ä¿æŒæ‰¹æ¬¡ç»“æ„
        
        # æ­¥éª¤3ï¼šå¹¶å‘ç­–ç•¥æ€§ç”Ÿæˆ - 5ä¸ªAgentåŒæ—¶ä»ä¸åŒæ‰¹æ¬¡çš„æœºä¼šä¸­æ™ºèƒ½æŒ‘é€‰
        print(f"ğŸš€ å¯åŠ¨{num_generations}ä¸ªå¹¶å‘IdeaGeneratorï¼Œæ¯ä¸ªå¤„ç†ä¸åŒçš„æœºä¼šæ‰¹æ¬¡")
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        import asyncio
        tasks = []
        valid_batches = []
        
        for generation_round in range(num_generations):
            current_opportunities = opportunity_batches[generation_round] if generation_round < len(opportunity_batches) else []
            
            if not current_opportunities:
                print(f"âš ï¸ ç¬¬{generation_round + 1}è½®æ²¡æœ‰å¯ç”¨æœºä¼šï¼Œè·³è¿‡")
                continue
            
            print(f"ğŸ§  ç¬¬{generation_round + 1}è½®ï¼šå‡†å¤‡ä»ç¬¬{generation_round + 1}æ‰¹{len(current_opportunities)}ä¸ªæœºä¼šä¸­æ™ºèƒ½æŒ‘é€‰{ideas_per_generation}ä¸ª")
            
            # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
            task = asyncio.create_task(
                self._strategic_idea_generation(current_opportunities, ideas_per_generation, generation_round, graph)
            )
            tasks.append(task)
            valid_batches.append(generation_round + 1)
        
        if not tasks:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„ç”Ÿæˆä»»åŠ¡ï¼Œæ— æ³•ç»§ç»­")
            idea_batches = []
        else:
            print(f"âš¡ å¼€å§‹å¹¶å‘æ‰§è¡Œ{len(tasks)}ä¸ªIdeaGeneratorä»»åŠ¡...")
            
            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ç”Ÿæˆä»»åŠ¡
            try:
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # å¤„ç†å¹¶å‘ç»“æœ
                for i, result in enumerate(results):
                    batch_num = valid_batches[i]
                    if isinstance(result, Exception):
                        print(f"âš ï¸ ç¬¬{batch_num}è½®ç”Ÿæˆå¤±è´¥: {str(result)}")
                        idea_batches.append([])  # æ·»åŠ ç©ºæ‰¹æ¬¡ä¿æŒç´¢å¼•å¯¹åº”
                    else:
                        batch_candidates = result
                        idea_batches.append(batch_candidates)
                        all_candidates.extend(batch_candidates)
                        print(f"âœ… ç¬¬{batch_num}è½®å®Œæˆï¼Œç”Ÿæˆäº†{len(batch_candidates)}ä¸ªé«˜è´¨é‡æƒ³æ³•")
                
                print(f"ğŸ‰ å¹¶å‘ç”Ÿæˆå®Œæˆï¼æ€»å…±{len(tasks)}ä¸ªä»»åŠ¡ï¼ŒæˆåŠŸ{len([r for r in results if not isinstance(r, Exception)])}ä¸ª")
                
            except Exception as e:
                print(f"âŒ å¹¶å‘æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
                idea_batches = []
        
        print(f"ğŸ¯ ç­–ç•¥æ€§ç”Ÿæˆå®Œæˆï¼šæ€»å…±ç”Ÿæˆ{len(all_candidates)}ä¸ªæƒ³æ³•ï¼Œåˆ†ä¸º{len(idea_batches)}æ‰¹")
        
        # æ­¥éª¤3ï¼šå»é‡å’Œæœ€ç»ˆæ’åºï¼ˆä½†ä¿æŒæ‰¹æ¬¡ç»“æ„ï¼‰
        final_candidates = await self._rank_and_filter(all_candidates, max_ideas)
        
        return {
            'all_candidates': final_candidates,
            'batches': idea_batches,
            'generation_info': {
                'num_generations': len(idea_batches),
                'ideas_per_generation': ideas_per_generation,
                'total_generated': len(all_candidates),
                'final_count': len(final_candidates),
                'opportunities_used': len(all_opportunities)
            }
        }

    async def _identify_triggers(self, graph: SemanticOpportunityGraph) -> List[Dict[str, Any]]:
        """è¯†åˆ«å›¾è°±ä¸­çš„è§¦å‘æ¨¡å¼ã€‚
        
        è¾“å…¥:
            - graph: è¯­ä¹‰æœºä¼šå›¾è°±ã€‚
            
        è¾“å‡º:
            - List[Dict]: è§¦å‘ä¿¡å·åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« patternã€nodesã€strategyã€confidenceã€‚
            
        å®ç°æ€è·¯:
            1) éå† graph.graph['gaps']ï¼Œæå–å·²æ£€æµ‹çš„æœºä¼šç¼ºå£ã€‚
            2) è¡¥å……å®æ—¶æ¨¡å¼åŒ¹é…ï¼šåˆ©ç”¨ NetworkX æŸ¥æ‰¾ç‰¹å®šå­å›¾ç»“æ„ã€‚
            3) ä¸ºæ¯ä¸ªtriggerå…³è”åˆé€‚çš„ç­–ç•¥æ¨¡æ¿ï¼Œä¼°è®¡è§¦å‘ç½®ä¿¡åº¦ã€‚
        """
        triggers = []
        
        # æ­¥éª¤1ï¼šä»å·²æ£€æµ‹çš„gapsä¸­æå–è§¦å‘ä¿¡å·
        gaps = graph.graph.get('gaps', [])
        print(f"    ğŸ¯ ä» {len(gaps)} ä¸ªgapsä¸­æå–è§¦å‘ä¿¡å·")
        
        for gap in gaps:
            pattern = gap.get('pattern', 'unknown')
            confidence = gap.get('confidence', 0.5)
            
            # æ ¹æ®gapæ¨¡å¼æ˜ å°„åˆ°ç­–ç•¥æ¨¡æ¿
            strategy = self._map_gap_to_strategy(pattern)
            
            if strategy:
                trigger = {
                    "pattern": pattern,
                    "nodes": gap.get('related_nodes', []),
                    "strategy": strategy,
                    "confidence": confidence,
                    "source": "gap_detection",
                    "gap_data": gap
                }
                triggers.append(trigger)
        
        # æ­¥éª¤2ï¼šè¡¥å……å®æ—¶æ¨¡å¼åŒ¹é…
        additional_triggers = await self._find_additional_patterns(graph)
        triggers.extend(additional_triggers)
        
        # æ­¥éª¤3ï¼šæŒ‰ç½®ä¿¡åº¦æ’åºå¹¶é™åˆ¶æ•°é‡
        triggers.sort(key=lambda x: x.get('confidence', 0), reverse=True)
        
        return triggers
    
    async def _collect_comprehensive_opportunities(self, graph: SemanticOpportunityGraph) -> List[Dict[str, Any]]:
        """æ”¶é›†å…¨é¢çš„æœºä¼šç‚¹ï¼Œä¸ºç­–ç•¥æ€§ç”Ÿæˆåšå‡†å¤‡ã€‚
        
        è¾“å…¥:
            - graph: è¯­ä¹‰æœºä¼šå›¾è°±ã€‚
            
        è¾“å‡º:
            - List[Dict[str, Any]]: è¯¦ç»†çš„æœºä¼šç‚¹åˆ—è¡¨ã€‚
        """
        opportunities = []
        
        # 1. åŸºç¡€è§¦å‘æ¨¡å¼è¯†åˆ«
        basic_triggers = await self._identify_triggers(graph)
        opportunities.extend(basic_triggers)
        
        # 2. æ‰©å±•æœºä¼šè¯†åˆ« - åŸºäºå›¾è°±ç»“æ„æ·±å…¥æŒ–æ˜
        extended_opportunities = await self._identify_extended_opportunities(graph)
        opportunities.extend(extended_opportunities)
        
        # 3. ç»„åˆæœºä¼š - å‘ç°å¤šä¸ªèŠ‚ç‚¹/è¾¹çš„å¤æ‚ç»„åˆ
        combination_opportunities = await self._identify_combination_opportunities(graph)
        opportunities.extend(combination_opportunities)
        
        # 4. åå‘å·¥ç¨‹æœºä¼š - ä»ç°æœ‰æ‰¹è¯„/é—®é¢˜ä¸­å‘ç°æœºä¼š
        reverse_opportunities = await self._identify_reverse_engineering_opportunities(graph)
        opportunities.extend(reverse_opportunities)
        
        # 5. è·¨é¢†åŸŸè¿ç§»æœºä¼š
        cross_domain_opportunities = await self._identify_cross_domain_opportunities(graph)
        opportunities.extend(cross_domain_opportunities)
        
        # å»é‡å¹¶ä¸°å¯Œæœºä¼šä¿¡æ¯
        unique_opportunities = self._deduplicate_and_enrich_opportunities(opportunities, graph)
        
        # ç”¨å®é™…èŠ‚ç‚¹åç§°å¢å¼ºæœºä¼šä¿¡æ¯
        enhanced_opportunities = self._enhance_opportunities_with_node_names(unique_opportunities, graph)
        
        return enhanced_opportunities
    
    def _allocate_opportunities_to_batches(self, opportunities: List[Dict[str, Any]], 
                                         num_batches: int) -> List[List[Dict[str, Any]]]:
        """å°†æœºä¼šæ™ºèƒ½åˆ†é…åˆ°ä¸åŒæ‰¹æ¬¡ä¸­ï¼Œç¡®ä¿æ¯æ‰¹æ¬¡çš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚"""
        if not opportunities:
            return [[] for _ in range(num_batches)]
        
        # æŒ‰é‡è¦æ€§å’Œç±»å‹å¯¹æœºä¼šè¿›è¡Œæ’åº
        sorted_opportunities = sorted(opportunities, 
                                   key=lambda x: (x.get('confidence', 0), 
                                                x.get('opportunity_type', 'zzz')), 
                                   reverse=True)
        
        # æ™ºèƒ½åˆ†é…ï¼šè½®æµåˆ†é…è€Œä¸æ˜¯ç®€å•åˆ‡ç‰‡ï¼Œç¡®ä¿æ¯æ‰¹éƒ½æœ‰é«˜è´¨é‡æœºä¼š
        batches = [[] for _ in range(num_batches)]
        
        for i, opportunity in enumerate(sorted_opportunities):
            batch_index = i % num_batches
            batches[batch_index].append(opportunity)
        
        # ç¡®ä¿æ¯æ‰¹æ¬¡éƒ½æœ‰è¶³å¤Ÿçš„æœºä¼šï¼ˆè‡³å°‘20ä¸ªï¼Œå¦‚æœæ€»æ•°å…è®¸ï¼‰
        min_per_batch = max(20, len(opportunities) // (num_batches * 2))
        
        # å¦‚æœæŸäº›æ‰¹æ¬¡å¤ªå°ï¼Œä»å¤§æ‰¹æ¬¡ä¸­é‡æ–°åˆ†é…
        for i in range(num_batches):
            if len(batches[i]) < min_per_batch:
                # æ‰¾åˆ°æœ€å¤§çš„æ‰¹æ¬¡
                largest_batch_idx = max(range(num_batches), key=lambda x: len(batches[x]))
                if len(batches[largest_batch_idx]) > min_per_batch:
                    # ç§»åŠ¨ä¸€äº›æœºä¼šåˆ°å°æ‰¹æ¬¡
                    need = min_per_batch - len(batches[i])
                    can_move = min(need, len(batches[largest_batch_idx]) - min_per_batch)
                    if can_move > 0:
                        moved_items = batches[largest_batch_idx][-can_move:]
                        batches[largest_batch_idx] = batches[largest_batch_idx][:-can_move]
                        batches[i].extend(moved_items)
        
        print(f"    ğŸ“Š æœºä¼šåˆ†é…è¯¦æƒ…: {[len(batch) for batch in batches]}")
        return batches
    
    async def _strategic_idea_generation(self, opportunities: List[Dict[str, Any]], 
                                       num_ideas: int, generation_round: int, 
                                       graph: SemanticOpportunityGraph) -> List[CandidateIdea]:
        """ç­–ç•¥æ€§æƒ³æ³•ç”Ÿæˆï¼šè®©LLMä»å¤§é‡æœºä¼šä¸­æ™ºèƒ½æŒ‘é€‰å¹¶ç”Ÿæˆæƒ³æ³•ã€‚
        
        è¾“å…¥:
            - opportunities: æ‰€æœ‰å¯ç”¨çš„æœºä¼šç‚¹ã€‚
            - num_ideas: è¦ç”Ÿæˆçš„æƒ³æ³•æ•°é‡ã€‚
            - generation_round: å½“å‰ç”Ÿæˆè½®æ¬¡ã€‚
            - graph: è¯­ä¹‰æœºä¼šå›¾è°±ã€‚
            
        è¾“å‡º:
            - List[CandidateIdea]: ç”Ÿæˆçš„å€™é€‰æƒ³æ³•åˆ—è¡¨ã€‚
        """
        print(f"    ğŸ¯ ç¬¬{generation_round + 1}è½®ï¼šä»{len(opportunities)}ä¸ªæœºä¼šä¸­ç­–ç•¥æ€§æŒ‘é€‰{num_ideas}ä¸ªidea")
        
        # å‡†å¤‡æœºä¼šæ‘˜è¦ï¼Œè®©LLMèƒ½å¤Ÿå…¨é¢äº†è§£æ‰€æœ‰é€‰æ‹©
        opportunities_summary = self._prepare_opportunities_summary(opportunities)
        
        # æ„é€ ç­–ç•¥æ€§ç”Ÿæˆçš„prompt
        prompt = f"""
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ç§‘ç ”æƒ³æ³•ç­–ç•¥å¸ˆã€‚ç°åœ¨éœ€è¦ä½ ä»ä»¥ä¸‹ç¬¬{generation_round + 1}æ‰¹å…± {len(opportunities)} ä¸ªç ”ç©¶æœºä¼šä¸­ï¼Œç­–ç•¥æ€§åœ°æŒ‘é€‰å‡º {num_ideas} ä¸ªæœ€æœ‰å‰æ™¯çš„æƒ³æ³•å¹¶è¿›è¡Œè¯¦ç»†è®¾è®¡ã€‚

**èƒŒæ™¯è¯´æ˜**ï¼š
- è¿™æ˜¯ç¬¬{generation_round + 1}è½®æƒ³æ³•ç”Ÿæˆ
- æœºä¼šå·²ç»è¿‡é¢„ç­›é€‰å’Œæ™ºèƒ½åˆ†é…
- éœ€è¦ä»å½“å‰æ‰¹æ¬¡ä¸­å‘ç°æœ€ä¼˜è´¨çš„ç ”ç©¶æ–¹å‘

**ä½ çš„ä»»åŠ¡**ï¼š
1. **æ‰¹æ¬¡åˆ†æ**ï¼šæ·±å…¥åˆ†æå½“å‰æ‰¹æ¬¡æœºä¼šçš„ç‰¹ç‚¹å’Œæ½œåŠ›
2. **ç­–ç•¥æ€§ç­›é€‰**ï¼šåŸºäºæ–°é¢–æ€§ã€å¯è¡Œæ€§ã€å½±å“åŠ›æ½œåŠ›è¿›è¡Œintelligent selection
3. **è¯¦ç»†è®¾è®¡**ï¼šä¸ºé€‰ä¸­çš„æƒ³æ³•æä¾›å®Œæ•´çš„ideaè®¾è®¡

**å¯ç”¨ç ”ç©¶æœºä¼š**ï¼š
{opportunities_summary}

**ç­›é€‰æ ‡å‡†**ï¼š
- **æ–°é¢–æ€§**: æ˜¯å¦å¡«è¡¥äº†é‡è¦çš„ç ”ç©¶ç©ºç™½ï¼Ÿæ˜¯å¦æä¾›äº†æ–°çš„è§†è§’ï¼Ÿ
- **å¯è¡Œæ€§**: æ‰€éœ€èµ„æºæ˜¯å¦åˆç†ï¼ŸæŠ€æœ¯è·¯å¾„æ˜¯å¦å¯è¡Œï¼Ÿ
- **å½±å“åŠ›**: æ˜¯å¦èƒ½æ¨åŠ¨é¢†åŸŸå‘å±•ï¼Ÿæ˜¯å¦æœ‰å®é™…åº”ç”¨ä»·å€¼ï¼Ÿ
- **ç§‘å­¦ä»·å€¼**: æ˜¯å¦æœ‰æ·±åˆ»çš„ç†è®ºè´¡çŒ®ï¼Ÿ
- **å®ç°æ½œåŠ›**: åœ¨å½“å‰æŠ€æœ¯æ¡ä»¶ä¸‹æ˜¯å¦å¯å®ç°ï¼Ÿ

**ç”Ÿæˆè¦æ±‚**ï¼š
- è¯·æŒ‘é€‰ {num_ideas} ä¸ªæœ€æœ‰å‰æ™¯çš„æœºä¼š
- ä¸ºæ¯ä¸ªé€‰ä¸­çš„æƒ³æ³•æä¾›è¯¦ç»†çš„è®¾è®¡
- è§£é‡Šä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªæœºä¼šè€Œéå…¶ä»–
- ç¡®ä¿æƒ³æ³•ä¹‹é—´æœ‰ä¸€å®šçš„å¤šæ ·æ€§

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«é€‰ä¸­çš„æƒ³æ³•ï¼š
{{
  "strategy_analysis": "ä½ çš„å…¨å±€åˆ†æå’Œé€‰æ‹©ç­–ç•¥",
  "selected_ideas": [
    {{
      "rank": 1,
      "title": "æƒ³æ³•æ ‡é¢˜",
      "core_hypothesis": "æ ¸å¿ƒå‡è®¾",
      "selection_rationale": "ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªæœºä¼šçš„è¯¦ç»†ç†ç”±",
      "opportunity_source": "åŸºäºå“ªä¸ª/å“ªäº›æœºä¼šç‚¹",
      "innovation_points": ["åˆ›æ–°ç‚¹1", "åˆ›æ–°ç‚¹2", "åˆ›æ–°ç‚¹3"],
      "expected_contributions": ["è´¡çŒ®1", "è´¡çŒ®2"],
      "required_assets": [
        {{"type": "dataset", "name": "æ•°æ®é›†åç§°", "availability": "å¯è·å¾—æ€§"}},
        {{"type": "method", "name": "æ–¹æ³•åç§°", "status": "ç°æœ‰/éœ€å¼€å‘"}}
      ],
      "preliminary_experiments": [
        {{"name": "å®éªŒ1", "purpose": "éªŒè¯ç›®çš„", "expected_outcome": "é¢„æœŸç»“æœ"}}
      ],
      "potential_risks": ["é£é™©1", "é£é™©2"],
      "uniqueness_vs_existing": "ä¸ç°æœ‰å·¥ä½œçš„åŒºåˆ«å’Œä¼˜åŠ¿"
    }}
  ],
  "rejected_opportunities": [
    {{
      "opportunity": "æœºä¼šæè¿°",
      "rejection_reason": "ä¸ºä»€ä¹ˆä¸é€‰æ‹©çš„ç†ç”±"
    }}
  ]
}}
"""

        try:
            # è°ƒç”¨LLMè¿›è¡Œç­–ç•¥æ€§ç”Ÿæˆ
            response_data = await self.llm.generate(
                model_name=self.config.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=15000,
                agent_name=self.name,
                task_type="strategic_idea_generation"
            )
            response = response_data.get('content', '')
            
            # è§£æLLMå“åº”
            import json
            import re
            try:
                if "```json" in response:
                    json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
                    if json_match:
                        json_content = json_match.group(1).strip()
                    else:
                        json_start = response.find("```json") + 7
                        json_content = response[json_start:].strip()
                        if json_content.endswith("```"):
                            json_content = json_content[:-3].strip()
                else:
                    json_content = response.strip()
                
                result = json.loads(json_content)
                
                # è½¬æ¢ä¸ºCandidateIdeaå¯¹è±¡
                candidates = []
                for i, idea_data in enumerate(result.get("selected_ideas", [])):
                    idea_id = f"strategic-gen-{generation_round+1}-{i+1:02d}"
                    
                    candidate = CandidateIdea(
                        id=idea_id,
                        title=idea_data.get("title", f"ç­–ç•¥æ€§æƒ³æ³•{i+1}"),
                        core_hypothesis=idea_data.get("core_hypothesis", ""),
                        initial_innovation_points=idea_data.get("innovation_points", []),
                        source_trigger_nodes=idea_data.get("opportunity_source", []),
                        expected_contribution=idea_data.get("expected_contributions", []),
                        required_assets=idea_data.get("required_assets", []),
                        preliminary_experiments=idea_data.get("preliminary_experiments", []),
                        risks=idea_data.get("potential_risks", []),
                        provenance={
                            "generation_round": generation_round + 1,
                            "selection_rationale": idea_data.get("selection_rationale", ""),
                            "uniqueness_analysis": idea_data.get("uniqueness_vs_existing", ""),
                            "rank_in_batch": idea_data.get("rank", i+1),
                            "strategy_analysis": result.get("strategy_analysis", ""),
                            "generation_method": "strategic_selection"
                        }
                    )
                    candidates.append(candidate)
                
                print(f"    âœ… æˆåŠŸç”Ÿæˆ{len(candidates)}ä¸ªç­–ç•¥æ€§æƒ³æ³•")
                return candidates
                
            except json.JSONDecodeError as e:
                print(f"    âš ï¸ JSONè§£æå¤±è´¥: {str(e)}")
                print(f"    ğŸ”§ å°è¯•ä¿®å¤JSONæ ¼å¼...")
                
                # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é”™è¯¯
                fixed_candidates = self._try_fix_json_and_parse(json_content, generation_round, e)
                if fixed_candidates:
                    print(f"    âœ… JSONä¿®å¤æˆåŠŸï¼Œç”Ÿæˆ{len(fixed_candidates)}ä¸ªç­–ç•¥æ€§æƒ³æ³•")
                    return fixed_candidates
                else:
                    print(f"    âŒ JSONä¿®å¤å¤±è´¥ï¼Œä¿å­˜å“åº”ç”¨äºè°ƒè¯•")
                    # ä¿å­˜åŸå§‹å“åº”ç”¨äºè°ƒè¯•
                    self._save_failed_response(response, generation_round, e)
                    return []
                
        except Exception as e:
            print(f"    âŒ ç­–ç•¥æ€§ç”Ÿæˆå¤±è´¥: {str(e)}")
            return []
    
    def _try_fix_json_and_parse(self, json_content: str, generation_round: int, original_error: Exception) -> List[CandidateIdea]:
        """å°è¯•ä¿®å¤JSONæ ¼å¼é”™è¯¯å¹¶é‡æ–°è§£æã€‚"""
        import json
        import re
        
        # å¸¸è§çš„JSONä¿®å¤ç­–ç•¥
        fixes = [
            # 1. ç§»é™¤æœ«å°¾çš„ä¸å®Œæ•´å†…å®¹
            lambda x: re.sub(r',\s*[}\]]\s*[^}\]]*$', '}', x.rsplit('}', 1)[0] + '}'),
            
            # 2. ä¿®å¤ç¼ºå¤±çš„å¼•å·
            lambda x: re.sub(r'(\w+):', r'"\1":', x),
            
            # 3. ä¿®å¤æœ«å°¾ç¼ºå¤±çš„æ‹¬å·
            lambda x: x.strip() + ('}' if x.count('{') > x.count('}') else ''),
            
            # 4. ç§»é™¤æœ«å°¾å¤šä½™çš„é€—å·
            lambda x: re.sub(r',(\s*[}\]])', r'\1', x),
            
            # 5. å°è¯•æˆªå–åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„æƒ³æ³•
            lambda x: self._truncate_to_last_complete_idea(x)
        ]
        
        for i, fix_func in enumerate(fixes):
            try:
                fixed_content = fix_func(json_content)
                result = json.loads(fixed_content)
                
                print(f"    ğŸ”§ JSONä¿®å¤ç­–ç•¥ {i+1} æˆåŠŸ")
                
                # è½¬æ¢ä¸ºCandidateIdeaå¯¹è±¡
                candidates = []
                for j, idea_data in enumerate(result.get("selected_ideas", [])):
                    idea_id = f"strategic-gen-{generation_round+1}-{j+1:02d}"
                    
                    candidate = CandidateIdea(
                        id=idea_id,
                        title=idea_data.get("title", f"ç­–ç•¥æ€§æƒ³æ³•{j+1}"),
                        core_hypothesis=idea_data.get("core_hypothesis", ""),
                        initial_innovation_points=idea_data.get("innovation_points", []),
                        source_trigger_nodes=idea_data.get("opportunity_source", []),
                        expected_contribution=idea_data.get("expected_contributions", []),
                        required_assets=idea_data.get("required_assets", []),
                        preliminary_experiments=idea_data.get("preliminary_experiments", []),
                        risks=idea_data.get("potential_risks", []),
                        provenance={
                            "generation_round": generation_round + 1,
                            "selection_rationale": idea_data.get("selection_rationale", ""),
                            "uniqueness_analysis": idea_data.get("uniqueness_vs_existing", ""),
                            "rank_in_batch": idea_data.get("rank", j+1),
                            "strategy_analysis": result.get("strategy_analysis", ""),
                            "generation_method": "strategic_selection_fixed",
                            "json_fix_strategy": i+1
                        }
                    )
                    candidates.append(candidate)
                
                return candidates
                
            except (json.JSONDecodeError, Exception) as e:
                continue
        
        return []
    
    def _truncate_to_last_complete_idea(self, json_content: str) -> str:
        """æˆªå–åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„æƒ³æ³•ã€‚"""
        import re
        
        # æŸ¥æ‰¾æ‰€æœ‰æƒ³æ³•çš„å¼€å§‹ä½ç½®
        idea_pattern = r'"title":\s*"[^"]*"'
        matches = list(re.finditer(idea_pattern, json_content))
        
        if len(matches) < 2:
            return json_content
        
        # æ‰¾åˆ°å€’æ•°ç¬¬äºŒä¸ªæƒ³æ³•çš„ä½ç½®ï¼Œæˆªå–åˆ°é‚£é‡Œ
        second_last_match = matches[-2]
        truncate_pos = second_last_match.start()
        
        # å‘å‰æŸ¥æ‰¾è¿™ä¸ªæƒ³æ³•çš„å¼€å§‹ "{"
        bracket_count = 0
        for i in range(truncate_pos, -1, -1):
            if json_content[i] == '}':
                bracket_count += 1
            elif json_content[i] == '{':
                bracket_count -= 1
                if bracket_count == 0:
                    # æ‰¾åˆ°è¿™ä¸ªæƒ³æ³•çš„å¼€å§‹ï¼Œæˆªå–åˆ°å‰ä¸€ä¸ªæƒ³æ³•ç»“æŸ
                    truncated = json_content[:i]
                    if truncated.rstrip().endswith(','):
                        truncated = truncated.rstrip()[:-1]  # ç§»é™¤æœ«å°¾é€—å·
                    return truncated + '\n  ]\n}'
        
        return json_content
    
    def _save_failed_response(self, response: str, generation_round: int, error: Exception) -> None:
        """ä¿å­˜å¤±è´¥çš„å“åº”ç”¨äºè°ƒè¯•ã€‚"""
        import os
        from datetime import datetime
        
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        log_dir = "./logs"
        os.makedirs(log_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"failed_json_response_round{generation_round}_{timestamp}.txt"
        filepath = os.path.join(log_dir, filename)
        
        # ä¿å­˜å“åº”å†…å®¹
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(f"Generation Round: {generation_round}\n")
            f.write(f"Error: {str(error)}\n")
            f.write(f"Timestamp: {datetime.now().isoformat()}\n")
            f.write("="*80 + "\n")
            f.write("Raw Response:\n")
            f.write("="*80 + "\n")
            f.write(response)
        
        print(f"    ğŸ’¾ å¤±è´¥å“åº”å·²ä¿å­˜åˆ°: {filepath}")
    
    def _enhance_opportunities_with_node_names(self, opportunities: List[Dict[str, Any]], 
                                             graph: SemanticOpportunityGraph) -> List[Dict[str, Any]]:
        """ç”¨å®é™…çš„èŠ‚ç‚¹åç§°å¢å¼ºæœºä¼šä¿¡æ¯ã€‚"""
        enhanced_opportunities = []
        
        for opp in opportunities:
            enhanced_opp = opp.copy()
            
            # è·å–èŠ‚ç‚¹çš„å®é™…åç§°
            nodes = opp.get('nodes', [])
            if nodes:
                node_names = []
                for node_id in nodes:
                    if node_id in graph.nodes:
                        node_name = graph.nodes[node_id].get('name', str(node_id))
                        node_names.append(node_name)
                    else:
                        node_names.append(str(node_id))
                enhanced_opp['node_names'] = node_names
                enhanced_opp['nodes'] = node_names  # æ›¿æ¢åŸå§‹çš„èŠ‚ç‚¹ID
            
            # è·å–ç›¸å…³èŠ‚ç‚¹çš„å®é™…åç§°
            related_nodes = opp.get('related_nodes', [])
            if related_nodes:
                related_node_names = []
                for node_id in related_nodes:
                    if node_id in graph.nodes:
                        node_name = graph.nodes[node_id].get('name', str(node_id))
                        related_node_names.append(node_name)
                    else:
                        related_node_names.append(str(node_id))
                enhanced_opp['related_node_names'] = related_node_names
            
            enhanced_opportunities.append(enhanced_opp)
        
        return enhanced_opportunities
    
    async def _identify_extended_opportunities(self, graph: SemanticOpportunityGraph) -> List[Dict[str, Any]]:
        """è¯†åˆ«æ‰©å±•æœºä¼š - åŸºäºå›¾è°±ç»“æ„æ·±å…¥æŒ–æ˜ã€‚"""
        opportunities = []
        
        # 1. é«˜åº¦è¿æ¥ä½†ç¼ºä¹æŸç§å…³ç³»çš„èŠ‚ç‚¹å¯¹
        nodes = list(graph.nodes())
        for i, node1 in enumerate(nodes):
            for node2 in nodes[i+1:]:
                # æ£€æŸ¥ä¸¤ä¸ªèŠ‚ç‚¹æ˜¯å¦åœ¨å›¾ä¸­æœ‰å¾ˆå¤šå…±åŒé‚»å±…ä½†æ²¡æœ‰ç›´æ¥è¿æ¥
                common_neighbors = set(graph.neighbors(node1)) & set(graph.neighbors(node2))
                if len(common_neighbors) >= 2 and not graph.has_edge(node1, node2):
                    opportunities.append({
                        "pattern": "missing_connection",
                        "nodes": [node1, node2],
                        "explanation": f"èŠ‚ç‚¹{graph.nodes[node1]['name']}å’Œ{graph.nodes[node2]['name']}æœ‰{len(common_neighbors)}ä¸ªå…±åŒé‚»å±…ä½†ç¼ºä¹ç›´æ¥è¿æ¥",
                        "confidence": min(len(common_neighbors) * 0.15, 0.8),
                        "related_nodes": [node1, node2] + list(common_neighbors)[:3],
                        "opportunity_type": "connection_gap"
                    })
        
        # 2. å…³é”®èŠ‚ç‚¹çš„æœªå¼€å‘æ½œåŠ›
        degree_centrality = nx.degree_centrality(graph)
        high_degree_nodes = [node for node, centrality in degree_centrality.items() if centrality > 0.1]
        
        for node in high_degree_nodes:
            node_edges = list(graph.edges(node, data=True))
            edge_types = [edge[2].get('relation', 'unknown') for edge in node_edges]
            
            # å¦‚æœè¿™ä¸ªé‡è¦èŠ‚ç‚¹ç¼ºå°‘æŸäº›å¸¸è§å…³ç³»ç±»å‹
            common_relations = ['applies_to', 'improves', 'evaluates_with', 'requires']
            missing_relations = [rel for rel in common_relations if rel not in edge_types]
            
            if missing_relations:
                opportunities.append({
                    "pattern": "underexplored_potential",
                    "nodes": [node], 
                    "explanation": f"é‡è¦èŠ‚ç‚¹{graph.nodes[node]['name']}ç¼ºå°‘{missing_relations}ç­‰å…³ç³»",
                    "confidence": 0.6,
                    "related_nodes": [node],
                    "missing_relations": missing_relations,
                    "opportunity_type": "node_potential"
                })
        
        return opportunities[:20]  # é™åˆ¶æ•°é‡
    
    async def _identify_combination_opportunities(self, graph: SemanticOpportunityGraph) -> List[Dict[str, Any]]:
        """è¯†åˆ«ç»„åˆæœºä¼š - å‘ç°å¤šä¸ªèŠ‚ç‚¹/è¾¹çš„å¤æ‚ç»„åˆã€‚"""
        opportunities = []
        
        # 1. æ–¹æ³•ç»„åˆæœºä¼š
        method_nodes = [node for node, data in graph.nodes(data=True) if data.get('type') == 'Method']
        
        for i, method1 in enumerate(method_nodes):
            for method2 in method_nodes[i+1:]:
                # æ£€æŸ¥ä¸¤ä¸ªæ–¹æ³•æ˜¯å¦é€‚åˆç»„åˆ
                method1_tasks = [neighbor for neighbor in graph.neighbors(method1) 
                               if graph.nodes[neighbor].get('type') == 'Task']
                method2_tasks = [neighbor for neighbor in graph.neighbors(method2) 
                               if graph.nodes[neighbor].get('type') == 'Task']
                
                # å¦‚æœä¸¤ä¸ªæ–¹æ³•å¤„ç†ç›¸å…³ä½†ä¸åŒçš„ä»»åŠ¡
                if len(set(method1_tasks) & set(method2_tasks)) == 0 and len(method1_tasks) > 0 and len(method2_tasks) > 0:
                    opportunities.append({
                        "pattern": "method_combination",
                        "nodes": [method1, method2],
                        "explanation": f"æ–¹æ³•{graph.nodes[method1]['name']}å’Œ{graph.nodes[method2]['name']}å¯èƒ½å…·æœ‰äº’è¡¥æ€§",
                        "confidence": 0.65,
                        "related_nodes": [method1, method2] + method1_tasks[:2] + method2_tasks[:2],
                        "opportunity_type": "method_synergy"
                    })
        
        # 2. æ•°æ®é›†-è¯„ä»·æŒ‡æ ‡æ–°ç»„åˆ
        dataset_nodes = [node for node, data in graph.nodes(data=True) if data.get('type') == 'Dataset']
        metric_nodes = [node for node, data in graph.nodes(data=True) if data.get('type') == 'Metric']
        
        for dataset in dataset_nodes[:10]:  # é™åˆ¶æ•°é‡
            dataset_metrics = [neighbor for neighbor in graph.neighbors(dataset) 
                             if graph.nodes[neighbor].get('type') == 'Metric']
            
            # å¯»æ‰¾æœªä¸æ­¤æ•°æ®é›†å…³è”çš„ç›¸å…³è¯„ä»·æŒ‡æ ‡
            for metric in metric_nodes:
                if metric not in dataset_metrics:
                    opportunities.append({
                        "pattern": "dataset_metric_combination",
                        "nodes": [dataset, metric],
                        "explanation": f"æ•°æ®é›†{graph.nodes[dataset]['name']}ä¸è¯„ä»·æŒ‡æ ‡{graph.nodes[metric]['name']}çš„æ–°ç»„åˆ",
                        "confidence": 0.5,
                        "related_nodes": [dataset, metric],
                        "opportunity_type": "evaluation_innovation"
                    })
        
        return opportunities[:15]  # é™åˆ¶æ•°é‡
    
    async def _identify_reverse_engineering_opportunities(self, graph: SemanticOpportunityGraph) -> List[Dict[str, Any]]:
        """è¯†åˆ«åå‘å·¥ç¨‹æœºä¼š - ä»ç°æœ‰æ‰¹è¯„/é—®é¢˜ä¸­å‘ç°æœºä¼šã€‚"""
        opportunities = []
        
        # 1. ä»é—®é¢˜èŠ‚ç‚¹åå‘æ¨å¯¼è§£å†³æ–¹æ¡ˆ
        problem_nodes = [node for node, data in graph.nodes(data=True) if data.get('type') == 'Problem']
        
        for problem in problem_nodes:
            problem_name = graph.nodes[problem]['name']
            
            # å¯»æ‰¾å¯èƒ½è§£å†³æ­¤é—®é¢˜çš„ç°æœ‰æ–¹æ³•
            related_methods = []
            for node in graph.nodes():
                if graph.nodes[node].get('type') == 'Method':
                    # æ£€æŸ¥æ˜¯å¦æœ‰è·¯å¾„è¿æ¥è¿™ä¸ªæ–¹æ³•å’Œé—®é¢˜
                    try:
                        if nx.has_path(graph, node, problem) and nx.shortest_path_length(graph, node, problem) <= 3:
                            related_methods.append(node)
                    except:
                        continue
            
            if related_methods:
                opportunities.append({
                    "pattern": "problem_solving",
                    "nodes": [problem] + related_methods[:2],
                    "explanation": f"é’ˆå¯¹é—®é¢˜'{problem_name}'çš„æ”¹è¿›è§£å†³æ–¹æ¡ˆ",
                    "confidence": 0.7,
                    "related_nodes": [problem] + related_methods[:3],
                    "opportunity_type": "problem_driven"
                })
        
        # 2. ä»è´Ÿé¢è¾¹ï¼ˆæ‰¹è¯„å…³ç³»ï¼‰ä¸­å‘ç°æ”¹è¿›æœºä¼š
        negative_edges = [(u, v, data) for u, v, data in graph.edges(data=True) 
                         if data.get('relation', '').lower() in ['critiques', 'improves', 'addresses']]
        
        for src, dst, edge_data in negative_edges[:10]:
            if edge_data.get('relation') == 'critiques':
                opportunities.append({
                    "pattern": "improvement_opportunity",
                    "nodes": [src, dst],
                    "explanation": f"åŸºäºå¯¹{graph.nodes[dst]['name']}çš„æ‰¹è¯„ï¼Œå¼€å‘æ”¹è¿›æ–¹æ¡ˆ",
                    "confidence": 0.75,
                    "related_nodes": [src, dst],
                    "opportunity_type": "critique_driven"
                })
        
        return opportunities[:10]  # é™åˆ¶æ•°é‡
    
    async def _identify_cross_domain_opportunities(self, graph: SemanticOpportunityGraph) -> List[Dict[str, Any]]:
        """è¯†åˆ«è·¨é¢†åŸŸè¿ç§»æœºä¼šã€‚"""
        opportunities = []
        
        # 1. è·¨ä»»åŠ¡è¿ç§»
        task_nodes = [node for node, data in graph.nodes(data=True) if data.get('type') == 'Task']
        
        for i, task1 in enumerate(task_nodes):
            for task2 in task_nodes[i+1:]:
                # æ£€æŸ¥ä¸¤ä¸ªä»»åŠ¡æ˜¯å¦æœ‰ç›¸ä¼¼çš„æ–¹æ³•ä½†æ²¡æœ‰ç›´æ¥å…³è”
                task1_methods = [neighbor for neighbor in graph.neighbors(task1) 
                               if graph.nodes[neighbor].get('type') == 'Method']
                task2_methods = [neighbor for neighbor in graph.neighbors(task2) 
                               if graph.nodes[neighbor].get('type') == 'Method']
                
                common_methods = set(task1_methods) & set(task2_methods)
                if len(common_methods) >= 1 and not graph.has_edge(task1, task2):
                    opportunities.append({
                        "pattern": "cross_task_transfer",
                        "nodes": [task1, task2],
                        "explanation": f"ä»»åŠ¡{graph.nodes[task1]['name']}å’Œ{graph.nodes[task2]['name']}é—´çš„æ–¹æ³•è¿ç§»",
                        "confidence": len(common_methods) * 0.2,
                        "related_nodes": [task1, task2] + list(common_methods)[:2],
                        "opportunity_type": "domain_transfer"
                    })
        
        return opportunities[:12]  # é™åˆ¶æ•°é‡
    
    def _deduplicate_and_enrich_opportunities(self, opportunities: List[Dict[str, Any]], 
                                            graph: SemanticOpportunityGraph) -> List[Dict[str, Any]]:
        """å»é‡å¹¶ä¸°å¯Œæœºä¼šä¿¡æ¯ã€‚"""
        # ç®€å•å»é‡ - åŸºäºèŠ‚ç‚¹ç»„åˆ
        seen_combinations = set()
        unique_opportunities = []
        
        for opp in opportunities:
            nodes_key = tuple(sorted(opp.get('nodes', [])))
            if nodes_key not in seen_combinations:
                seen_combinations.add(nodes_key)
                
                # ä¸°å¯Œæœºä¼šä¿¡æ¯
                opp['detailed_description'] = self._generate_opportunity_description(opp, graph)
                opp['complexity_estimate'] = self._estimate_opportunity_complexity(opp, graph)
                
                unique_opportunities.append(opp)
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        unique_opportunities.sort(key=lambda x: x.get('confidence', 0), reverse=True)
        
        return unique_opportunities
    
    def _generate_opportunity_description(self, opportunity: Dict[str, Any], 
                                        graph: SemanticOpportunityGraph) -> str:
        """ç”Ÿæˆæœºä¼šçš„è¯¦ç»†æè¿°ã€‚"""
        pattern = opportunity.get('pattern', 'unknown')
        nodes = opportunity.get('nodes', [])
        
        if not nodes:
            return opportunity.get('explanation', 'æœªçŸ¥æœºä¼š')
        
        node_names = [graph.nodes[node]['name'] for node in nodes if node in graph.nodes]
        
        if pattern == 'transfer':
            return f"å°†{node_names[0]}çš„æˆåŠŸç»éªŒè¿ç§»åˆ°{node_names[1]}é¢†åŸŸ"
        elif pattern == 'method_combination':
            return f"ç»“åˆ{node_names[0]}å’Œ{node_names[1]}çš„äº’è¡¥ä¼˜åŠ¿åˆ›é€ æ–°æ–¹æ³•"
        elif pattern == 'problem_solving':
            return f"é’ˆå¯¹{node_names[0]}é—®é¢˜å¼€å‘åˆ›æ–°è§£å†³æ–¹æ¡ˆ"
        else:
            return opportunity.get('explanation', 'ç ”ç©¶æœºä¼š')
    
    def _estimate_opportunity_complexity(self, opportunity: Dict[str, Any], 
                                       graph: SemanticOpportunityGraph) -> str:
        """ä¼°ç®—æœºä¼šçš„å¤æ‚ç¨‹åº¦ã€‚"""
        nodes = opportunity.get('nodes', [])
        
        if len(nodes) <= 1:
            return "ä½"
        elif len(nodes) <= 3:
            return "ä¸­"
        else:
            return "é«˜"
    
    def _prepare_opportunities_summary(self, opportunities: List[Dict[str, Any]]) -> str:
        """å‡†å¤‡æœºä¼šæ‘˜è¦ä¾›LLMåˆ†æã€‚"""
        summary_lines = []
        
        for i, opp in enumerate(opportunities[:50], 1):  # é™åˆ¶æœ€å¤š50ä¸ª
            pattern = opp.get('pattern', 'unknown')
            confidence = opp.get('confidence', 0)
            complexity = opp.get('complexity_estimate', 'æœªçŸ¥')
            opportunity_type = opp.get('opportunity_type', 'general')
            
            # ç”Ÿæˆå…·ä½“çš„æœºä¼šæè¿°
            detailed_description = self._generate_detailed_opportunity_description(opp)
            
            summary_lines.append(
                f"{i}. [{pattern.upper()}] {detailed_description}\n"
                f"   - ç±»å‹: {opportunity_type}\n"
                f"   - å¤æ‚åº¦: {complexity}\n"
                f"   - ç½®ä¿¡åº¦: {confidence:.2f}\n"
            )
        
        return "\n".join(summary_lines)
    
    def _generate_detailed_opportunity_description(self, opportunity: Dict[str, Any]) -> str:
        """æ ¹æ®æœºä¼šç»“æ„ç”Ÿæˆè¯¦ç»†çš„ç ”ç©¶æœºä¼šæè¿°ã€‚"""
        pattern = opportunity.get('pattern', 'unknown')
        nodes = opportunity.get('nodes', [])
        related_nodes = opportunity.get('related_nodes', [])
        gap_data = opportunity.get('gap_data', {})
        
        # ä¼˜å…ˆä½¿ç”¨å·²æœ‰çš„è¯¦ç»†æè¿°
        if opportunity.get('detailed_description'):
            return opportunity['detailed_description']
        
        # ä¼˜å…ˆä½¿ç”¨å·²æœ‰çš„explanationï¼ˆå¦‚æœä¸ä¸ºç©ºï¼‰
        explanation = opportunity.get('explanation', '').strip()
        if explanation and explanation != '':
            return explanation
        
        # æ ¹æ®ä¸åŒæ¨¡å¼ç”Ÿæˆå…·ä½“æè¿°
        if pattern == 'transfer':
            return self._generate_transfer_description(opportunity, gap_data)
        elif pattern == 'missing_connection':
            return self._generate_missing_connection_description(opportunity)
        elif pattern == 'cross_task_transfer':
            return self._generate_cross_task_description(opportunity)
        elif pattern == 'method_combination':
            return self._generate_method_combination_description(opportunity)
        elif pattern == 'dataset_metric_combination':
            return self._generate_dataset_metric_description(opportunity)
        elif pattern == 'problem_solving':
            return self._generate_problem_solving_description(opportunity)
        elif pattern == 'improvement_opportunity':
            return self._generate_improvement_description(opportunity)
        elif pattern == 'underexplored_potential':
            return self._generate_underexplored_description(opportunity)
        else:
            # é€šç”¨æè¿°ç”Ÿæˆ
            return self._generate_generic_description(opportunity, pattern)
    
    def _generate_transfer_description(self, opportunity: Dict[str, Any], gap_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆè¿ç§»æœºä¼šçš„å…·ä½“æè¿°ã€‚"""
        # ä»gap_dataä¸­æå–å…·ä½“ä¿¡æ¯
        source_method = gap_data.get('source_method', 'æœªçŸ¥æ–¹æ³•')
        source_task = gap_data.get('source_task', 'æœªçŸ¥ä»»åŠ¡')
        target_task = gap_data.get('target_task', 'æœªçŸ¥ç›®æ ‡ä»»åŠ¡')
        
        if source_method != 'æœªçŸ¥æ–¹æ³•' and target_task != 'æœªçŸ¥ç›®æ ‡ä»»åŠ¡':
            return f"å°†{source_method}æ–¹æ³•ä»{source_task}è¿ç§»åˆ°{target_task}ä»»åŠ¡"
        else:
            # ä»èŠ‚ç‚¹ä¿¡æ¯æ¨æ–­
            nodes = opportunity.get('nodes', [])
            if len(nodes) >= 2:
                return f"æ¢ç´¢{nodes[0]}ä¸{nodes[1]}ä¹‹é—´çš„è¿ç§»å­¦ä¹ æœºä¼š"
            else:
                return "æ¢ç´¢è·¨é¢†åŸŸæ–¹æ³•è¿ç§»çš„æ–°æœºä¼š"
    
    def _generate_missing_connection_description(self, opportunity: Dict[str, Any]) -> str:
        """ç”Ÿæˆç¼ºå¤±è¿æ¥çš„å…·ä½“æè¿°ã€‚"""
        nodes = opportunity.get('nodes', [])
        if len(nodes) >= 2:
            return f"æ¢ç´¢{nodes[0]}å’Œ{nodes[1]}ä¹‹é—´çš„æ½œåœ¨å…³è”æ€§ç ”ç©¶"
        else:
            return "å‘ç°å…³é”®æ¦‚å¿µé—´çš„ç¼ºå¤±è¿æ¥"
    
    def _generate_cross_task_description(self, opportunity: Dict[str, Any]) -> str:
        """ç”Ÿæˆè·¨ä»»åŠ¡è¿ç§»çš„å…·ä½“æè¿°ã€‚"""
        nodes = opportunity.get('nodes', [])
        if len(nodes) >= 2:
            return f"å¼€å‘ä»{nodes[0]}åˆ°{nodes[1]}çš„è·¨ä»»åŠ¡è¿ç§»æ–¹æ³•"
        else:
            return "æ¢ç´¢è·¨ä»»åŠ¡çŸ¥è¯†è¿ç§»çš„æ–°æ–¹æ³•"
    
    def _generate_method_combination_description(self, opportunity: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ–¹æ³•ç»„åˆçš„å…·ä½“æè¿°ã€‚"""
        nodes = opportunity.get('nodes', [])
        if len(nodes) >= 2:
            return f"ç ”ç©¶{nodes[0]}ä¸{nodes[1]}æ–¹æ³•çš„åˆ›æ–°æ€§ç»„åˆ"
        else:
            return "æ¢ç´¢å¤šç§æ–¹æ³•çš„ååŒç»„åˆç­–ç•¥"
    
    def _generate_dataset_metric_description(self, opportunity: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ•°æ®é›†-æŒ‡æ ‡ç»„åˆçš„å…·ä½“æè¿°ã€‚"""
        nodes = opportunity.get('nodes', [])
        if len(nodes) >= 2:
            return f"å¼€å‘{nodes[0]}æ•°æ®é›†ä¸Š{nodes[1]}æŒ‡æ ‡çš„è¯„ä¼°ä½“ç³»"
        else:
            return "å»ºç«‹æ–°çš„æ•°æ®é›†è¯„ä¼°æŒ‡æ ‡ä½“ç³»"
    
    def _generate_problem_solving_description(self, opportunity: Dict[str, Any]) -> str:
        """ç”Ÿæˆé—®é¢˜è§£å†³çš„å…·ä½“æè¿°ã€‚"""
        nodes = opportunity.get('nodes', [])
        if len(nodes) >= 1:
            return f"é’ˆå¯¹{nodes[0]}é—®é¢˜å¼€å‘åˆ›æ–°è§£å†³æ–¹æ¡ˆ"
        else:
            return "å¼€å‘é’ˆå¯¹å…³é”®æŠ€æœ¯é—®é¢˜çš„åˆ›æ–°è§£å†³æ–¹æ¡ˆ"
    
    def _generate_improvement_description(self, opportunity: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ”¹è¿›æœºä¼šçš„å…·ä½“æè¿°ã€‚"""
        nodes = opportunity.get('nodes', [])
        if len(nodes) >= 2:
            return f"åŸºäº{nodes[0]}çš„è§‚å¯Ÿï¼Œæ”¹è¿›{nodes[1]}çš„æ€§èƒ½å’Œæ•ˆç‡"
        else:
            return "åŸºäºç°æœ‰æ‰¹è¯„å’Œè§‚å¯Ÿï¼Œå¼€å‘æ€§èƒ½æ”¹è¿›æ–¹æ¡ˆ"
    
    def _generate_underexplored_description(self, opportunity: Dict[str, Any]) -> str:
        """ç”Ÿæˆæœªå……åˆ†æ¢ç´¢çš„å…·ä½“æè¿°ã€‚"""
        nodes = opportunity.get('nodes', [])
        missing_relations = opportunity.get('missing_relations', [])
        
        if nodes and missing_relations:
            return f"æ·±å…¥æŒ–æ˜{nodes[0]}åœ¨{', '.join(missing_relations[:2])}æ–¹é¢çš„åº”ç”¨æ½œåŠ›"
        elif nodes:
            return f"å…¨é¢æ¢ç´¢{nodes[0]}çš„æœªå¼€å‘åº”ç”¨æ½œåŠ›"
        else:
            return "è¯†åˆ«å’Œå¼€å‘å…³é”®æŠ€æœ¯çš„æœªæ¢ç´¢åº”ç”¨é¢†åŸŸ"
    
    def _generate_generic_description(self, opportunity: Dict[str, Any], pattern: str) -> str:
        """ç”Ÿæˆé€šç”¨æè¿°ã€‚"""
        nodes = opportunity.get('nodes', [])
        opportunity_type = opportunity.get('opportunity_type', 'general')
        
        if nodes:
            if len(nodes) == 1:
                return f"æ¢ç´¢{nodes[0]}çš„åˆ›æ–°åº”ç”¨å’Œå‘å±•æœºä¼šï¼ˆ{pattern}ç±»å‹ï¼‰"
            elif len(nodes) == 2:
                return f"ç ”ç©¶{nodes[0]}ä¸{nodes[1]}çš„ååŒåˆ›æ–°æœºä¼šï¼ˆ{pattern}ç±»å‹ï¼‰"
            else:
                return f"å¼€å‘æ¶‰åŠ{nodes[0]}ã€{nodes[1]}ç­‰å¤šè¦ç´ çš„ç»¼åˆåˆ›æ–°æ–¹æ¡ˆï¼ˆ{pattern}ç±»å‹ï¼‰"
        else:
            return f"æ¢ç´¢{opportunity_type}é¢†åŸŸçš„{pattern}ç ”ç©¶æœºä¼š"
    
    def _map_gap_to_strategy(self, gap_pattern: str) -> Optional[str]:
        """å°†gapæ¨¡å¼æ˜ å°„åˆ°ç­–ç•¥æ¨¡æ¿ã€‚"""
        pattern_to_strategy = {
            "transfer": "transfer_across_tasks",
            "composition": "compose_methods", 
            "reverse": "reverse_critique",
            "evaluation": "evaluation_reconstruction",
            "data_enhancement": "data_augmentation"
        }
        return pattern_to_strategy.get(gap_pattern)
    
    async def _find_additional_patterns(self, graph: SemanticOpportunityGraph) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾å›¾è°±ä¸­çš„é¢å¤–æ¨¡å¼ã€‚"""
        additional_triggers = []
        
        # æ¨¡å¼1ï¼šé«˜åº¦ä¸­å¿ƒçš„èŠ‚ç‚¹å¯èƒ½é€‚åˆè¿ç§»
        high_salience_nodes = []
        for node_id, node_data in graph.nodes(data=True):
            if node_data.get('salience', 0) > 0.7:
                high_salience_nodes.append(node_id)
        
        if len(high_salience_nodes) > 0:
            for node in high_salience_nodes[:3]:  # é™åˆ¶æ•°é‡
                additional_triggers.append({
                    "pattern": "high_impact_extension",
                    "nodes": [node],
                    "strategy": "transfer_across_tasks",
                    "confidence": 0.6,
                    "source": "salience_analysis"
                })
        
        # æ¨¡å¼2ï¼šå­¤ç«‹èŠ‚ç‚¹å¯èƒ½éœ€è¦è¿æ¥
        isolated_nodes = [node for node in graph.nodes() if graph.degree(node) == 0]
        if isolated_nodes:
            for node in isolated_nodes[:2]:  # é™åˆ¶æ•°é‡
                additional_triggers.append({
                    "pattern": "isolated_integration",
                    "nodes": [node],
                    "strategy": "compose_methods",
                    "confidence": 0.4,
                    "source": "topology_analysis"
                })
        
        return additional_triggers

    async def _generate_from_trigger(self, trigger: Dict[str, Any], graph: SemanticOpportunityGraph) -> Optional[CandidateIdea]:
        """ä»å•ä¸ªè§¦å‘ä¿¡å·ç”Ÿæˆå€™é€‰æƒ³æ³•ã€‚
        
        è¾“å…¥:
            - trigger: è§¦å‘ä¿¡å·å­—å…¸ã€‚
            - graph: è¯­ä¹‰æœºä¼šå›¾è°±ã€‚
            
        è¾“å‡º:
            - CandidateIdea: ç”Ÿæˆçš„å€™é€‰æƒ³æ³•ï¼Œå¤±è´¥æ—¶è¿”å›Noneã€‚
            
        å®ç°æ€è·¯:
            1) æ ¹æ® trigger['strategy'] é€‰æ‹©å¯¹åº”çš„ç­–ç•¥æ¨¡æ¿ã€‚
            2) ä½¿ç”¨ Chain-of-Ideas æç¤ºé“¾ï¼šè§¦å‘æ¨¡å¼ â†’ æ ¸å¿ƒå‡è®¾ â†’ æœºåˆ¶è®¾æƒ³ â†’ å®éªŒè‰æ¡ˆ â†’ é¢„æœŸæ”¶ç›Š â†’ é£é™©è¯„ä¼°ã€‚
            3) è°ƒç”¨ LLM ç”Ÿæˆç»“æ„åŒ–å†…å®¹ï¼Œè§£æä¸º CandidateIdea å¯¹è±¡ã€‚
        """
        try:
            strategy_name = trigger.get('strategy')
            if not strategy_name or strategy_name not in self.strategy_templates:
                return None
            
            strategy_template = self.strategy_templates[strategy_name]
            trigger_nodes = trigger.get('nodes', [])
            
            # æ”¶é›†èŠ‚ç‚¹ä¿¡æ¯
            node_info = {}
            for node_id in trigger_nodes:
                if node_id in graph.nodes():
                    node_info[node_id] = {
                        "name": graph.nodes[node_id].get('name', node_id),
                        "type": graph.nodes[node_id].get('type', 'Unknown'),
                        "salience": graph.nodes[node_id].get('salience', 0.0)
                    }
            
            # åŸºäºç­–ç•¥æ¨¡æ¿ç”Ÿæˆæƒ³æ³•å†…å®¹
            idea_content = await self._apply_strategy_template(strategy_template, trigger, node_info, graph)
            
            if not idea_content:
                return None
            
            # ç”Ÿæˆæƒ³æ³•ID
            idea_id = f"IDEA-{len(trigger_nodes):02d}-{strategy_name[:8]}-{hash(str(trigger_nodes)) % 10000:04d}"
            
            # æ„é€ CandidateIdeaå¯¹è±¡
            candidate_idea = CandidateIdea(
                id=idea_id,
                title=idea_content.get('title', f'åŸºäº{strategy_name}çš„æƒ³æ³•'),
                core_hypothesis=idea_content.get('hypothesis', ''),
                initial_innovation_points=idea_content.get('innovation_points', []),
                source_trigger_nodes=trigger_nodes,
                expected_contribution=idea_content.get('contributions', []),
                required_assets=idea_content.get('assets', []),
                preliminary_experiments=idea_content.get('experiments', []),
                risks=idea_content.get('risks', []),
                provenance={
                    "trigger": trigger,
                    "strategy": strategy_name,
                    "generated_at": datetime.now().isoformat(),
                    "confidence": trigger.get('confidence', 0.5)
                },
                version=1
            )
            
            return candidate_idea
            
        except Exception as e:
            print(f"    âš ï¸ ä»è§¦å‘å™¨ç”Ÿæˆæƒ³æ³•å¤±è´¥: {e}")
            return None
    
    async def _apply_strategy_template(self, template: Dict[str, Any], trigger: Dict[str, Any], 
                                     node_info: Dict[str, Any], graph: SemanticOpportunityGraph) -> Optional[Dict[str, Any]]:
        """åº”ç”¨ç­–ç•¥æ¨¡æ¿ç”Ÿæˆæƒ³æ³•å†…å®¹ã€‚"""
        strategy_name = trigger.get('strategy')
        
        # æ ¹æ®ä¸åŒç­–ç•¥ç”Ÿæˆå†…å®¹
        if strategy_name == "transfer_across_tasks":
            return await self._generate_transfer_idea(template, trigger, node_info, graph)
        elif strategy_name == "compose_methods":
            return await self._generate_composition_idea(template, trigger, node_info, graph)
        elif strategy_name == "reverse_critique":
            return await self._generate_reverse_idea(template, trigger, node_info, graph)
        else:
            # é€šç”¨æ¨¡æ¿
            return await self._generate_generic_idea(template, trigger, node_info, graph)
    
    async def _generate_transfer_idea(self, template: Dict[str, Any], trigger: Dict[str, Any], 
                                    node_info: Dict[str, Any], graph: SemanticOpportunityGraph) -> Dict[str, Any]:
        """ç”Ÿæˆè¿ç§»ç±»å‹çš„æƒ³æ³•ã€‚"""
        trigger_nodes = trigger.get('nodes', [])
        gap_data = trigger.get('gap_data', {})
        
        # è¯†åˆ«æ–¹æ³•å’Œä»»åŠ¡
        method_node = None
        task_node = None
        
        for node_id in trigger_nodes:
            if node_id in node_info:
                if node_info[node_id]['type'] == 'Method':
                    method_node = node_id
                elif node_info[node_id]['type'] == 'Task':
                    task_node = node_id
        
        if not method_node:
            method_node = gap_data.get('from_method')
        if not task_node:
            task_node = gap_data.get('to_task')
        
        method_name = graph.nodes[method_node]['name'] if method_node in graph.nodes() else "Unknown Method"
        task_name = graph.nodes[task_node]['name'] if task_node in graph.nodes() else "Unknown Task"
        
        # æ”¶é›†ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_info = self._collect_context_info(graph, [method_node, task_node])
        
        # ä½¿ç”¨LLMç”Ÿæˆå…·ä½“çš„ç ”ç©¶æƒ³æ³•
        prompt = f"""
åŸºäºè¯­ä¹‰æœºä¼šå›¾è°±åˆ†æï¼Œè®¾è®¡ä¸€ä¸ªåˆ›æ–°çš„ç ”ç©¶æƒ³æ³•ï¼Œå°†æ–¹æ³•"{method_name}"è¿ç§»åº”ç”¨åˆ°ä»»åŠ¡"{task_name}"ä¸­ã€‚

**èƒŒæ™¯ä¿¡æ¯**ï¼š
- æºæ–¹æ³•: {method_name}
- ç›®æ ‡ä»»åŠ¡: {task_name}
- å‘ç°çš„æœºä¼šgap: {gap_data.get('description', 'è·¨ä»»åŠ¡è¿ç§»æœºä¼š')}

**ä¸Šä¸‹æ–‡ä¿¡æ¯**ï¼š
{context_info}

è¯·è®¾è®¡ä¸€ä¸ªå…·ä½“çš„ã€æœ‰åˆ›æ–°æ€§çš„ç ”ç©¶æƒ³æ³•ï¼ŒåŒ…å«ä»¥ä¸‹è¦ç´ ï¼š

1. **æ ‡é¢˜**: ç®€æ´æ˜ç¡®çš„ç ”ç©¶é¢˜ç›®
2. **æ ¸å¿ƒå‡è®¾**: ç§‘å­¦çš„ã€å¯éªŒè¯çš„å‡è®¾
3. **åˆ›æ–°ç‚¹**: 3-4ä¸ªå…·ä½“çš„æŠ€æœ¯åˆ›æ–°ç‚¹
4. **é¢„æœŸè´¡çŒ®**: å¯¹å­¦æœ¯ç•Œå’Œå®è·µçš„å…·ä½“è´¡çŒ®
5. **å®éªŒè®¾è®¡**: å…·ä½“çš„å®éªŒæ–¹æ¡ˆå’Œè¯„ä¼°æŒ‡æ ‡
6. **æ½œåœ¨é£é™©**: å¯èƒ½é‡åˆ°çš„æŠ€æœ¯æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆ

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
  "title": "ç ”ç©¶é¢˜ç›®",
  "hypothesis": "æ ¸å¿ƒç ”ç©¶å‡è®¾",
  "innovation_points": ["åˆ›æ–°ç‚¹1", "åˆ›æ–°ç‚¹2", "åˆ›æ–°ç‚¹3"],
  "contributions": ["è´¡çŒ®1", "è´¡çŒ®2"],
  "experiments": [
    {{
      "name": "å®éªŒåç§°",
      "description": "å®éªŒæè¿°", 
      "metrics": ["è¯„ä¼°æŒ‡æ ‡1", "è¯„ä¼°æŒ‡æ ‡2"],
      "datasets": ["æ•°æ®é›†1", "æ•°æ®é›†2"]
    }}
  ],
  "risks": ["é£é™©1", "é£é™©2"],
  "technical_approach": "å…·ä½“çš„æŠ€æœ¯å®ç°è·¯å¾„"
}}
"""

        try:
            response_data = await self.llm.generate(
                model_name=self.config.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=15000,
                agent_name=self.name,
                task_type="idea_generation"
            )
            response = response_data.get("content", "")
            
            # è§£æLLMå“åº”
            import json
            import re
            try:
                if "```json" in response:
                    json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
                    if json_match:
                        json_content = json_match.group(1).strip()
                    else:
                        json_start = response.find("```json") + 7
                        json_content = response[json_start:].strip()
                        if json_content.endswith("```"):
                            json_content = json_content[:-3].strip()
                else:
                    json_content = response.strip()
                
                result = json.loads(json_content)
                
                # è¡¥å……èµ„äº§ä¿¡æ¯
                result['assets'] = [
                    {'type': 'Method', 'id': method_node, 'availability': 'adaptation_required'},
                    {'type': 'Dataset', 'id': f'{task_name}_dataset', 'availability': 'to_be_identified'}
                ]
                
                print(f"    ğŸ’¡ LLMç”Ÿæˆæƒ³æ³•: {result.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
                return result
                
            except json.JSONDecodeError:
                print(f"    âš ï¸ LLMæƒ³æ³•ç”Ÿæˆå“åº”è§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ¨¡æ¿")
                # å¤‡ç”¨æ¨¡æ¿
                return self._generate_fallback_transfer_idea(method_name, task_name, method_node)
                
        except Exception as e:
            print(f"    âŒ LLMæƒ³æ³•ç”Ÿæˆå¤±è´¥: {str(e)}ï¼Œä½¿ç”¨å¤‡ç”¨æ¨¡æ¿")
            return self._generate_fallback_transfer_idea(method_name, task_name, method_node)
    
    def _generate_fallback_transfer_idea(self, method_name: str, task_name: str, method_node: str) -> Dict[str, Any]:
        """å¤‡ç”¨çš„æƒ³æ³•ç”Ÿæˆæ¨¡æ¿ï¼ˆå½“LLMè°ƒç”¨å¤±è´¥æ—¶ä½¿ç”¨ï¼‰ã€‚"""
        return {
            'title': f'å°†{method_name}è¿ç§»åº”ç”¨äº{task_name}',
            'hypothesis': f'å°†åœ¨å…¶ä»–ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚çš„{method_name}æ–¹æ³•é€‚é…åˆ°{task_name}ä»»åŠ¡ä¸­ï¼Œå¯èƒ½è·å¾—æ˜¾è‘—æ€§èƒ½æå‡',
            'innovation_points': [
                f'è·¨ä»»åŠ¡è¿ç§»{method_name}çš„æ ¸å¿ƒæœºåˆ¶',
                f'é’ˆå¯¹{task_name}çš„ç‰¹å®šé€‚é…ç­–ç•¥',
                'åˆ›æ–°çš„è¯„ä¼°ä¸å¯¹æ¯”åŸºå‡†'
            ],
            'contributions': ['æ–¹æ³•å­¦è´¡çŒ®', 'å®è¯éªŒè¯'],
            'assets': [
                {'type': 'Method', 'id': method_node, 'availability': 'adaptation_required'},
                {'type': 'Dataset', 'id': f'{task_name}_dataset', 'availability': 'to_be_identified'}
            ],
            'experiments': [
                {'name': f'{task_name}åŸºçº¿å¯¹æ¯”', 'metric': 'domain_specific_metrics', 'dataset': f'{task_name}_benchmark'}
            ],
            'risks': [
                'è·¨ä»»åŠ¡çš„æ–¹æ³•é€‚é…å¯èƒ½å­˜åœ¨å…¼å®¹æ€§é—®é¢˜',
                f'{task_name}é¢†åŸŸçš„è¯„ä¼°æ ‡å‡†å¯èƒ½éœ€è¦é‡æ–°è®¾è®¡'
            ]
        }
    
    def _collect_context_info(self, graph: SemanticOpportunityGraph, node_ids: List[str]) -> str:
        """æ”¶é›†èŠ‚ç‚¹çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ç”¨äºLLMæç¤ºã€‚"""
        context_lines = []
        
        for node_id in node_ids:
            if node_id and node_id in graph.nodes():
                node_data = graph.nodes[node_id]
                context_lines.append(f"- {node_data.get('name', node_id)}: {node_data.get('type', 'Unknown')} (é‡è¦æ€§: {node_data.get('salience', 0.0):.2f})")
                
                # æ·»åŠ ç›¸å…³è¾¹ä¿¡æ¯
                edges = list(graph.edges(node_id, data=True))
                if edges:
                    context_lines.append(f"  ç›¸å…³å…³ç³»: {len(edges)}ä¸ª")
                    for src, dst, edge_data in edges[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                        rel_type = edge_data.get('relation', 'unknown')
                        dst_name = graph.nodes[dst].get('name', dst) if dst in graph.nodes() else dst
                        context_lines.append(f"    â†’ {dst_name} ({rel_type})")
        
        return "\n".join(context_lines) if context_lines else "æ— ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯"
    
    async def _generate_composition_idea(self, template: Dict[str, Any], trigger: Dict[str, Any], 
                                       node_info: Dict[str, Any], graph: SemanticOpportunityGraph) -> Dict[str, Any]:
        """ç”Ÿæˆç»„åˆç±»å‹çš„æƒ³æ³•ã€‚"""
        trigger_nodes = trigger.get('nodes', [])
        gap_data = trigger.get('gap_data', {})
        
        method_nodes = [node for node in trigger_nodes if node_info.get(node, {}).get('type') == 'Method']
        target_task = gap_data.get('target_task')
        
        if len(method_nodes) >= 2:
            method1_name = graph.nodes[method_nodes[0]]['name'] if method_nodes[0] in graph.nodes() else "Method1"
            method2_name = graph.nodes[method_nodes[1]]['name'] if method_nodes[1] in graph.nodes() else "Method2"
        else:
            method1_name, method2_name = "Method1", "Method2"
        
        task_name = graph.nodes[target_task]['name'] if target_task and target_task in graph.nodes() else "Target Task"
        
        # æ”¶é›†ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_info = self._collect_context_info(graph, method_nodes + ([target_task] if target_task else []))
        
        # ä½¿ç”¨LLMç”Ÿæˆç»„åˆæƒ³æ³•
        prompt = f"""
åŸºäºè¯­ä¹‰æœºä¼šå›¾è°±åˆ†æï¼Œè®¾è®¡ä¸€ä¸ªåˆ›æ–°çš„ç ”ç©¶æƒ³æ³•ï¼Œå°†å¤šä¸ªæ–¹æ³•"{method1_name}"å’Œ"{method2_name}"ç»„åˆåº”ç”¨åˆ°ä»»åŠ¡"{task_name}"ä¸­ã€‚

**èƒŒæ™¯ä¿¡æ¯**ï¼š
- æ–¹æ³•1: {method1_name}
- æ–¹æ³•2: {method2_name}
- ç›®æ ‡ä»»åŠ¡: {task_name}
- å‘ç°çš„æœºä¼šgap: {gap_data.get('description', 'æ–¹æ³•ç»„åˆæœºä¼š')}

**ä¸Šä¸‹æ–‡ä¿¡æ¯**ï¼š
{context_info}

è¯·è®¾è®¡ä¸€ä¸ªå…·ä½“çš„ã€åˆ›æ–°çš„æ–¹æ³•èåˆç ”ç©¶æƒ³æ³•ï¼ŒåŒ…å«ä»¥ä¸‹è¦ç´ ï¼š

1. **æ ‡é¢˜**: ç®€æ´æ˜ç¡®çš„ç ”ç©¶é¢˜ç›®
2. **æ ¸å¿ƒå‡è®¾**: ä¸ºä»€ä¹ˆè¿™ä¸¤ä¸ªæ–¹æ³•çš„ç»„åˆä¼šæœ‰æ•ˆ
3. **åˆ›æ–°ç‚¹**: 3-4ä¸ªå…·ä½“çš„æŠ€æœ¯èåˆåˆ›æ–°ç‚¹
4. **é¢„æœŸè´¡çŒ®**: å¯¹å­¦æœ¯ç•Œå’Œå®è·µçš„å…·ä½“è´¡çŒ®
5. **å®éªŒè®¾è®¡**: å…·ä½“çš„å®éªŒæ–¹æ¡ˆï¼ŒåŒ…æ‹¬æ¶ˆèç ”ç©¶
6. **æ½œåœ¨é£é™©**: æ–¹æ³•ç»„åˆå¯èƒ½é‡åˆ°çš„æŒ‘æˆ˜

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
  "title": "ç ”ç©¶é¢˜ç›®",
  "hypothesis": "æ ¸å¿ƒç ”ç©¶å‡è®¾",
  "innovation_points": ["åˆ›æ–°ç‚¹1", "åˆ›æ–°ç‚¹2", "åˆ›æ–°ç‚¹3"],
  "contributions": ["è´¡çŒ®1", "è´¡çŒ®2"],
  "experiments": [
    {{
      "name": "å®éªŒåç§°",
      "description": "å®éªŒæè¿°", 
      "metrics": ["è¯„ä¼°æŒ‡æ ‡1", "è¯„ä¼°æŒ‡æ ‡2"],
      "datasets": ["æ•°æ®é›†1", "æ•°æ®é›†2"]
    }}
  ],
  "risks": ["é£é™©1", "é£é™©2"],
  "technical_approach": "å…·ä½“çš„èåˆæŠ€æœ¯è·¯å¾„"
}}
"""

        try:
            response_data = await self.llm.generate(
                model_name=self.config.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=15000,
                agent_name=self.name,
                task_type="idea_generation"
            )
            response = response_data.get("content", "")
            
            # è§£æLLMå“åº”
            import json
            import re
            try:
                if "```json" in response:
                    json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
                    if json_match:
                        json_content = json_match.group(1).strip()
                    else:
                        json_start = response.find("```json") + 7
                        json_content = response[json_start:].strip()
                        if json_content.endswith("```"):
                            json_content = json_content[:-3].strip()
                else:
                    json_content = response.strip()
                
                result = json.loads(json_content)
                
                # è¡¥å……èµ„äº§ä¿¡æ¯
                result['assets'] = [
                    {'type': 'Method', 'id': method_nodes[0] if method_nodes else 'method1', 'availability': 'public'},
                    {'type': 'Method', 'id': method_nodes[1] if len(method_nodes) > 1 else 'method2', 'availability': 'public'}
                ]
                
                print(f"    ğŸ’¡ LLMç”Ÿæˆç»„åˆæƒ³æ³•: {result.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
                return result
                
            except json.JSONDecodeError:
                print(f"    âš ï¸ LLMç»„åˆæƒ³æ³•ç”Ÿæˆå“åº”è§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ¨¡æ¿")
                return self._generate_fallback_composition_idea(method1_name, method2_name, task_name, method_nodes)
                
        except Exception as e:
            print(f"    âŒ LLMç»„åˆæƒ³æ³•ç”Ÿæˆå¤±è´¥: {str(e)}ï¼Œä½¿ç”¨å¤‡ç”¨æ¨¡æ¿")
            return self._generate_fallback_composition_idea(method1_name, method2_name, task_name, method_nodes)
    
    def _generate_fallback_composition_idea(self, method1_name: str, method2_name: str, task_name: str, method_nodes: List[str]) -> Dict[str, Any]:
        """å¤‡ç”¨çš„ç»„åˆæƒ³æ³•ç”Ÿæˆæ¨¡æ¿ã€‚"""
        return {
            'title': f'èåˆ{method1_name}ä¸{method2_name}æ”¹è¿›{task_name}',
            'hypothesis': f'é€šè¿‡åˆ›æ–°æ€§åœ°ç»„åˆ{method1_name}å’Œ{method2_name}çš„ä¼˜åŠ¿ï¼Œå¯ä»¥åœ¨{task_name}ä¸Šå®ç°æ€§èƒ½çªç ´',
            'innovation_points': [
                f'æ–°é¢–çš„{method1_name}ä¸{method2_name}èåˆæ¶æ„',
                'äº’è¡¥ä¼˜åŠ¿çš„ååŒæœºåˆ¶è®¾è®¡',
                'ç»Ÿä¸€çš„ç«¯åˆ°ç«¯è®­ç»ƒç­–ç•¥'
            ],
            'contributions': ['æ¶æ„åˆ›æ–°', 'æ€§èƒ½æå‡', 'æ–¹æ³•å­¦è´¡çŒ®'],
            'assets': [
                {'type': 'Method', 'id': method_nodes[0] if method_nodes else 'method1', 'availability': 'public'},
                {'type': 'Method', 'id': method_nodes[1] if len(method_nodes) > 1 else 'method2', 'availability': 'public'}
            ],
            'experiments': [
                {'name': f'{task_name}æ€§èƒ½å¯¹æ¯”', 'metric': 'accuracy,efficiency', 'dataset': f'{task_name}_standard_benchmark'},
                {'name': 'æ¶ˆèç ”ç©¶', 'metric': 'component_contribution', 'dataset': 'same_as_main'}
            ],
            'risks': [
                'æ–¹æ³•ç»„åˆå¯èƒ½å¢åŠ è®¡ç®—å¤æ‚åº¦',
                'ä¸åŒæ–¹æ³•çš„è®­ç»ƒç­–ç•¥å¯èƒ½å­˜åœ¨å†²çª',
                'é›†æˆæ•ˆæœçš„å¯è§£é‡Šæ€§å¯èƒ½é™ä½'
            ]
        }
    
    async def _generate_reverse_idea(self, template: Dict[str, Any], trigger: Dict[str, Any], 
                                   node_info: Dict[str, Any], graph: SemanticOpportunityGraph) -> Dict[str, Any]:
        """ç”Ÿæˆåè½¬ç±»å‹çš„æƒ³æ³•ï¼ˆé’ˆå¯¹æ‰¹è¯„çš„æ”¹è¿›ï¼‰ã€‚"""
        gap_data = trigger.get('gap_data', {})
        critiqued_method = gap_data.get('critiqued_method')
        
        method_name = graph.nodes[critiqued_method]['name'] if critiqued_method and critiqued_method in graph.nodes() else "Target Method"
        
        return {
            'title': f'é’ˆå¯¹{method_name}ç¼ºé™·çš„æ”¹è¿›æ–¹æ¡ˆ',
            'hypothesis': f'é€šè¿‡åˆ†æ{method_name}çš„å·²çŸ¥å±€é™æ€§ï¼Œè®¾è®¡é’ˆå¯¹æ€§çš„æ”¹è¿›ç­–ç•¥ï¼Œå®ç°æ€§èƒ½å’Œæ•ˆç‡çš„åŒé‡æå‡',
            'innovation_points': [
                f'è¯†åˆ«å¹¶è§£å†³{method_name}çš„æ ¸å¿ƒç“¶é¢ˆ',
                'åˆ›æ–°çš„ä¼˜åŒ–ç®—æ³•æˆ–æ¶æ„æ”¹è¿›',
                'ä¿æŒåŸæœ‰ä¼˜åŠ¿çš„åŒæ—¶æ¶ˆé™¤åŠ£åŠ¿'
            ],
            'contributions': ['æ–¹æ³•æ”¹è¿›', 'ç†è®ºåˆ†æ', 'å®è¯éªŒè¯'],
            'assets': [
                {'type': 'Method', 'id': critiqued_method, 'availability': 'baseline_reference'},
                {'type': 'Analysis', 'id': 'critique_analysis', 'availability': 'literature_based'}
            ],
            'experiments': [
                {'name': 'æ”¹è¿›æ•ˆæœéªŒè¯', 'metric': 'performance_improvement', 'dataset': 'original_benchmark'},
                {'name': 'æ•ˆç‡åˆ†æ', 'metric': 'computational_efficiency', 'dataset': 'efficiency_benchmark'}
            ],
            'risks': [
                'æ”¹è¿›å¯èƒ½å¼•å…¥æ–°çš„é—®é¢˜',
                'å¯¹åŸæœ‰ä¼˜åŠ¿çš„ä¿æŒå­˜åœ¨ä¸ç¡®å®šæ€§',
                'æ”¹è¿›çš„æ³›åŒ–èƒ½åŠ›éœ€è¦éªŒè¯'
            ]
        }
    
    async def _generate_generic_idea(self, template: Dict[str, Any], trigger: Dict[str, Any], 
                                   node_info: Dict[str, Any], graph: SemanticOpportunityGraph) -> Dict[str, Any]:
        """ç”Ÿæˆé€šç”¨ç±»å‹çš„æƒ³æ³•ã€‚"""
        trigger_nodes = trigger.get('nodes', [])
        pattern = trigger.get('pattern', 'unknown')
        gap_data = trigger.get('gap_data', {})
        
        node_names = [node_info.get(node, {}).get('name', node) for node in trigger_nodes]
        context_info = self._collect_context_info(graph, trigger_nodes)
        
        # ä½¿ç”¨LLMç”Ÿæˆé€šç”¨æƒ³æ³•
        prompt = f"""
åŸºäºè¯­ä¹‰æœºä¼šå›¾è°±åˆ†æï¼Œå‘ç°äº†ä¸€ä¸ª"{pattern}"ç±»å‹çš„ç ”ç©¶æœºä¼šï¼Œæ¶‰åŠä»¥ä¸‹å®ä½“ï¼š{", ".join(node_names)}ã€‚

**èƒŒæ™¯ä¿¡æ¯**ï¼š
- æ¨¡å¼ç±»å‹: {pattern}
- ç›¸å…³å®ä½“: {", ".join(node_names)}
- æœºä¼šæè¿°: {gap_data.get('description', 'å‘ç°çš„ç ”ç©¶æœºä¼š')}

**ä¸Šä¸‹æ–‡ä¿¡æ¯**ï¼š
{context_info}

è¯·è®¾è®¡ä¸€ä¸ªå…·ä½“çš„ã€åˆ›æ–°çš„ç ”ç©¶æƒ³æ³•ï¼ŒåŒ…å«ä»¥ä¸‹è¦ç´ ï¼š

1. **æ ‡é¢˜**: ç®€æ´æ˜ç¡®çš„ç ”ç©¶é¢˜ç›®
2. **æ ¸å¿ƒå‡è®¾**: åŸºäºå‘ç°çš„æœºä¼šæå‡ºçš„ç ”ç©¶å‡è®¾
3. **åˆ›æ–°ç‚¹**: 3-4ä¸ªå…·ä½“çš„æŠ€æœ¯æˆ–æ–¹æ³•åˆ›æ–°ç‚¹
4. **é¢„æœŸè´¡çŒ®**: å¯¹å­¦æœ¯ç•Œå’Œå®è·µçš„å…·ä½“è´¡çŒ®
5. **å®éªŒè®¾è®¡**: å…·ä½“çš„éªŒè¯æ–¹æ¡ˆå’Œè¯„ä¼°æ–¹æ³•
6. **æ½œåœ¨é£é™©**: å¯èƒ½é‡åˆ°çš„æŒ‘æˆ˜å’Œè§£å†³æ€è·¯

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
  "title": "ç ”ç©¶é¢˜ç›®",
  "hypothesis": "æ ¸å¿ƒç ”ç©¶å‡è®¾",
  "innovation_points": ["åˆ›æ–°ç‚¹1", "åˆ›æ–°ç‚¹2", "åˆ›æ–°ç‚¹3"],
  "contributions": ["è´¡çŒ®1", "è´¡çŒ®2"],
  "experiments": [
    {{
      "name": "å®éªŒåç§°",
      "description": "å®éªŒæè¿°", 
      "metrics": ["è¯„ä¼°æŒ‡æ ‡1", "è¯„ä¼°æŒ‡æ ‡2"],
      "datasets": ["æ•°æ®é›†1", "æ•°æ®é›†2"]
    }}
  ],
  "risks": ["é£é™©1", "é£é™©2"],
  "technical_approach": "å…·ä½“çš„æŠ€æœ¯å®ç°è·¯å¾„"
}}
"""

        try:
            response_data = await self.llm.generate(
                model_name=self.config.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.8,  # é€šç”¨æƒ³æ³•å¯ä»¥æ›´æœ‰åˆ›é€ æ€§
                max_tokens=15000,
                agent_name=self.name,
                task_type="idea_generation"
            )
            response = response_data.get("content", "")
            
            # è§£æLLMå“åº”
            import json
            import re
            try:
                if "```json" in response:
                    json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
                    if json_match:
                        json_content = json_match.group(1).strip()
                    else:
                        json_start = response.find("```json") + 7
                        json_content = response[json_start:].strip()
                        if json_content.endswith("```"):
                            json_content = json_content[:-3].strip()
                else:
                    json_content = response.strip()
                
                result = json.loads(json_content)
                
                # è¡¥å……èµ„äº§ä¿¡æ¯
                result['assets'] = [
                    {'type': 'Unknown', 'id': 'to_be_determined', 'availability': 'unknown'}
                ]
                
                print(f"    ğŸ’¡ LLMç”Ÿæˆé€šç”¨æƒ³æ³•: {result.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
                return result
                
            except json.JSONDecodeError:
                print(f"    âš ï¸ LLMé€šç”¨æƒ³æ³•ç”Ÿæˆå“åº”è§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ¨¡æ¿")
                return self._generate_fallback_generic_idea(pattern, node_names)
                
        except Exception as e:
            print(f"    âŒ LLMé€šç”¨æƒ³æ³•ç”Ÿæˆå¤±è´¥: {str(e)}ï¼Œä½¿ç”¨å¤‡ç”¨æ¨¡æ¿")
            return self._generate_fallback_generic_idea(pattern, node_names)
    
    def _generate_fallback_generic_idea(self, pattern: str, node_names: List[str]) -> Dict[str, Any]:
        """å¤‡ç”¨çš„é€šç”¨æƒ³æ³•ç”Ÿæˆæ¨¡æ¿ã€‚"""
        return {
            'title': f'åŸºäº{pattern}æ¨¡å¼çš„åˆ›æ–°ç ”ç©¶',
            'hypothesis': f'é€šè¿‡åˆ†æ{", ".join(node_names)}ä¹‹é—´çš„å…³ç³»ï¼Œæ¢ç´¢æ–°çš„ç ”ç©¶æœºä¼š',
            'innovation_points': [
                f'æ–°é¢–çš„{pattern}åº”ç”¨',
                'è·¨é¢†åŸŸçš„æ–¹æ³•è¿ç§»',
                'åˆ›æ–°çš„è¯„ä¼°æ¡†æ¶'
            ],
            'contributions': ['æ¢ç´¢æ€§ç ”ç©¶', 'æ–¹æ³•å­¦è´¡çŒ®'],
            'assets': [
                {'type': 'Unknown', 'id': 'to_be_determined', 'availability': 'unknown'}
            ],
            'experiments': [
                {'name': 'å¯è¡Œæ€§éªŒè¯', 'metric': 'proof_of_concept', 'dataset': 'pilot_study'}
            ],
            'risks': [
                'ç ”ç©¶æ–¹å‘çš„ä¸ç¡®å®šæ€§',
                'é¢„æœŸæ•ˆæœçš„ä¸å¯é¢„æµ‹æ€§'
            ]
        }

    async def _rank_and_filter(self, candidates: List[CandidateIdea], max_ideas: int) -> List[CandidateIdea]:
        """å¯¹å€™é€‰æƒ³æ³•è¿›è¡Œæ’åºä¸è¿‡æ»¤ã€‚
        
        è¾“å…¥:
            - candidates: åŸå§‹å€™é€‰æƒ³æ³•åˆ—è¡¨ã€‚
            - max_ideas: è¿”å›æ•°é‡ä¸Šé™ã€‚
            
        è¾“å‡º:
            - List[CandidateIdea]: æ’åºåçš„top-kæƒ³æ³•ã€‚
            
        å®ç°æ€è·¯:
            1) å»é‡ï¼šåŸºäº core_hypothesis ç›¸ä¼¼åº¦æ£€æµ‹é‡å¤æƒ³æ³•ã€‚
            2) é¢„è¯„ä¼°ï¼šæ ¹æ®è§¦å‘ç½®ä¿¡åº¦ã€èŠ‚ç‚¹é‡è¦æ€§ã€é£é™©è¯„ä¼°ç­‰è®¡ç®—åˆæ­¥å¾—åˆ†ã€‚
            3) æ’åºè¿”å›top-kã€‚
        """
        if not candidates:
            return []
        
        print(f"    ğŸ“Š å¯¹ {len(candidates)} ä¸ªå€™é€‰æƒ³æ³•è¿›è¡Œæ’åºä¸è¿‡æ»¤")
        
        # æ­¥éª¤1ï¼šå»é‡
        unique_candidates = await self._deduplicate_ideas(candidates)
        print(f"    ğŸ”„ å»é‡åä¿ç•™ {len(unique_candidates)} ä¸ªæƒ³æ³•")
        
        # æ­¥éª¤2ï¼šé¢„è¯„ä¼°
        scored_candidates = []
        for candidate in unique_candidates:
            score = await self._calculate_preliminary_score(candidate)
            scored_candidates.append((candidate, score))
        
        # æ­¥éª¤3ï¼šæ’åº
        scored_candidates.sort(key=lambda x: x[1], reverse=True)
        
        # æ­¥éª¤4ï¼šè¿”å›top-k
        final_candidates = [candidate for candidate, score in scored_candidates[:max_ideas]]
        
        print(f"    âœ… æœ€ç»ˆä¿ç•™ {len(final_candidates)} ä¸ªé«˜è´¨é‡æƒ³æ³•")
        return final_candidates
    
    async def _deduplicate_ideas(self, candidates: List[CandidateIdea]) -> List[CandidateIdea]:
        """å¯¹æƒ³æ³•è¿›è¡Œå»é‡ã€‚"""
        if len(candidates) <= 1:
            return candidates
        
        unique_candidates = []
        seen_hypotheses = set()
        
        for candidate in candidates:
            # ç®€åŒ–çš„å»é‡ï¼šåŸºäºæ ¸å¿ƒå‡è®¾çš„å…³é”®è¯é‡å 
            hypothesis_keywords = set(candidate.core_hypothesis.lower().split())
            
            # æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰æƒ³æ³•é‡å¤
            is_duplicate = False
            for seen_hypothesis in seen_hypotheses:
                seen_keywords = set(seen_hypothesis.split())
                overlap = len(hypothesis_keywords & seen_keywords)
                
                # å¦‚æœé‡å åº¦è¶…è¿‡50%ï¼Œè®¤ä¸ºæ˜¯é‡å¤
                if overlap > 0 and overlap / len(hypothesis_keywords | seen_keywords) > 0.5:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_candidates.append(candidate)
                seen_hypotheses.add(candidate.core_hypothesis.lower())
        
        return unique_candidates
    
    async def _calculate_preliminary_score(self, candidate: CandidateIdea) -> float:
        """è®¡ç®—æƒ³æ³•çš„åˆæ­¥å¾—åˆ†ã€‚"""
        score = 0.0
        
        # å› å­1ï¼šè§¦å‘ç½®ä¿¡åº¦ (30%)
        trigger_confidence = candidate.provenance.get('confidence', 0.5)
        score += trigger_confidence * 0.3
        
        # å› å­2ï¼šåˆ›æ–°ç‚¹æ•°é‡å’Œè´¨é‡ (25%)
        innovation_count = len(candidate.initial_innovation_points)
        innovation_score = min(innovation_count / 3.0, 1.0)  # æœ€å¤š3ä¸ªåˆ›æ–°ç‚¹å¾—æ»¡åˆ†
        score += innovation_score * 0.25
        
        # å› å­3ï¼šé¢„æœŸè´¡çŒ®å¤šæ ·æ€§ (20%)
        contribution_count = len(candidate.expected_contribution)
        contribution_score = min(contribution_count / 2.0, 1.0)  # æœ€å¤š2ä¸ªè´¡çŒ®å¾—æ»¡åˆ†
        score += contribution_score * 0.20
        
        # å› å­4ï¼šé£é™©è¯„ä¼° (15%) - é£é™©è¶Šå°‘å¾—åˆ†è¶Šé«˜
        risk_count = len(candidate.risks)
        risk_penalty = min(risk_count / 5.0, 1.0)  # æœ€å¤š5ä¸ªé£é™©å…¨æ‰£åˆ†
        score += (1.0 - risk_penalty) * 0.15
        
        # å› å­5ï¼šå®éªŒè®¾è®¡å®Œæ•´æ€§ (10%)
        experiment_count = len(candidate.preliminary_experiments)
        experiment_score = min(experiment_count / 2.0, 1.0)  # æœ€å¤š2ä¸ªå®éªŒå¾—æ»¡åˆ†
        score += experiment_score * 0.10
        
        return min(score, 1.0)  # ç¡®ä¿å¾—åˆ†ä¸è¶…è¿‡1.0

    async def refine_idea(self, idea: CandidateIdea, refinement_prompt: RefinementPrompt, graph: SemanticOpportunityGraph) -> CandidateIdea:
        """æ ¹æ®ç²¾ç‚¼æŒ‡ä»¤ç”Ÿæˆæƒ³æ³•çš„æ–°ç‰ˆæœ¬ã€‚
        
        è¾“å…¥:
            - idea: å½“å‰æƒ³æ³•ç‰ˆæœ¬ã€‚
            - refinement_prompt: ç²¾ç‚¼æŒ‡ä»¤ã€‚
            - graph: è¯­ä¹‰æœºä¼šå›¾è°±ã€‚
            
        è¾“å‡º:
            - CandidateIdea: ç²¾ç‚¼åçš„æ–°ç‰ˆæœ¬æƒ³æ³•ï¼ˆversion+1ï¼‰ã€‚
            
        å®ç°æ€è·¯:
            1) è§£æ refinement_prompt.instructionsï¼Œè¯†åˆ«ä¿ç•™/æ›¿æ¢/è¡¥å……çš„å…·ä½“é¡¹ç›®ã€‚
            2) è°ƒç”¨ LLM é‡æ–°ç”Ÿæˆå¯¹åº”å­—æ®µï¼Œä¿æŒå…¶ä»–å­—æ®µä¸å˜æˆ–å±€éƒ¨è°ƒæ•´ã€‚
            3) æ›´æ–° versionã€provenanceï¼Œè®°å½•å˜æ›´è½¨è¿¹ã€‚
        """
        print(f"    ğŸ”„ ç²¾ç‚¼æƒ³æ³• {idea.id} (å½“å‰ç‰ˆæœ¬: {idea.version})")
        
        # åˆ›å»ºæ–°ç‰ˆæœ¬çš„æƒ³æ³•ï¼ŒåŸºäºåŸæƒ³æ³•
        refined_idea = CandidateIdea(
            id=idea.id,
            title=idea.title,
            core_hypothesis=idea.core_hypothesis,
            initial_innovation_points=idea.initial_innovation_points.copy(),
            source_trigger_nodes=idea.source_trigger_nodes.copy(),
            expected_contribution=idea.expected_contribution.copy(),
            required_assets=idea.required_assets.copy(),
            preliminary_experiments=idea.preliminary_experiments.copy(),
            risks=idea.risks.copy(),
            provenance=idea.provenance.copy(),
            version=idea.version + 1
        )
        
        # è§£æç²¾ç‚¼æŒ‡ä»¤å¹¶åº”ç”¨ä¿®æ”¹
        instructions = refinement_prompt.instructions
        
        for instruction in instructions:
            await self._apply_refinement_instruction(refined_idea, instruction, graph)
        
        # æ›´æ–°provenanceè®°å½•å˜æ›´è½¨è¿¹
        refined_idea.provenance.update({
            "refined_at": datetime.now().isoformat(),
            "refinement_reason": refinement_prompt.rationale,
            "previous_version": idea.version,
            "applied_instructions": instructions
        })
        
        print(f"    âœ¨ å®Œæˆæƒ³æ³•ç²¾ç‚¼ï¼Œæ–°ç‰ˆæœ¬: {refined_idea.version}")
        return refined_idea
    
    async def _apply_refinement_instruction(self, idea: CandidateIdea, instruction: str, graph: SemanticOpportunityGraph) -> None:
        """åº”ç”¨å•ä¸ªç²¾ç‚¼æŒ‡ä»¤ã€‚"""
        instruction_lower = instruction.lower()
        
        # æŒ‡ä»¤ç±»å‹1ï¼šæ›¿æ¢èµ„äº§
        if "æ›¿æ¢" in instruction and ("èµ„äº§" in instruction or "dataset" in instruction_lower or "method" in instruction_lower):
            await self._refine_assets(idea, instruction, graph)
        
        # æŒ‡ä»¤ç±»å‹2ï¼šå¼ºåŒ–å·®å¼‚ç‚¹
        elif "å¼ºåŒ–" in instruction and "å·®å¼‚" in instruction:
            await self._enhance_innovation_points(idea, instruction)
        
        # æŒ‡ä»¤ç±»å‹3ï¼šç¼“è§£é£é™©
        elif "ç¼“è§£" in instruction and "é£é™©" in instruction:
            await self._mitigate_risks(idea, instruction)
        
        # æŒ‡ä»¤ç±»å‹4ï¼šæ”¹è¿›æ ‡é¢˜æˆ–å‡è®¾
        elif "æ”¹è¿›" in instruction and ("æ ‡é¢˜" in instruction or "å‡è®¾" in instruction):
            await self._improve_core_content(idea, instruction)
        
        # æŒ‡ä»¤ç±»å‹5ï¼šè¡¥å……å®éªŒ
        elif "è¡¥å……" in instruction and "å®éªŒ" in instruction:
            await self._supplement_experiments(idea, instruction)
        
        else:
            # é€šç”¨æŒ‡ä»¤å¤„ç†
            await self._apply_generic_instruction(idea, instruction)
    
    async def _refine_assets(self, idea: CandidateIdea, instruction: str, graph: SemanticOpportunityGraph) -> None:
        """ç²¾ç‚¼æ‰€éœ€èµ„äº§ã€‚"""
        # ç®€åŒ–å®ç°ï¼šæ·»åŠ æ›¿ä»£èµ„äº§
        if "dataset" in instruction.lower():
            # æ›¿æ¢æ•°æ®é›†
            new_asset = {
                'type': 'Dataset',
                'id': 'alternative_dataset',
                'availability': 'public'
            }
            idea.required_assets.append(new_asset)
        
        elif "method" in instruction.lower():
            # æ›¿æ¢æ–¹æ³•
            new_asset = {
                'type': 'Method',
                'id': 'alternative_method',
                'availability': 'open_source'
            }
            idea.required_assets.append(new_asset)
    
    async def _enhance_innovation_points(self, idea: CandidateIdea, instruction: str) -> None:
        """å¼ºåŒ–åˆ›æ–°ç‚¹ã€‚"""
        # æ·»åŠ å·®å¼‚åŒ–çš„åˆ›æ–°ç‚¹
        enhanced_points = [
            "å¼•å…¥æ–°é¢–çš„æŠ€æœ¯æœºåˆ¶",
            "å»ºç«‹åˆ›æ–°çš„è¯„ä¼°æ¡†æ¶",
            "æå‡ºç‹¬ç‰¹çš„ç†è®ºåˆ†æè§†è§’"
        ]
        
        # é¿å…é‡å¤æ·»åŠ 
        for point in enhanced_points:
            if point not in idea.initial_innovation_points:
                idea.initial_innovation_points.append(point)
                break  # åªæ·»åŠ ä¸€ä¸ªæ–°çš„åˆ›æ–°ç‚¹
    
    async def _mitigate_risks(self, idea: CandidateIdea, instruction: str) -> None:
        """ç¼“è§£é£é™©ã€‚"""
        # ç®€åŒ–å®ç°ï¼šä¸ºç°æœ‰é£é™©æ·»åŠ ç¼“è§£ç­–ç•¥
        risk_mitigations = [
            "é€šè¿‡æ¸è¿›å¼å®éªŒéªŒè¯å‡å°‘ä¸ç¡®å®šæ€§",
            "å»ºç«‹å¤šé‡éªŒè¯æœºåˆ¶ç¡®ä¿ç»“æœå¯é æ€§",
            "è®¾è®¡å¤‡é€‰æ–¹æ¡ˆåº”å¯¹æ½œåœ¨å¤±è´¥"
        ]
        
        # å°†ç¼“è§£ç­–ç•¥æ·»åŠ åˆ°åˆ›æ–°ç‚¹æˆ–è€…ä¿®æ”¹é£é™©æè¿°
        if risk_mitigations[0] not in idea.initial_innovation_points:
            idea.initial_innovation_points.append(risk_mitigations[0])
    
    async def _improve_core_content(self, idea: CandidateIdea, instruction: str) -> None:
        """æ”¹è¿›æ ¸å¿ƒå†…å®¹ã€‚"""
        if "æ ‡é¢˜" in instruction:
            # æ”¹è¿›æ ‡é¢˜ï¼Œä½¿å…¶æ›´å…·ä½“å’Œå¸å¼•åŠ›
            if "åŸºäº" not in idea.title:
                idea.title = f"åŸºäºåˆ›æ–°æ–¹æ³•çš„{idea.title}"
        
        elif "å‡è®¾" in instruction:
            # æ”¹è¿›æ ¸å¿ƒå‡è®¾ï¼Œä½¿å…¶æ›´æ˜ç¡®
            if "é¢„æœŸ" not in idea.core_hypothesis:
                idea.core_hypothesis += "ï¼Œé¢„æœŸèƒ½å¤Ÿå–å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡å’Œç†è®ºè´¡çŒ®"
    
    async def _supplement_experiments(self, idea: CandidateIdea, instruction: str) -> None:
        """è¡¥å……å®éªŒè®¾è®¡ã€‚"""
        # æ·»åŠ æ›´å…¨é¢çš„å®éªŒè®¾è®¡
        additional_experiments = [
            {'name': 'æ¶ˆèç ”ç©¶', 'metric': 'component_analysis', 'dataset': 'validation_set'},
            {'name': 'æ³›åŒ–èƒ½åŠ›æµ‹è¯•', 'metric': 'cross_domain_performance', 'dataset': 'diverse_testset'},
            {'name': 'æ•ˆç‡å¯¹æ¯”åˆ†æ', 'metric': 'computational_efficiency', 'dataset': 'benchmark_suite'}
        ]
        
        # é¿å…é‡å¤æ·»åŠ ç›¸åŒçš„å®éªŒ
        existing_names = {exp.get('name', '') for exp in idea.preliminary_experiments}
        for exp in additional_experiments:
            if exp['name'] not in existing_names:
                idea.preliminary_experiments.append(exp)
                break  # åªæ·»åŠ ä¸€ä¸ªæ–°å®éªŒ
    
    async def _apply_generic_instruction(self, idea: CandidateIdea, instruction: str) -> None:
        """åº”ç”¨é€šç”¨æŒ‡ä»¤ã€‚"""
        # é€šç”¨æ”¹è¿›ï¼šå¢åŠ ä¸€ä¸ªç»¼åˆæ€§çš„åˆ›æ–°ç‚¹
        generic_improvement = "ç»“åˆå¤šç§å…ˆè¿›æŠ€æœ¯å®ç°ç»¼åˆæ€§èƒ½æå‡"
        
        if generic_improvement not in idea.initial_innovation_points:
            idea.initial_innovation_points.append(generic_improvement)


class NoveltyCriticAgent(BaseIdeaAgent):
    """ç¬¬ä¸‰é˜¶æ®µï¼šæ–°é¢–æ€§æ‰¹åˆ¤ï¼ˆå¹¶è¡Œä¹‹ä¸€ï¼‰ã€‚

    æ ¸å¿ƒèŒè´£:
        - é‡‡ç”¨ RAG ä¸¤é˜¶æ®µ(å¬å›â†’é‡æ’)å¯¹ `CandidateIdea` è¿›è¡Œå¤šåˆ†é¢æ–°é¢–æ€§è¯„å®¡ã€‚

    æ³¨æ„äº‹é¡¹:
        - å¬å›æ„é€ éœ€èåˆ idea.title/core_hypothesis/trigger_nodes çš„å…³é”®è¯åŠåˆ«åã€‚
        - é‡æ’æç¤ºéœ€è¦†ç›–"æ¦‚å¿µ/æ–¹æ³•/åº”ç”¨/è¯„æµ‹/è¯æ®"åˆ†é¢ï¼Œå¹¶äº§å‡ºå·®å¼‚æ€§ä¸»å¼  `difference_claims`ã€‚
    """

    def __init__(self, name: str, llm_factory: LLMFactory, db: AcademicPaperDatabase, config: Optional[AgentConfig] = None):
        super().__init__(name, llm_factory, db, config)
        self.novelty_facets = ["conceptual", "methodological", "application", "evaluation"]

    async def assess_novelty(self, idea: CandidateIdea, retrieve_k: int = 30) -> NoveltyCritique:
        """è¯„ä¼°å•ä¸ªæƒ³æ³•çš„æ–°é¢–æ€§ã€‚"""
        results = await self.assess_novelty_batch([idea], retrieve_k)
        return results[0]
    
    async def assess_novelty_batch(self, ideas: List[CandidateIdea], retrieve_k: int = 30) -> List[NoveltyCritique]:
        """æ‰¹é‡è¯„ä¼°æƒ³æ³•çš„æ–°é¢–æ€§ã€‚"""
        import asyncio
        
        print(f"ğŸ”¬ å¼€å§‹æ‰¹é‡æ–°é¢–æ€§è¯„ä¼°ï¼š{len(ideas)}ä¸ªæƒ³æ³•")
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = []
        for idea in ideas:
            task = asyncio.create_task(self._assess_single_novelty(idea, retrieve_k))
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è¿‡æ»¤æˆåŠŸçš„ç»“æœ
        critiques = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"âš ï¸ æƒ³æ³•{i}æ–°é¢–æ€§è¯„ä¼°å¤±è´¥: {str(result)}")
                # åˆ›å»ºé»˜è®¤çš„critique
                critiques.append(NoveltyCritique(
                    idea_id=ideas[i].id,
                    novelty_score=5.0,
                    facet_scores={"æ¦‚å¿µ": 5.0, "æ–¹æ³•": 5.0, "åº”ç”¨": 5.0, "è¯„æµ‹": 5.0},
                    similar_works=[],
                    difference_claims=[],
                    method={"error": f"è¯„ä¼°å¤±è´¥: {str(result)}"}
                ))
            else:
                critiques.append(result)
        
        return critiques
    
    async def assess_batch_comprehensive(self, ideas: List[CandidateIdea], graph: SemanticOpportunityGraph = None, retrieve_k: int = 30) -> List[NoveltyCritique]:
        """ä¸€æ¬¡LLMè°ƒç”¨å¯¹æ•´æ‰¹æƒ³æ³•è¿›è¡Œç»¼åˆæ–°é¢–æ€§è¯„ä¼°ã€‚"""
        print(f"ğŸ”¬ å¼€å§‹æ‰¹é‡ç»¼åˆæ–°é¢–æ€§è¯„ä¼°ï¼š{len(ideas)}ä¸ªæƒ³æ³•")
        
        # ä¸ºæ•´æ‰¹æƒ³æ³•æ„å»ºç»¼åˆè¯„ä¼°prompt
        ideas_summary = "\n".join([
            f"{i+1}. {idea.title}\n   æ ¸å¿ƒå‡è®¾: {idea.core_hypothesis}\n   åˆ›æ–°ç‚¹: {', '.join(idea.initial_innovation_points)}"
            for i, idea in enumerate(ideas)
        ])
        
        prompt = f"""
ä½œä¸ºèµ„æ·±å­¦æœ¯ä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹{len(ideas)}ä¸ªç ”ç©¶æƒ³æ³•è¿›è¡Œç»¼åˆæ–°é¢–æ€§è¯„ä¼°ã€‚

**å¾…è¯„ä¼°æƒ³æ³•åˆ—è¡¨**ï¼š
{ideas_summary}

**è¯„ä¼°ä»»åŠ¡**ï¼š
1. å¯¹æ¯ä¸ªæƒ³æ³•ä»å¤šä¸ªç»´åº¦è¯„ä¼°æ–°é¢–æ€§ï¼ˆæ¦‚å¿µã€æ–¹æ³•ã€åº”ç”¨ã€è¯„æµ‹ï¼‰
2. è¯†åˆ«æƒ³æ³•é—´çš„ç›¸äº’å…³ç³»å’Œå·®å¼‚åŒ–ç¨‹åº¦
3. ç»™å‡ºæ¯ä¸ªæƒ³æ³•çš„æ–°é¢–æ€§åˆ†æ•°(1-10åˆ†)
4. æä¾›è¯„ä¼°ç†ç”±å’Œå»ºè®®

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
  "batch_analysis": "å¯¹æ•´æ‰¹æƒ³æ³•çš„ç»¼åˆåˆ†æ",
  "novelty_assessments": [
    {{
      "idea_index": 1,
      "novelty_score": X.X,
      "facet_scores": {{
        "conceptual": X.X,
        "methodological": X.X, 
        "application": X.X,
        "evaluation": X.X
      }},
      "reasoning": "è¯¦ç»†è¯„ä¼°ç†ç”±",
      "strengths": ["ä¼˜åŠ¿1", "ä¼˜åŠ¿2"],
      "concerns": ["å…³æ³¨ç‚¹1", "å…³æ³¨ç‚¹2"],
      "differentiation": "ä¸å…¶ä»–æƒ³æ³•çš„å·®å¼‚åŒ–ç¨‹åº¦"
    }}
  ]
}}
"""
        
        try:
            response_data = await self.llm.generate(
                model_name=self.config.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=15000,
                agent_name=self.name,
                task_type="batch_novelty_assessment"
            )
            response = response_data.get("content", "")
            
            # è§£æJSONå“åº”
            import json
            import re
            json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
            if json_match:
                json_content = json_match.group(1).strip()
            else:
                json_content = response.strip()
            
            result = json.loads(json_content)
            assessments = result.get("novelty_assessments", [])
            
            # ä¸ºæ¯ä¸ªæƒ³æ³•åˆ›å»ºNoveltyCritiqueå¯¹è±¡
            critiques = []
            for i, idea in enumerate(ideas):
                if i < len(assessments):
                    assessment = assessments[i]
                    critique = NoveltyCritique(
                        idea_id=idea.id,
                        novelty_score=assessment.get("novelty_score", 5.0),
                        facet_scores=assessment.get("facet_scores", {"æ¦‚å¿µ": 5.0, "æ–¹æ³•": 5.0, "åº”ç”¨": 5.0, "è¯„æµ‹": 5.0}),
                        similar_works=[],
                        difference_claims=[assessment.get("differentiation", "é€‚åº¦åˆ›æ–°")],
                        method={"type": "batch_comprehensive", "reasoning": assessment.get("reasoning", "")}
                    )
                else:
                    # é»˜è®¤è¯„ä¼°
                    critique = NoveltyCritique(
                        idea_id=idea.id,
                        novelty_score=5.0,
                        facet_scores={"æ¦‚å¿µ": 5.0, "æ–¹æ³•": 5.0, "åº”ç”¨": 5.0, "è¯„æµ‹": 5.0},
                        similar_works=[],
                        difference_claims=["å¾…è¿›ä¸€æ­¥è¯„ä¼°"],
                        method={"type": "batch_comprehensive", "error": "æ‰¹é‡è¯„ä¼°è§£æå¤±è´¥"}
                    )
                critiques.append(critique)
            
            print(f"âœ… æ‰¹é‡ç»¼åˆæ–°é¢–æ€§è¯„ä¼°å®Œæˆ")
            return critiques
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡ç»¼åˆæ–°é¢–æ€§è¯„ä¼°å¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤è¯„ä¼°
            return [NoveltyCritique(
                idea_id=idea.id,
                novelty_score=5.0,
                facet_scores={"æ¦‚å¿µ": 5.0, "æ–¹æ³•": 5.0, "åº”ç”¨": 5.0, "è¯„æµ‹": 5.0},
                similar_works=[],
                difference_claims=["è¯„ä¼°å¤±è´¥"],
                method={"type": "batch_comprehensive", "error": str(e)}
            ) for idea in ideas]
    
    async def _assess_single_novelty(self, idea: CandidateIdea, retrieve_k: int = 30) -> NoveltyCritique:
        """å¯¹å•ä¸ªæƒ³æ³•è¿›è¡Œæ–°é¢–æ€§è¯„å®¡ã€‚

        è¾“å…¥:
            - idea: å€™é€‰æƒ³æ³•ã€‚
            - retrieve_k: RAGå¬å›é‡ï¼ˆé»˜è®¤30ï¼‰ã€‚

        è¾“å‡º:
            - NoveltyCritique: åŒ…å«åˆ†é¢å¾—åˆ†ã€ç›¸ä¼¼å·¥ä½œæ¸…å•ã€å·®å¼‚ä¸»å¼ ä¸æ–¹æ³•è¯´æ˜ã€‚

        å®ç°æ­¥éª¤å»ºè®®:
            1) æ„é€ å¤šæ ·åŒ–æŸ¥è¯¢ï¼ˆä¸»æœ¯è¯­+åˆ«å+è§¦å‘èŠ‚ç‚¹+ä»»åŠ¡/æŒ‡æ ‡ï¼‰ã€‚
            2) `db.search_content` å¬å›å¹¶å»é‡ï¼›æŒ‰åˆ†é¢ç”¨ LLM é‡æ’ä¸å¯¹æ¯”ã€‚
            3) æ±‡æ€» facet_scores/novelty_scoreï¼Œç”Ÿæˆ difference_claims ä¸ similar_worksã€‚
        """
        # é˜¶æ®µ1ï¼šå¬å›ç›¸å…³æ–‡çŒ®
        retrieved_papers = await self._retrieve_similar_works(idea, retrieve_k)
        
        # é˜¶æ®µ2ï¼šå¤šåˆ†é¢é‡æ’ä¸å¯¹æ¯”
        facet_results = await self._rerank_by_facets(idea, retrieved_papers)
        
        # é˜¶æ®µ3ï¼šç»¼åˆè¯„ä¼°ä¸å·®å¼‚æ€§åˆ†æ
        return await self._synthesize_novelty_critique(idea, facet_results)

    async def _retrieve_similar_works(self, idea: CandidateIdea, k: int) -> List[Dict[str, Any]]:
        """RAGç¬¬ä¸€é˜¶æ®µï¼šå¬å›ç›¸ä¼¼å·¥ä½œã€‚
        
        è¾“å…¥:
            - idea: å€™é€‰æƒ³æ³•ã€‚
            - k: å¬å›æ•°é‡ã€‚
            
        è¾“å‡º:
            - List[Dict]: å¬å›çš„æ–‡çŒ®åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« paper_idã€contentã€metadataã€‚
            
        å®ç°æ€è·¯:
            1) ä» idea.title/core_hypothesis/source_trigger_nodes æŠ½å–å…³é”®è¯ã€‚
            2) æ„é€ å¤šæ ·åŒ–æŸ¥è¯¢ï¼šæ ¸å¿ƒæ¦‚å¿µ+æ–¹æ³•å+ä»»åŠ¡å+è§¦å‘èŠ‚ç‚¹åç§°ã€‚
            3) è°ƒç”¨ `db.search_content` è¿›è¡Œå‘é‡æ£€ç´¢ã€‚
            4) å»é‡ï¼ˆåŒä¸€è®ºæ–‡çš„å¤šä¸ªç‰‡æ®µï¼‰ä¸è´¨é‡è¿‡æ»¤ã€‚
        """
        print(f"    ğŸ” å¼€å§‹å¬å›ä¸æƒ³æ³• '{idea.title}' ç›¸ä¼¼çš„æ–‡çŒ®")
        
        # æ­¥éª¤1ï¼šæ„é€ å¤šæ ·åŒ–æŸ¥è¯¢
        queries = await self._construct_search_queries(idea)
        print(f"    ğŸ“ æ„é€ äº† {len(queries)} ä¸ªæŸ¥è¯¢")
        
        # æ­¥éª¤2ï¼šæ‰§è¡Œæ£€ç´¢
        all_results = []
        for query in queries:
            try:
                # ä½¿ç”¨æ•°æ®åº“æœç´¢ï¼Œæ¯ä¸ªæŸ¥è¯¢å¬å›k/2ä¸ªç»“æœ
                results = self.db.search_content(
                    query, 
                    content_type="texts", 
                    n_results=max(k // len(queries), 5)
                )
                
                # æ ‡å‡†åŒ–æ£€ç´¢ç»“æœæ ¼å¼
                for result in results:
                    processed_result = {
                        "paper_id": result.get("paper_id", "unknown"),
                        "content": result.get("content", ""),
                        "metadata": {
                            "title": result.get("title", ""),
                            "authors": result.get("authors", []),
                            "venue": result.get("venue", ""),
                            "year": result.get("year", ""),
                            "query_source": query,
                            "relevance_score": result.get("score", 0.0)
                        }
                    }
                    all_results.append(processed_result)
                    
            except Exception as e:
                print(f"    âš ï¸ æŸ¥è¯¢ '{query}' å¤±è´¥: {e}")
                continue
        
        # æ­¥éª¤3ï¼šå»é‡å¤„ç†
        deduplicated_results = await self._deduplicate_papers(all_results)
        print(f"    ğŸ”„ å»é‡åä¿ç•™ {len(deduplicated_results)} ä¸ªæ–‡çŒ®ç‰‡æ®µ")
        
        # æ­¥éª¤4ï¼šè´¨é‡è¿‡æ»¤å’Œæ’åº
        filtered_results = await self._filter_and_rank_papers(deduplicated_results, k)
        print(f"    âœ… æœ€ç»ˆå¬å› {len(filtered_results)} ä¸ªé«˜è´¨é‡æ–‡çŒ®")
        
        return filtered_results
    
    async def _construct_search_queries(self, idea: CandidateIdea) -> List[str]:
        """æ„é€ å¤šæ ·åŒ–çš„æœç´¢æŸ¥è¯¢ã€‚"""
        queries = []
        
        # æŸ¥è¯¢1ï¼šåŸºäºæ ‡é¢˜çš„æ ¸å¿ƒæ¦‚å¿µ
        title_keywords = self._extract_core_concepts(idea.title)
        if title_keywords:
            queries.append(" ".join(title_keywords))
        
        # æŸ¥è¯¢2ï¼šåŸºäºæ ¸å¿ƒå‡è®¾çš„æ–¹æ³•å’Œä»»åŠ¡
        hypothesis_keywords = self._extract_core_concepts(idea.core_hypothesis)
        if hypothesis_keywords:
            queries.append(" ".join(hypothesis_keywords))
        
        # æŸ¥è¯¢3ï¼šåŸºäºåˆ›æ–°ç‚¹çš„æŠ€æœ¯å…³é”®è¯
        innovation_keywords = []
        for point in idea.initial_innovation_points:
            innovation_keywords.extend(self._extract_core_concepts(point))
        if innovation_keywords:
            # å–å‰3ä¸ªæœ€é‡è¦çš„åˆ›æ–°å…³é”®è¯
            queries.append(" ".join(innovation_keywords[:3]))
        
        # æŸ¥è¯¢4ï¼šåŸºäºè§¦å‘èŠ‚ç‚¹çš„ä¸“é—¨æœ¯è¯­
        if hasattr(idea, 'source_trigger_nodes') and idea.source_trigger_nodes:
            # ç®€åŒ–å¤„ç†ï¼šç›´æ¥ä½¿ç”¨èŠ‚ç‚¹IDä½œä¸ºæŸ¥è¯¢
            trigger_query = " ".join(idea.source_trigger_nodes[:2])  # æœ€å¤š2ä¸ªèŠ‚ç‚¹
            if trigger_query.strip():
                queries.append(trigger_query)
        
        # æŸ¥è¯¢5ï¼šç»„åˆæŸ¥è¯¢ï¼ˆæ ‡é¢˜+æ–¹æ³•ï¼‰
        if len(title_keywords) > 0 and len(hypothesis_keywords) > 0:
            combined_query = f"{title_keywords[0]} {hypothesis_keywords[0]}"
            queries.append(combined_query)
        
        # å»é™¤ç©ºæŸ¥è¯¢å’Œè¿‡çŸ­æŸ¥è¯¢
        filtered_queries = [q for q in queries if len(q.strip()) > 3]
        
        return filtered_queries[:5]  # æœ€å¤š5ä¸ªæŸ¥è¯¢ï¼Œé¿å…è¿‡åº¦æ£€ç´¢
    
    def _extract_core_concepts(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–æ ¸å¿ƒæ¦‚å¿µã€‚"""
        import re
        
        # ç®€åŒ–çš„å…³é”®è¯æå–ï¼šåŸºäºå¸¸è§å­¦æœ¯æœ¯è¯­æ¨¡å¼
        patterns = [
            r'\b[A-Z][a-z]*(?:\s+[A-Z][a-z]*)*\b',  # å¤§å†™å¼€å¤´çš„ä¸“æœ‰åè¯
            r'\b(?:learning|model|algorithm|method|approach|technique|framework)\b',  # æ ¸å¿ƒæ–¹æ³•è¯
            r'\b(?:classification|detection|generation|optimization|prediction|analysis)\b',  # ä»»åŠ¡è¯
            r'\b(?:neural|deep|machine|artificial|intelligence|network)\b',  # AIç›¸å…³è¯
            r'\b(?:transformer|attention|lstm|cnn|bert|gpt)\b',  # å…·ä½“æ¨¡å‹å
        ]
        
        keywords = []
        text_lower = text.lower()
        
        for pattern in patterns:
            matches = re.findall(pattern, text_lower, re.IGNORECASE)
            keywords.extend(matches)
        
        # å»é‡å¹¶ä¿æŒé¡ºåº
        seen = set()
        unique_keywords = []
        for kw in keywords:
            if kw.lower() not in seen:
                seen.add(kw.lower())
                unique_keywords.append(kw)
        
        return unique_keywords[:5]  # æœ€å¤š5ä¸ªå…³é”®è¯
    
    async def _deduplicate_papers(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¯¹æ£€ç´¢ç»“æœè¿›è¡Œå»é‡ã€‚"""
        seen_papers = {}
        deduplicated = []
        
        for result in results:
            paper_id = result["paper_id"]
            
            if paper_id in seen_papers:
                # å¦‚æœå·²å­˜åœ¨ï¼Œä¿ç•™ç›¸å…³æ€§åˆ†æ•°æ›´é«˜çš„
                existing_score = seen_papers[paper_id]["metadata"]["relevance_score"]
                current_score = result["metadata"]["relevance_score"]
                
                if current_score > existing_score:
                    # æ›¿æ¢ä¸ºåˆ†æ•°æ›´é«˜çš„ç»“æœ
                    seen_papers[paper_id] = result
            else:
                seen_papers[paper_id] = result
        
        return list(seen_papers.values())
    
    async def _filter_and_rank_papers(self, results: List[Dict[str, Any]], k: int) -> List[Dict[str, Any]]:
        """è¿‡æ»¤å’Œæ’åºæ£€ç´¢ç»“æœã€‚"""
        # è¿‡æ»¤ï¼šç§»é™¤å†…å®¹è¿‡çŸ­æˆ–è´¨é‡è¿‡ä½çš„ç»“æœ
        filtered = []
        for result in results:
            content = result["content"]
            score = result["metadata"]["relevance_score"]
            
            # åŸºæœ¬è´¨é‡è¿‡æ»¤
            if len(content) >= 50 and score > 0.1:  # æœ€ä½è´¨é‡é—¨æ§›
                filtered.append(result)
        
        # æ’åºï¼šæŒ‰ç›¸å…³æ€§åˆ†æ•°é™åº
        filtered.sort(key=lambda x: x["metadata"]["relevance_score"], reverse=True)
        
        # è¿”å›top-k
        return filtered[:k]

    async def _rerank_by_facets(self, idea: CandidateIdea, papers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """RAGç¬¬äºŒé˜¶æ®µï¼šåˆ†é¢é‡æ’ä¸å¯¹æ¯”ã€‚
        
        è¾“å…¥:
            - idea: å€™é€‰æƒ³æ³•ã€‚
            - papers: å¬å›çš„æ–‡çŒ®åˆ—è¡¨ã€‚
            
        è¾“å‡º:
            - Dict: åˆ†é¢åˆ†æç»“æœï¼ŒåŒ…å«æ¯ä¸ªfacetçš„ç›¸ä¼¼å·¥ä½œä¸å·®å¼‚åˆ†æã€‚
            
        å®ç°æ€è·¯:
            1) å¯¹æ¯ä¸ªnovelty facetï¼ˆæ¦‚å¿µ/æ–¹æ³•/åº”ç”¨/è¯„æµ‹ï¼‰ï¼Œç”¨ä¸“é—¨æç¤ºè¯è®©LLMå¯¹æ¯”ideaä¸papersã€‚
            2) è¯†åˆ«æœ€ç›¸ä¼¼çš„å·¥ä½œï¼Œè®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œæå–å…³é”®å·®å¼‚ç‚¹ã€‚
            3) ç”Ÿæˆåˆ†é¢å¾—åˆ†(1-10)ä¸å·®å¼‚æ€§ä¸»å¼ ï¼ˆclaim+evidenceï¼‰ã€‚
        """
        print(f"    ğŸ”¬ å¼€å§‹å¤šåˆ†é¢æ–°é¢–æ€§åˆ†æ")
        
        facet_results = {}
        
        # å¯¹æ¯ä¸ªæ–°é¢–æ€§åˆ†é¢è¿›è¡Œåˆ†æ
        for facet in self.novelty_facets:
            print(f"    ğŸ“Š åˆ†æ {facet} åˆ†é¢")
            
            try:
                # åˆ†é¢ç‰¹å®šçš„åˆ†æ
                facet_analysis = await self._analyze_single_facet(idea, papers, facet)
                facet_results[facet] = facet_analysis
                
            except Exception as e:
                print(f"    âš ï¸ {facet} åˆ†é¢åˆ†æå¤±è´¥: {e}")
                # è®¾ç½®é»˜è®¤åˆ†æç»“æœ
                facet_results[facet] = {
                    "score": 5.0,  # ä¸­ç­‰åˆ†æ•°
                    "most_similar_papers": [],
                    "differences": [f"{facet}åˆ†é¢åˆ†æå¤±è´¥"],
                    "analysis_method": "fallback"
                }
        
        print(f"    âœ… å®Œæˆ {len(facet_results)} ä¸ªåˆ†é¢çš„åˆ†æ")
        return facet_results
    
    async def _analyze_single_facet(self, idea: CandidateIdea, papers: List[Dict[str, Any]], facet: str) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªæ–°é¢–æ€§åˆ†é¢ã€‚"""
        
        # é€‰æ‹©æœ€ç›¸å…³çš„è®ºæ–‡è¿›è¡Œè¯¦ç»†å¯¹æ¯”ï¼ˆæœ€å¤š5ç¯‡ï¼‰
        top_papers = papers[:5]
        
        if not top_papers:
            return {
                "score": 8.0,  # å¦‚æœæ²¡æœ‰ç›¸ä¼¼å·¥ä½œï¼Œè®¤ä¸ºè¾ƒæ–°é¢–
                "most_similar_papers": [],
                "differences": [f"æœªå‘ç°{facet}åˆ†é¢çš„ç›´æ¥ç›¸ä¼¼å·¥ä½œ"],
                "analysis_method": "no_similar_works"
            }
        
        # æ ¹æ®åˆ†é¢ç±»å‹è¿›è¡Œä¸“é—¨åˆ†æ
        if facet == "conceptual":
            return await self._analyze_conceptual_novelty(idea, top_papers)
        elif facet == "methodological":
            return await self._analyze_methodological_novelty(idea, top_papers)
        elif facet == "application":
            return await self._analyze_application_novelty(idea, top_papers)
        elif facet == "evaluation":
            return await self._analyze_evaluation_novelty(idea, top_papers)
        else:
            # é€šç”¨åˆ†æ
            return await self._analyze_generic_novelty(idea, top_papers, facet)
    
    async def _analyze_conceptual_novelty(self, idea: CandidateIdea, papers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åŸºäºLLMçš„æ¦‚å¿µæ–°é¢–æ€§åˆ†æã€‚"""
        
        # å‡†å¤‡ç›¸ä¼¼æ–‡çŒ®çš„ä¸Šä¸‹æ–‡
        papers_context = ""
        for i, paper in enumerate(papers[:3]):  # åªåˆ†ææœ€ç›¸ä¼¼çš„3ç¯‡
            papers_context += f"\nè®ºæ–‡ {i+1}: {paper['metadata']['title']}\n"
            papers_context += f"å†…å®¹æ‘˜è¦: {paper['content'][:500]}...\n"
        
        if not papers_context.strip():
            papers_context = "æœªæ‰¾åˆ°ç›´æ¥ç›¸å…³çš„å·²æœ‰å·¥ä½œã€‚"
        
        # æ„é€ LLMæç¤ºè¯
        prompt = f"""
ä½œä¸ºå­¦æœ¯ä¸“å®¶ï¼Œè¯·ä»**æ¦‚å¿µåˆ›æ–°**è§’åº¦è¯„ä¼°ä»¥ä¸‹ç ”ç©¶æƒ³æ³•çš„æ–°é¢–æ€§ã€‚

**å¾…è¯„ä¼°æƒ³æ³•**ï¼š
- æ ‡é¢˜: {idea.title}
- æ ¸å¿ƒå‡è®¾: {idea.core_hypothesis}
- åˆ›æ–°ç‚¹: {', '.join(idea.initial_innovation_points)}

**å·²æœ‰ç›¸å…³å·¥ä½œ**ï¼š
{papers_context}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œæ¦‚å¿µæ–°é¢–æ€§åˆ†æï¼š

1. **æ ¸å¿ƒæ¦‚å¿µåˆ›æ–°æ€§**: æƒ³æ³•å¼•å…¥çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯å¦æ–°é¢–ï¼Ÿä¸å·²æœ‰æ¦‚å¿µæ¡†æ¶çš„åŒºåˆ«ï¼Ÿ
2. **ç†è®ºè´¡çŒ®**: æ˜¯å¦æå‡ºäº†æ–°çš„ç†è®ºè§‚ç‚¹æˆ–æ¦‚å¿µæ¨¡å‹ï¼Ÿ
3. **æ¦‚å¿µæ•´åˆ**: æ˜¯å¦åˆ›æ–°æ€§åœ°ç»“åˆäº†ä¸åŒé¢†åŸŸçš„æ¦‚å¿µï¼Ÿ
4. **é—®é¢˜å®šä¹‰**: æ˜¯å¦ä»¥æ–°çš„è§’åº¦é‡æ–°å®šä¹‰äº†é—®é¢˜ï¼Ÿ

è¯·åŸºäºä½ çš„åˆ†æç»™å‡ºè¯„åˆ†ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{{
  "score": X.X,  // æ¦‚å¿µæ–°é¢–æ€§è¯„åˆ† (1-10åˆ†ï¼Œ10åˆ†æœ€æ–°é¢–ï¼Œè¯·åŸºäºå®é™…åˆ†æç»™å‡º)
  "analysis": {{
    "core_innovation": "è¯¦ç»†åˆ†ææ ¸å¿ƒæ¦‚å¿µçš„åˆ›æ–°æ€§",
    "theoretical_contribution": "åˆ†æç†è®ºè´¡çŒ®",
    "concept_integration": "åˆ†ææ¦‚å¿µæ•´åˆçš„åˆ›æ–°æ€§",
    "problem_redefinition": "åˆ†æé—®é¢˜é‡æ–°å®šä¹‰çš„ç¨‹åº¦"
  }},
  "most_similar_papers": [
    {{
      "title": "æœ€ç›¸ä¼¼è®ºæ–‡æ ‡é¢˜",
      "similarity_reason": "ç›¸ä¼¼æ€§åˆ†æ",
      "key_differences": "å…³é”®å·®å¼‚ç‚¹"
    }}
  ],
  "differences": [
    "å·®å¼‚ç‚¹1ï¼šå…·ä½“çš„æ¦‚å¿µåˆ›æ–°å·®å¼‚",
    "å·®å¼‚ç‚¹2ï¼šç†è®ºæ¡†æ¶çš„å·®å¼‚"
  ],
  "strengths": ["æ¦‚å¿µæ–°é¢–æ€§çš„ä¼˜åŠ¿1", "æ¦‚å¿µæ–°é¢–æ€§çš„ä¼˜åŠ¿2"],
  "weaknesses": ["å¯èƒ½çš„æ¦‚å¿µå±€é™æ€§1", "å¯èƒ½çš„æ¦‚å¿µå±€é™æ€§2"]
}}
"""

        try:
            response_data = await self.llm.generate(
                model_name=self.config.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=15000,
                agent_name=self.name,
                task_type="novelty_assessment"
            )
            response = response_data.get("content", "")
            
            # è§£æLLMå“åº”
            import json
            import re
            try:
                if "```json" in response:
                    json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
                    if json_match:
                        json_content = json_match.group(1).strip()
                    else:
                        json_start = response.find("```json") + 7
                        json_content = response[json_start:].strip()
                        if json_content.endswith("```"):
                            json_content = json_content[:-3].strip()
                else:
                    json_content = response.strip()
                
                result = json.loads(json_content)
                
                # æ ‡å‡†åŒ–è¾“å‡ºæ ¼å¼
                return {
                    "score": float(result.get("score", 5.0)),
                    "most_similar_papers": result.get("most_similar_papers", [])[:3],
                    "differences": result.get("differences", ["LLMåˆ†æçš„æ¦‚å¿µå·®å¼‚"]),
                    "analysis_method": "llm_conceptual_analysis",
                    "detailed_analysis": result.get("analysis", {}),
                    "strengths": result.get("strengths", []),
                    "weaknesses": result.get("weaknesses", [])
                }
                
            except json.JSONDecodeError:
                print(f"    âš ï¸ LLMæ¦‚å¿µæ–°é¢–æ€§åˆ†æå“åº”è§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨è¯„ä¼°")
                return self._fallback_conceptual_analysis(idea, papers)
                
        except Exception as e:
            print(f"    âŒ LLMæ¦‚å¿µæ–°é¢–æ€§åˆ†æå¤±è´¥: {str(e)}ï¼Œä½¿ç”¨å¤‡ç”¨è¯„ä¼°")
            return self._fallback_conceptual_analysis(idea, papers)
    
    def _fallback_conceptual_analysis(self, idea: CandidateIdea, papers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å¤‡ç”¨çš„æ¦‚å¿µæ–°é¢–æ€§åˆ†æã€‚"""
        return {
            "score": 6.0,  # ä¸­ç­‰åä¸Šåˆ†æ•°
            "most_similar_papers": [{"title": p["metadata"]["title"], "similarity_reason": "å…³é”®è¯åŒ¹é…"} for p in papers[:2]],
            "differences": ["æ¦‚å¿µæ¡†æ¶å­˜åœ¨ä¸€å®šåˆ›æ–°æ€§", "ç†è®ºè´¡çŒ®éœ€è¦è¿›ä¸€æ­¥éªŒè¯"],
            "analysis_method": "fallback_analysis",
            "detailed_analysis": {"core_innovation": "åˆ†æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨è¯„ä¼°"},
            "strengths": ["æƒ³æ³•ç»“æ„å®Œæ•´"],
            "weaknesses": ["éœ€è¦æ›´æ·±å…¥çš„æ¦‚å¿µåˆ†æ"]
        }
    
    async def _analyze_methodological_novelty(self, idea: CandidateIdea, papers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åŸºäºLLMçš„æ–¹æ³•æ–°é¢–æ€§åˆ†æã€‚"""
        
        # å‡†å¤‡ç›¸ä¼¼æ–‡çŒ®çš„ä¸Šä¸‹æ–‡
        papers_context = ""
        for i, paper in enumerate(papers[:3]):
            papers_context += f"\nè®ºæ–‡ {i+1}: {paper['metadata']['title']}\n"
            papers_context += f"å†…å®¹æ‘˜è¦: {paper['content'][:500]}...\n"
        
        if not papers_context.strip():
            papers_context = "æœªæ‰¾åˆ°ç›´æ¥ç›¸å…³çš„å·²æœ‰å·¥ä½œã€‚"
        
        prompt = f"""
ä½œä¸ºå­¦æœ¯ä¸“å®¶ï¼Œè¯·ä»**æ–¹æ³•åˆ›æ–°**è§’åº¦è¯„ä¼°ä»¥ä¸‹ç ”ç©¶æƒ³æ³•çš„æ–°é¢–æ€§ã€‚

**å¾…è¯„ä¼°æƒ³æ³•**ï¼š
- æ ‡é¢˜: {idea.title}
- æ ¸å¿ƒå‡è®¾: {idea.core_hypothesis}
- åˆ›æ–°ç‚¹: {', '.join(idea.initial_innovation_points)}

**å·²æœ‰ç›¸å…³å·¥ä½œ**ï¼š
{papers_context}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œæ–¹æ³•æ–°é¢–æ€§åˆ†æï¼š

1. **æŠ€æœ¯æ–¹æ³•åˆ›æ–°**: æ˜¯å¦æå‡ºäº†æ–°çš„ç®—æ³•ã€æ¨¡å‹æˆ–æŠ€æœ¯æ¡†æ¶ï¼Ÿ
2. **æ–¹æ³•ç»„åˆåˆ›æ–°**: æ˜¯å¦åˆ›æ–°æ€§åœ°ç»„åˆäº†ç°æœ‰æ–¹æ³•ï¼Ÿ
3. **å®ç°åˆ›æ–°**: åœ¨å…·ä½“å®ç°å±‚é¢æ˜¯å¦æœ‰æŠ€æœ¯çªç ´ï¼Ÿ
4. **ä¼˜åŒ–åˆ›æ–°**: æ˜¯å¦åœ¨æ•ˆç‡ã€å‡†ç¡®æ€§æˆ–èµ„æºä½¿ç”¨ä¸Šæœ‰æ–¹æ³•å­¦æ”¹è¿›ï¼Ÿ

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{{
  "score": 7.5,  // æ–¹æ³•æ–°é¢–æ€§è¯„åˆ† (1-10åˆ†ï¼Œ10åˆ†æœ€æ–°é¢–)
  "analysis": {{
    "technical_innovation": "æŠ€æœ¯æ–¹æ³•åˆ›æ–°åˆ†æ",
    "combination_innovation": "æ–¹æ³•ç»„åˆåˆ›æ–°åˆ†æ",
    "implementation_innovation": "å®ç°å±‚é¢åˆ›æ–°åˆ†æ",
    "optimization_innovation": "ä¼˜åŒ–æ”¹è¿›åˆ†æ"
  }},
  "most_similar_papers": [
    {{
      "title": "æœ€ç›¸ä¼¼è®ºæ–‡æ ‡é¢˜",
      "method_similarity": "æ–¹æ³•ç›¸ä¼¼æ€§åˆ†æ",
      "key_differences": "å…³é”®æŠ€æœ¯å·®å¼‚"
    }}
  ],
  "differences": [
    "å·®å¼‚ç‚¹1ï¼šå…·ä½“çš„æŠ€æœ¯æ–¹æ³•å·®å¼‚",
    "å·®å¼‚ç‚¹2ï¼šç®—æ³•æ¶æ„çš„åˆ›æ–°"
  ],
  "strengths": ["æ–¹æ³•åˆ›æ–°çš„ä¼˜åŠ¿1", "æ–¹æ³•åˆ›æ–°çš„ä¼˜åŠ¿2"],
  "potential_issues": ["å¯èƒ½çš„æŠ€æœ¯æŒ‘æˆ˜1", "å¯èƒ½çš„æŠ€æœ¯æŒ‘æˆ˜2"]
}}
"""

        try:
            response_data = await self.llm.generate(
                model_name=self.config.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=15000,
                agent_name=self.name,
                task_type="novelty_assessment"
            )
            response = response_data.get("content", "")
            
            # è§£æLLMå“åº”
            import json
            import re
            try:
                if "```json" in response:
                    json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
                    if json_match:
                        json_content = json_match.group(1).strip()
                    else:
                        json_start = response.find("```json") + 7
                        json_content = response[json_start:].strip()
                        if json_content.endswith("```"):
                            json_content = json_content[:-3].strip()
                else:
                    json_content = response.strip()
                
                result = json.loads(json_content)
                
                return {
                    "score": float(result.get("score", 5.0)),
                    "most_similar_papers": result.get("most_similar_papers", [])[:3],
                    "differences": result.get("differences", ["LLMåˆ†æçš„æ–¹æ³•å·®å¼‚"]),
                    "analysis_method": "llm_methodological_analysis",
                    "detailed_analysis": result.get("analysis", {}),
                    "strengths": result.get("strengths", []),
                    "potential_issues": result.get("potential_issues", [])
                }
                
            except json.JSONDecodeError:
                print(f"    âš ï¸ LLMæ–¹æ³•æ–°é¢–æ€§åˆ†æå“åº”è§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨è¯„ä¼°")
                return {"score": 6.0, "most_similar_papers": [], "differences": ["æ–¹æ³•åˆ†æå¤±è´¥"], "analysis_method": "fallback"}
                
        except Exception as e:
            print(f"    âŒ LLMæ–¹æ³•æ–°é¢–æ€§åˆ†æå¤±è´¥: {str(e)}ï¼Œä½¿ç”¨å¤‡ç”¨è¯„ä¼°")
            return {"score": 6.0, "most_similar_papers": [], "differences": ["æ–¹æ³•åˆ†æå¤±è´¥"], "analysis_method": "fallback"}
    
    async def _analyze_application_novelty(self, idea: CandidateIdea, papers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æåº”ç”¨æ–°é¢–æ€§ã€‚"""
        
        # æå–åº”ç”¨é¢†åŸŸä¿¡æ¯
        idea_domains = self._extract_application_domains(idea.title + " " + idea.core_hypothesis)
        
        similar_papers = []
        max_similarity = 0.0
        
        for paper in papers:
            paper_domains = self._extract_application_domains(paper["content"])
            
            if len(idea_domains) > 0:
                overlap = len(set(idea_domains) & set(paper_domains))
                similarity = overlap / max(len(set(idea_domains) | set(paper_domains)), 1)
                
                similar_papers.append({
                    "paper_id": paper["paper_id"],
                    "title": paper["metadata"]["title"],
                    "similarity": similarity,
                    "shared_domains": list(set(idea_domains) & set(paper_domains))
                })
                
                max_similarity = max(max_similarity, similarity)
        
        similar_papers.sort(key=lambda x: x["similarity"], reverse=True)
        
        # åŸºäºç›¸ä¼¼åº¦è®¡ç®—åº”ç”¨æ–°é¢–æ€§è¯„åˆ†ï¼ˆå¦‚æœæ²¡æœ‰LLMè¯„ä¼°ç»“æœï¼‰
        novelty_score = max(1.0, 8.5 - max_similarity * 7.5)
        
        differences = []
        if max_similarity < 0.3:
            differences.append("å¼€æ‹“äº†å…¨æ–°çš„åº”ç”¨é¢†åŸŸ")
        elif max_similarity < 0.6:
            differences.append("åœ¨ç°æœ‰åº”ç”¨åŸºç¡€ä¸Šæ‰©å±•åˆ°æ–°çš„åœºæ™¯")
        else:
            differences.append("åº”ç”¨åœºæ™¯ä¸ç°æœ‰å·¥ä½œé‡å è¾ƒå¤š")
            
        return {
            "score": novelty_score,
            "most_similar_papers": similar_papers[:3],
            "differences": differences,
            "analysis_method": "application_analysis"
        }
    
    async def _analyze_evaluation_novelty(self, idea: CandidateIdea, papers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æè¯„ä¼°æ–°é¢–æ€§ã€‚"""
        
        # æå–å®éªŒå’Œè¯„ä¼°ç›¸å…³ä¿¡æ¯
        idea_evaluations = []
        for exp in idea.preliminary_experiments:
            idea_evaluations.append(exp.get('metric', ''))
            idea_evaluations.append(exp.get('dataset', ''))
        
        similar_papers = []
        max_similarity = 0.0
        
        for paper in papers:
            paper_evaluations = self._extract_evaluation_terms(paper["content"])
            
            if len(idea_evaluations) > 0:
                overlap = len(set(idea_evaluations) & set(paper_evaluations))
                similarity = overlap / max(len(set(idea_evaluations) | set(paper_evaluations)), 1)
                
                similar_papers.append({
                    "paper_id": paper["paper_id"],
                    "title": paper["metadata"]["title"],
                    "similarity": similarity,
                    "shared_evaluations": list(set(idea_evaluations) & set(paper_evaluations))
                })
                
                max_similarity = max(max_similarity, similarity)
        
        similar_papers.sort(key=lambda x: x["similarity"], reverse=True)
        
        # åŸºäºç›¸ä¼¼åº¦è®¡ç®—è¯„ä¼°æ–°é¢–æ€§è¯„åˆ†ï¼ˆå¦‚æœæ²¡æœ‰LLMè¯„ä¼°ç»“æœï¼‰
        novelty_score = max(1.0, 9.0 - max_similarity * 8.0)
        
        differences = []
        if max_similarity < 0.25:
            differences.append("è®¾è®¡äº†åˆ›æ–°çš„è¯„ä¼°æ¡†æ¶å’ŒæŒ‡æ ‡")
        elif max_similarity < 0.55:
            differences.append("åœ¨ç°æœ‰è¯„ä¼°åŸºç¡€ä¸Šå¼•å…¥äº†æ–°çš„è¯„ä¼°ç»´åº¦")
        else:
            differences.append("è¯„ä¼°æ–¹æ³•ä¸ç°æœ‰å·¥ä½œç›¸ä¼¼åº¦è¾ƒé«˜")
            
        return {
            "score": novelty_score,
            "most_similar_papers": similar_papers[:3],
            "differences": differences,
            "analysis_method": "evaluation_analysis"
        }
    
    async def _analyze_generic_novelty(self, idea: CandidateIdea, papers: List[Dict[str, Any]], facet: str) -> Dict[str, Any]:
        """é€šç”¨æ–°é¢–æ€§åˆ†æã€‚"""
        
        # åŸºæœ¬çš„æ–‡æœ¬ç›¸ä¼¼åº¦åˆ†æ
        idea_text = idea.title + " " + idea.core_hypothesis
        
        similarities = []
        for paper in papers:
            # ç®€åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—
            similarity = self._calculate_text_similarity(idea_text, paper["content"])
            similarities.append({
                "paper_id": paper["paper_id"],
                "title": paper["metadata"]["title"],
                "similarity": similarity
            })
        
        similarities.sort(key=lambda x: x["similarity"], reverse=True)
        max_similarity = similarities[0]["similarity"] if similarities else 0.0
        
        # åŸºäºç›¸ä¼¼åº¦è®¡ç®—é€šç”¨æ–°é¢–æ€§è¯„åˆ†ï¼ˆå¦‚æœæ²¡æœ‰LLMè¯„ä¼°ç»“æœï¼‰
        novelty_score = max(1.0, 8.0 - max_similarity * 7.0)
        
        return {
            "score": novelty_score,
            "most_similar_papers": similarities[:3],
            "differences": [f"{facet}åˆ†é¢æ˜¾ç¤ºé€‚åº¦çš„æ–°é¢–æ€§"],
            "analysis_method": "generic_similarity"
        }
    
    def _extract_method_terms(self, text: str) -> List[str]:
        """æå–æ–¹æ³•ç›¸å…³æœ¯è¯­ã€‚"""
        import re
        
        method_patterns = [
            r'\b(?:algorithm|approach|method|technique|framework|model|architecture)\b',
            r'\b(?:neural|deep|machine|learning|training|optimization)\b',
            r'\b(?:transformer|attention|lstm|cnn|bert|gpt|resnet)\b',
            r'\b(?:supervised|unsupervised|reinforcement|transfer|meta)\b'
        ]
        
        methods = []
        text_lower = text.lower()
        
        for pattern in method_patterns:
            matches = re.findall(pattern, text_lower)
            methods.extend(matches)
        
        return list(set(methods))
    
    def _extract_application_domains(self, text: str) -> List[str]:
        """æå–åº”ç”¨é¢†åŸŸæœ¯è¯­ã€‚"""
        import re
        
        domain_patterns = [
            r'\b(?:computer vision|natural language|speech|robotics|healthcare|finance)\b',
            r'\b(?:classification|detection|generation|translation|recommendation)\b',
            r'\b(?:image|text|video|audio|graph|code|medical)\b'
        ]
        
        domains = []
        text_lower = text.lower()
        
        for pattern in domain_patterns:
            matches = re.findall(pattern, text_lower)
            domains.extend(matches)
        
        return list(set(domains))
    
    def _extract_evaluation_terms(self, text: str) -> List[str]:
        """æå–è¯„ä¼°ç›¸å…³æœ¯è¯­ã€‚"""
        import re
        
        eval_patterns = [
            r'\b(?:accuracy|precision|recall|f1|auc|bleu|rouge|meteor)\b',
            r'\b(?:benchmark|dataset|evaluation|metric|measure|score)\b',
            r'\b(?:test|validation|experiment|ablation|comparison)\b'
        ]
        
        evaluations = []
        text_lower = text.lower()
        
        for pattern in eval_patterns:
            matches = re.findall(pattern, text_lower)
            evaluations.extend(matches)
        
        return list(set(evaluations))
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦çš„ç®€åŒ–å®ç°ã€‚"""
        
        # åŸºäºè¯æ±‡é‡å çš„ç®€å•ç›¸ä¼¼åº¦è®¡ç®—
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if len(words1) == 0 or len(words2) == 0:
            return 0.0
        
        overlap = len(words1 & words2)
        union = len(words1 | words2)
        
        return overlap / union if union > 0 else 0.0

    async def _synthesize_novelty_critique(self, idea: CandidateIdea, facet_results: Dict[str, Any]) -> NoveltyCritique:
        """ç¬¬ä¸‰é˜¶æ®µï¼šç»¼åˆæ–°é¢–æ€§è¯„å®¡ã€‚
        
        è¾“å…¥:
            - idea: å€™é€‰æƒ³æ³•ã€‚
            - facet_results: åˆ†é¢åˆ†æç»“æœã€‚
            
        è¾“å‡º:
            - NoveltyCritique: å®Œæ•´çš„æ–°é¢–æ€§è¯„å®¡ç»“æœã€‚
            
        å®ç°æ€è·¯:
            1) æ±‡æ€»å„åˆ†é¢å¾—åˆ†ï¼Œè®¡ç®—åŠ æƒå¹³å‡ä½œä¸ºæ€»ä½“æ–°é¢–æ€§åˆ†æ•°ã€‚
            2) æ•´ç†æœ€ç›¸ä¼¼å·¥ä½œåˆ—è¡¨ï¼ŒæŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—ã€‚
            3) åˆå¹¶å„åˆ†é¢çš„å·®å¼‚æ€§ä¸»å¼ ï¼Œå½¢æˆç»“æ„åŒ–çš„difference_claimsã€‚
            4) è®°å½•è¯„å®¡æ–¹æ³•ä¸å‚æ•°ä¾›å¤ç°ã€‚
        """
        print(f"    ğŸ¯ ç»¼åˆæ–°é¢–æ€§è¯„å®¡ç»“æœ")
        
        # æ­¥éª¤1ï¼šè®¡ç®—åŠ æƒæ€»ä½“æ–°é¢–æ€§åˆ†æ•°
        novelty_score = await self._calculate_weighted_novelty_score(facet_results)
        
        # æ­¥éª¤2ï¼šæ•´ç†æœ€ç›¸ä¼¼å·¥ä½œåˆ—è¡¨
        similar_works = await self._compile_similar_works(facet_results)
        
        # æ­¥éª¤3ï¼šåˆå¹¶å·®å¼‚æ€§ä¸»å¼ 
        difference_claims = await self._compile_difference_claims(facet_results)
        
        # æ­¥éª¤4ï¼šç”Ÿæˆåˆ†é¢å¾—åˆ†è¯¦æƒ…
        facet_scores = {facet: result["score"] for facet, result in facet_results.items()}
        
        # åˆ›å»ºNoveltyCritiqueå¯¹è±¡
        critique = NoveltyCritique(
            idea_id=idea.id,
            novelty_score=novelty_score,
            facet_scores=facet_scores,
            similar_works=similar_works,
            difference_claims=difference_claims,
            method={
                "approach": "multi_facet_rag",
                "facets": list(self.novelty_facets),
                "weights": self._get_facet_weights(),
                "retrieval_k": len(similar_works),
                "timestamp": datetime.now().isoformat()
            }
        )
        
        print(f"    âœ… æ–°é¢–æ€§è¯„åˆ†: {novelty_score:.2f}/10.0")
        print(f"    ğŸ“‹ å‘ç° {len(similar_works)} ä¸ªç›¸ä¼¼å·¥ä½œ")
        print(f"    ğŸ’¡ ç”Ÿæˆ {len(difference_claims)} æ¡å·®å¼‚æ€§ä¸»å¼ ")
        
        return critique
    
    async def _calculate_weighted_novelty_score(self, facet_results: Dict[str, Any]) -> float:
        """è®¡ç®—åŠ æƒæ–°é¢–æ€§åˆ†æ•°ã€‚"""
        
        # åˆ†é¢æƒé‡è®¾ç½®
        weights = self._get_facet_weights()
        
        total_score = 0.0
        total_weight = 0.0
        
        for facet, result in facet_results.items():
            weight = weights.get(facet, 0.25)  # é»˜è®¤æƒé‡
            score = result.get("score", 5.0)
            
            total_score += score * weight
            total_weight += weight
        
        # å½’ä¸€åŒ–
        if total_weight > 0:
            final_score = total_score / total_weight
        else:
            final_score = 5.0  # é»˜è®¤ä¸­ç­‰åˆ†æ•°
        
        # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…
        return max(1.0, min(10.0, final_score))
    
    def _get_facet_weights(self) -> Dict[str, float]:
        """è·å–åˆ†é¢æƒé‡é…ç½®ã€‚"""
        return {
            "conceptual": 0.30,      # æ¦‚å¿µæ–°é¢–æ€§æœ€é‡è¦
            "methodological": 0.35,  # æ–¹æ³•æ–°é¢–æ€§æ¬¡ä¹‹
            "application": 0.20,     # åº”ç”¨æ–°é¢–æ€§
            "evaluation": 0.15       # è¯„ä¼°æ–°é¢–æ€§
        }
    
    async def _compile_similar_works(self, facet_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ•´ç†æœ€ç›¸ä¼¼å·¥ä½œåˆ—è¡¨ã€‚"""
        
        all_similar_works = {}  # ä½¿ç”¨å­—å…¸å»é‡
        
        # æ”¶é›†æ‰€æœ‰åˆ†é¢çš„ç›¸ä¼¼å·¥ä½œ
        for facet, result in facet_results.items():
            similar_papers = result.get("most_similar_papers", [])
            
            for paper in similar_papers:
                paper_id = paper.get("paper_id", "unknown")
                
                if paper_id in all_similar_works:
                    # å¦‚æœå·²å­˜åœ¨ï¼Œæ›´æ–°æœ€é«˜ç›¸ä¼¼åº¦å’Œå¢åŠ ç›¸å…³åˆ†é¢
                    existing = all_similar_works[paper_id]
                    current_similarity = paper.get("similarity", 0.0)
                    
                    if current_similarity > existing.get("max_similarity", 0.0):
                        existing["max_similarity"] = current_similarity
                    
                    existing["relevant_facets"].append(facet)
                else:
                    # æ–°çš„ç›¸ä¼¼å·¥ä½œ
                    all_similar_works[paper_id] = {
                        "paper_id": paper_id,
                        "title": paper.get("title", "Unknown Title"),
                        "max_similarity": paper.get("similarity", 0.0),
                        "relevant_facets": [facet],
                        "details": paper
                    }
        
        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰ç›¸ä¼¼åº¦æ’åº
        similar_works_list = list(all_similar_works.values())
        similar_works_list.sort(key=lambda x: x["max_similarity"], reverse=True)
        
        # è¿”å›å‰10ä¸ªæœ€ç›¸ä¼¼çš„å·¥ä½œ
        return similar_works_list[:10]
    
    async def _compile_difference_claims(self, facet_results: Dict[str, Any]) -> List[str]:
        """åˆå¹¶å·®å¼‚æ€§ä¸»å¼ ã€‚"""
        
        all_claims = []
        
        for facet, result in facet_results.items():
            differences = result.get("differences", [])
            
            # ä¸ºæ¯ä¸ªå·®å¼‚ç‚¹æ·»åŠ åˆ†é¢æ ‡è¯†
            for diff in differences:
                claim = f"[{facet.upper()}] {diff}"
                all_claims.append(claim)
        
        # å»é‡å¹¶ä¿æŒé¡ºåº
        unique_claims = []
        seen = set()
        
        for claim in all_claims:
            if claim not in seen:
                unique_claims.append(claim)
                seen.add(claim)
        
        # å¦‚æœæ²¡æœ‰ç‰¹å®šçš„å·®å¼‚ç‚¹ï¼Œæ·»åŠ é€šç”¨ä¸»å¼ 
        if not unique_claims:
            unique_claims.append("[GENERAL] è¯¥æƒ³æ³•åœ¨å¤šä¸ªç»´åº¦ä¸Šæ˜¾ç¤ºäº†é€‚åº¦çš„åˆ›æ–°æ€§")
        
        return unique_claims


class FeasibilityCriticAgent(BaseIdeaAgent):
    """ç¬¬ä¸‰é˜¶æ®µï¼šå¯è¡Œæ€§æ‰¹åˆ¤ï¼ˆå¹¶è¡Œä¹‹äºŒï¼‰ã€‚

    æ ¸å¿ƒèŒè´£:
        - ç»“åˆæœºä¼šå›¾è°±çš„èµ„æºèŠ‚ç‚¹ä¸å…³ç³»è¾¹ï¼Œå¯¹æƒ³æ³•çš„ç›¸å…³æ€§ä¸å¯è¡Œæ€§åšå¤šç»´è¯„ä¼°ã€‚

    æ³¨æ„äº‹é¡¹:
        - æ˜ç¡® required_assets çš„å¯å¾—æ€§ä¸æ›¿ä»£è·¯å¾„ï¼›å†²çªè¾¹åº”é™ä½ç½®ä¿¡åº¦ä½†ä¸ç›´æ¥å¦å®šã€‚
    """

    def __init__(self, name: str, llm_factory: LLMFactory, db: AcademicPaperDatabase, config: Optional[AgentConfig] = None):
        super().__init__(name, llm_factory, db, config)
        self.feasibility_dimensions = ["relevance", "asset_availability", "complexity", "risk_assessment"]

    async def assess_batch_comprehensive(self, ideas: List[CandidateIdea], graph: SemanticOpportunityGraph) -> List[FeasibilityCritique]:
        """ä¸€æ¬¡LLMè°ƒç”¨å¯¹æ•´æ‰¹æƒ³æ³•è¿›è¡Œç»¼åˆå¯è¡Œæ€§è¯„ä¼°ã€‚"""
        print(f"ğŸ”¬ å¼€å§‹æ‰¹é‡ç»¼åˆå¯è¡Œæ€§è¯„ä¼°ï¼š{len(ideas)}ä¸ªæƒ³æ³•")
        
        # ä¸ºæ•´æ‰¹æƒ³æ³•æ„å»ºç»¼åˆè¯„ä¼°prompt
        ideas_summary = "\n".join([
            f"{i+1}. {idea.title}\n   æ ¸å¿ƒå‡è®¾: {idea.core_hypothesis}\n   æ‰€éœ€èµ„äº§: {', '.join([asset.get('name', asset.get('type', 'unknown')) for asset in getattr(idea, 'required_assets', [])])}"
            for i, idea in enumerate(ideas)
        ])
        
        # ä»å›¾è°±ä¸­è·å–å¯ç”¨èµ„æºæ¦‚å†µ
        available_methods = len([n for n, d in graph.nodes(data=True) if d.get('type') == 'Method'])
        available_datasets = len([n for n, d in graph.nodes(data=True) if d.get('type') == 'Dataset'])
        available_metrics = len([n for n, d in graph.nodes(data=True) if d.get('type') == 'Metric'])
        
        prompt = f"""
ä½œä¸ºæŠ€æœ¯å¯è¡Œæ€§ä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹{len(ideas)}ä¸ªç ”ç©¶æƒ³æ³•è¿›è¡Œç»¼åˆå¯è¡Œæ€§è¯„ä¼°ã€‚

**å¾…è¯„ä¼°æƒ³æ³•åˆ—è¡¨**ï¼š
{ideas_summary}

**å¯ç”¨èµ„æºæ¦‚å†µ**ï¼š
- æ–¹æ³•åº“: {available_methods}ä¸ªå¯ç”¨æ–¹æ³•
- æ•°æ®é›†: {available_datasets}ä¸ªå¯ç”¨æ•°æ®é›†  
- è¯„ä¼°æŒ‡æ ‡: {available_metrics}ä¸ªå¯ç”¨æŒ‡æ ‡

**è¯„ä¼°ç»´åº¦**ï¼š
1. **ç›¸å…³æ€§** (0-3åˆ†): æƒ³æ³•ä¸ç°æœ‰æŠ€æœ¯ç”Ÿæ€çš„å¥‘åˆåº¦
2. **èµ„äº§å¯è·å¾—æ€§** (0-3åˆ†): æ‰€éœ€æ•°æ®é›†ã€æ–¹æ³•ã€å·¥å…·çš„å¯è·å¾—æ€§
3. **å®ç°å¤æ‚åº¦** (0-2åˆ†): æŠ€æœ¯å®ç°çš„éš¾åº¦ï¼ˆåˆ†æ•°è¶Šä½è¶Šå¤æ‚ï¼‰
4. **é£é™©è¯„ä¼°** (0-2åˆ†): æŠ€æœ¯é£é™©å’Œä¸ç¡®å®šæ€§ï¼ˆåˆ†æ•°è¶Šä½é£é™©è¶Šé«˜ï¼‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
  "batch_analysis": "å¯¹æ•´æ‰¹æƒ³æ³•çš„ç»¼åˆå¯è¡Œæ€§åˆ†æ",
  "feasibility_assessments": [
    {{
      "idea_index": 1,
      "feasibility_score": X.X,
      "dimension_scores": {{
        "relevance": X.X,
        "asset_availability": X.X,
        "complexity": X.X, 
        "risk_assessment": X.X
      }},
      "reasoning": "è¯¦ç»†å¯è¡Œæ€§åˆ†æ",
      "required_resources": ["èµ„æº1", "èµ„æº2"],
      "potential_risks": ["é£é™©1", "é£é™©2"],
      "implementation_suggestions": ["å»ºè®®1", "å»ºè®®2"]
    }}
  ]
}}
"""
        
        try:
            response_data = await self.llm.generate(
                model_name=self.config.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=15000,
                agent_name=self.name,
                task_type="batch_feasibility_assessment"
            )
            response = response_data.get("content", "")
            
            # è§£æJSONå“åº”
            import json
            import re
            json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
            if json_match:
                json_content = json_match.group(1).strip()
            else:
                json_content = response.strip()
            
            result = json.loads(json_content)
            assessments = result.get("feasibility_assessments", [])
            
            # ä¸ºæ¯ä¸ªæƒ³æ³•åˆ›å»ºFeasibilityCritiqueå¯¹è±¡
            critiques = []
            for i, idea in enumerate(ideas):
                if i < len(assessments):
                    assessment = assessments[i]
                    dimension_scores = assessment.get("dimension_scores", {})
                    feasibility_score = sum(dimension_scores.values())
                    
                    critique = FeasibilityCritique(
                        idea_id=idea.id,
                        feasibility_score=feasibility_score,
                        potential_risks =assessment.get("potential_risks", []),
                        graph_checks={"type": "batch_comprehensive", "reasoning": assessment.get("reasoning", "")}
                    )
                else:
                    # é»˜è®¤è¯„ä¼°
                    critique = FeasibilityCritique(
                        idea_id=idea.id,
                        feasibility_score=6.0,
                        required_assets={"relevance": 1.5, "asset_availability": 1.5, "complexity": 1.5, "risk_assessment": 1.5},
                        potential_risks=["å¾…è¿›ä¸€æ­¥è¯„ä¼°"],
                        graph_checks={"type": "batch_comprehensive", "error": "æ‰¹é‡è¯„ä¼°è§£æå¤±è´¥"}
                    )
                critiques.append(critique)
            
            print(f"âœ… æ‰¹é‡ç»¼åˆå¯è¡Œæ€§è¯„ä¼°å®Œæˆ")
            return critiques
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡ç»¼åˆå¯è¡Œæ€§è¯„ä¼°å¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤è¯„ä¼°
            return [FeasibilityCritique(
                idea_id=idea.id,
                feasibility_score=6.0,
                dimension_scores={"relevance": 1.5, "asset_availability": 1.5, "complexity": 1.5, "risk_assessment": 1.5},
                required_resources=[],
                risks=["è¯„ä¼°å¤±è´¥"],
                graph_checks={"type": "batch_comprehensive", "error": str(e)}
            ) for idea in ideas]

    async def assess_feasibility(self, idea: CandidateIdea, graph: SemanticOpportunityGraph) -> FeasibilityCritique:
        """è¯„ä¼°å•ä¸ªæƒ³æ³•çš„å¯è¡Œæ€§ã€‚"""
        results = await self.assess_feasibility_batch([idea], graph)
        return results[0]
    
    async def assess_feasibility_batch(self, ideas: List[CandidateIdea], graph: SemanticOpportunityGraph) -> List[FeasibilityCritique]:
        """æ‰¹é‡è¯„ä¼°æƒ³æ³•çš„å¯è¡Œæ€§ã€‚"""
        import asyncio
        
        print(f"ğŸ”¬ å¼€å§‹æ‰¹é‡å¯è¡Œæ€§è¯„ä¼°ï¼š{len(ideas)}ä¸ªæƒ³æ³•")
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = []
        for idea in ideas:
            task = asyncio.create_task(self._assess_single_feasibility(idea, graph))
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        critiques = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"âš ï¸ æƒ³æ³•{i+1}å¯è¡Œæ€§è¯„ä¼°å¤±è´¥: {str(result)}")
                # åˆ›å»ºé»˜è®¤è¯„ä¼°ç»“æœ
                critiques.append(FeasibilityCritique(
                    idea_id=ideas[i].id,
                    feasibility_score=5.0,
                    relevance="è¯„ä¼°å¤±è´¥",
                    required_assets=[],
                    potential_risks=[{"risk": "è¯„ä¼°å¤±è´¥", "impact": "unknown", "probability": "unknown", "mitigation": "é‡æ–°è¯„ä¼°"}],
                    graph_checks={"error": str(result)}
                ))
            else:
                critiques.append(result)
        
        print(f"âœ… æ‰¹é‡å¯è¡Œæ€§è¯„ä¼°å®Œæˆï¼š{len(critiques)}ä¸ªç»“æœ")
        return critiques
    
    async def _assess_single_feasibility(self, idea: CandidateIdea, graph: SemanticOpportunityGraph) -> FeasibilityCritique:
        """åŸºäºLLMçš„æ™ºèƒ½å¯è¡Œæ€§è¯„å®¡ã€‚"""
        
        print(f"    ğŸ”¬ å¼€å§‹å¯¹æƒ³æ³• '{idea.title}' è¿›è¡Œå…¨é¢å¯è¡Œæ€§åˆ†æ")
        
        # æ”¶é›†å›¾è°±ä¸Šä¸‹æ–‡ä¿¡æ¯
        graph_context = self._collect_graph_context(graph)
        
        # æ”¶é›†æƒ³æ³•çš„å®Œæ•´ä¿¡æ¯
        idea_details = self._prepare_idea_details(idea)
        
        prompt = f"""
ä½œä¸ºç ”ç©¶å¯è¡Œæ€§ä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹ç ”ç©¶æƒ³æ³•è¿›è¡Œå…¨é¢çš„å¯è¡Œæ€§è¯„ä¼°ã€‚

**å¾…è¯„ä¼°æƒ³æ³•**ï¼š
{idea_details}

**ç ”ç©¶é¢†åŸŸèƒŒæ™¯**ï¼š
{graph_context}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œæ·±å…¥çš„å¯è¡Œæ€§åˆ†æï¼š

### 1. æŠ€æœ¯å¯è¡Œæ€§ (Technical Feasibility)
- æ‰€éœ€æŠ€æœ¯æ˜¯å¦æˆç†Ÿå¯é ï¼Ÿ
- æ˜¯å¦å­˜åœ¨æŠ€æœ¯å®ç°çš„å…³é”®æŒ‘æˆ˜ï¼Ÿ
- æŠ€æœ¯è·¯å¾„æ˜¯å¦æ¸…æ™°å¯è¡Œï¼Ÿ

### 2. èµ„æºå¯è¡Œæ€§ (Resource Feasibility)  
- æ‰€éœ€æ•°æ®é›†æ˜¯å¦å¯è·å¾—ï¼Ÿ
- è®¡ç®—èµ„æºéœ€æ±‚æ˜¯å¦åˆç†ï¼Ÿ
- äººåŠ›å’Œæ—¶é—´æˆæœ¬æ˜¯å¦å¯æ§ï¼Ÿ

### 3. æ–¹æ³•å¯è¡Œæ€§ (Methodological Feasibility)
- ç ”ç©¶æ–¹æ³•æ˜¯å¦ç§‘å­¦åˆç†ï¼Ÿ
- å®éªŒè®¾è®¡æ˜¯å¦å¯æ‰§è¡Œï¼Ÿ
- è¯„ä¼°æŒ‡æ ‡æ˜¯å¦æ°å½“ï¼Ÿ

### 4. é£é™©å¯æ§æ€§ (Risk Manageability)
- ä¸»è¦é£é™©ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ
- é£é™©æ˜¯å¦å¯ä»¥é¢„é˜²æˆ–ç¼“è§£ï¼Ÿ
- æ˜¯å¦æœ‰å¤‡é€‰æ–¹æ¡ˆï¼Ÿ

### 5. é¢†åŸŸç›¸å…³æ€§ (Domain Relevance)
- æƒ³æ³•æ˜¯å¦å¥‘åˆå½“å‰ç ”ç©¶çƒ­ç‚¹ï¼Ÿ
- æ˜¯å¦æœ‰å®é™…åº”ç”¨ä»·å€¼ï¼Ÿ
- å­¦æœ¯å½±å“åŠ›æ½œåŠ›å¦‚ä½•ï¼Ÿ

è¯·åŸºäºä½ çš„åˆ†æç»™å‡ºè¯„åˆ†ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›è¯¦ç»†ç»“æœï¼š
{{
  "feasibility_score": X.X,  // æ€»ä½“å¯è¡Œæ€§è¯„åˆ† (1-10åˆ†ï¼Œ10åˆ†æœ€å¯è¡Œ)
  "dimension_scores": {{
    "technical": X.X,      // æŠ€æœ¯å¯è¡Œæ€§è¯„åˆ†
    "resource": X.X,       // èµ„æºå¯è¡Œæ€§è¯„åˆ†  
    "methodological": X.X, // æ–¹æ³•å¯è¡Œæ€§è¯„åˆ†
    "risk": X.X,          // é£é™©å¯æ§æ€§è¯„åˆ†
    "relevance": X.X      // é¢†åŸŸç›¸å…³æ€§è¯„åˆ†
  }},
  "detailed_analysis": {{
    "technical_analysis": "æŠ€æœ¯å¯è¡Œæ€§è¯¦ç»†åˆ†æ",
    "resource_analysis": "èµ„æºå¯è¡Œæ€§è¯¦ç»†åˆ†æ",
    "methodological_analysis": "æ–¹æ³•å¯è¡Œæ€§è¯¦ç»†åˆ†æ",
    "risk_analysis": "é£é™©è¯„ä¼°è¯¦ç»†åˆ†æ",
    "relevance_analysis": "ç›¸å…³æ€§è¯¦ç»†åˆ†æ"
  }},
  "required_assets": [
    {{
      "type": "dataset",
      "name": "æ‰€éœ€æ•°æ®é›†åç§°",
      "availability": "public/restricted/need_creation",
      "difficulty": "è·å–/åˆ›å»ºéš¾åº¦è¯„ä¼°"
    }},
    {{
      "type": "compute",
      "name": "è®¡ç®—èµ„æºéœ€æ±‚",
      "specification": "å…·ä½“è§„æ ¼è¦æ±‚",
      "cost_estimate": "æˆæœ¬ä¼°ç®—"
    }}
  ],
  "potential_risks": [
    {{
      "risk": "é£é™©æè¿°",
      "impact": "high/medium/low",
      "probability": "high/medium/low",
      "mitigation": "ç¼“è§£ç­–ç•¥"
    }}
  ],
  "strengths": ["å¯è¡Œæ€§ä¼˜åŠ¿1", "å¯è¡Œæ€§ä¼˜åŠ¿2"],
  "weaknesses": ["å¯è¡Œæ€§æŒ‘æˆ˜1", "å¯è¡Œæ€§æŒ‘æˆ˜2"],
  "recommendations": [
    "æ”¹è¿›å»ºè®®1ï¼šå…·ä½“å»ºè®®",
    "æ”¹è¿›å»ºè®®2ï¼šå…·ä½“å»ºè®®"
  ],
  "timeline_estimate": "é¢„ä¼°ç ”ç©¶å‘¨æœŸ",
  "success_probability": "æˆåŠŸæ¦‚ç‡è¯„ä¼°"
}}
"""

        try:
            response_data = await self.llm.generate(
                model_name=self.config.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=15000,
                agent_name=self.name,
                task_type="feasibility_assessment"
            )
            response = response_data.get("content", "")
            
            # è§£æLLMå“åº”
            import json
            import re
            try:
                if "```json" in response:
                    json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
                    if json_match:
                        json_content = json_match.group(1).strip()
                    else:
                        json_start = response.find("```json") + 7
                        json_content = response[json_start:].strip()
                        if json_content.endswith("```"):
                            json_content = json_content[:-3].strip()
                else:
                    json_content = response.strip()
                
                result = json.loads(json_content)
                
                # åˆ›å»ºFeasibilityCritiqueå¯¹è±¡
                critique = FeasibilityCritique(
                    idea_id=idea.id,
                    feasibility_score=float(result.get("feasibility_score", 6.0)),
                    required_assets=result.get("required_assets", []),
                    potential_risks=result.get("potential_risks", []),
                    graph_checks={
                        "approach": "llm_comprehensive_analysis",
                        "dimensions": list(self.feasibility_dimensions),
                        "detailed_analysis": result.get("detailed_analysis", {}),
                        "dimension_scores": result.get("dimension_scores", {}),
                        "strengths": result.get("strengths", []),
                        "weaknesses": result.get("weaknesses", []),
                        "recommendations": result.get("recommendations", []),
                        "timeline_estimate": result.get("timeline_estimate", "æœªä¼°ç®—"),
                        "success_probability": result.get("success_probability", "æœªè¯„ä¼°")
                    }
                )
                
                print(f"    âœ… å¯è¡Œæ€§è¯„åˆ†: {critique.feasibility_score:.2f}/10.0")
                return critique
                
            except json.JSONDecodeError:
                print(f"    âš ï¸ LLMå¯è¡Œæ€§åˆ†æå“åº”è§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨è¯„ä¼°")
                return self._create_fallback_critique(idea)
                
        except Exception as e:
            print(f"    âŒ LLMå¯è¡Œæ€§åˆ†æå¤±è´¥: {str(e)}ï¼Œä½¿ç”¨å¤‡ç”¨è¯„ä¼°")
            return self._create_fallback_critique(idea)
    
    def _prepare_idea_details(self, idea: CandidateIdea) -> str:
        """å‡†å¤‡æƒ³æ³•çš„è¯¦ç»†ä¿¡æ¯ç”¨äºLLMåˆ†æã€‚"""
        details = []
        details.append(f"- æ ‡é¢˜: {idea.title}")
        details.append(f"- æ ¸å¿ƒå‡è®¾: {idea.core_hypothesis}")
        details.append(f"- åˆ›æ–°ç‚¹: {', '.join(idea.initial_innovation_points)}")
        
        if hasattr(idea, 'expected_contribution') and idea.expected_contribution:
            details.append(f"- é¢„æœŸè´¡çŒ®: {', '.join(idea.expected_contribution)}")
        
        if hasattr(idea, 'preliminary_experiments') and idea.preliminary_experiments:
            experiments = [exp.get('name', str(exp)) if isinstance(exp, dict) else str(exp) for exp in idea.preliminary_experiments]
            details.append(f"- åˆæ­¥å®éªŒè®¾è®¡: {', '.join(experiments)}")
        
        if hasattr(idea, 'required_assets') and idea.required_assets:
            assets = [asset.get('type', str(asset)) if isinstance(asset, dict) else str(asset) for asset in idea.required_assets]
            details.append(f"- æ‰€éœ€èµ„äº§: {', '.join(assets)}")
        
        if hasattr(idea, 'risks') and idea.risks:
            details.append(f"- å·²è¯†åˆ«é£é™©: {', '.join(idea.risks)}")
        
        return "\n".join(details)
    
    def _create_fallback_critique(self, idea: CandidateIdea) -> FeasibilityCritique:
        """åˆ›å»ºå¤‡ç”¨çš„å¯è¡Œæ€§è¯„å®¡ç»“æœã€‚"""
        return FeasibilityCritique(
            idea_id=idea.id,
            feasibility_score=6.5,  # ä¸­ç­‰åä¸Šåˆ†æ•°
            required_assets=[{"type": "unknown", "availability": "éœ€è¦è¿›ä¸€æ­¥è¯„ä¼°"}],
            potential_risks=[{"risk": "è¯„ä¼°å¤±è´¥ï¼Œéœ€è¦äººå·¥å¤æ ¸", "impact": "medium"}],
            graph_checks={
                "approach": "fallback_analysis",
                "note": "LLMåˆ†æå¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–è¯„ä¼°"
            }
        )

    async def _assess_relevance(self, idea: CandidateIdea, graph: SemanticOpportunityGraph) -> float:
        """åŸºäºLLMçš„é¢†åŸŸç›¸å…³æ€§è¯„ä¼°ã€‚"""
        print(f"    ğŸ¯ è¯„ä¼°æƒ³æ³• '{idea.title}' çš„é¢†åŸŸç›¸å…³æ€§")
        
        # æ”¶é›†å›¾è°±ä¸Šä¸‹æ–‡ä¿¡æ¯
        graph_context = self._collect_graph_context(graph)
        
        prompt = f"""
ä½œä¸ºå­¦æœ¯ä¸“å®¶ï¼Œè¯·è¯„ä¼°ä»¥ä¸‹ç ”ç©¶æƒ³æ³•ä¸å½“å‰ç ”ç©¶é¢†åŸŸçš„ç›¸å…³æ€§ã€‚

**å¾…è¯„ä¼°æƒ³æ³•**ï¼š
- æ ‡é¢˜: {idea.title}
- æ ¸å¿ƒå‡è®¾: {idea.core_hypothesis}
- åˆ›æ–°ç‚¹: {', '.join(idea.initial_innovation_points)}
- è§¦å‘èŠ‚ç‚¹: {', '.join(idea.source_trigger_nodes) if hasattr(idea, 'source_trigger_nodes') and idea.source_trigger_nodes else 'æ— '}

**å½“å‰ç ”ç©¶é¢†åŸŸèƒŒæ™¯**ï¼š
{graph_context}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°æƒ³æ³•çš„é¢†åŸŸç›¸å…³æ€§ï¼š

1. **ç ”ç©¶ä¸»é¢˜åŒ¹é…åº¦**: æƒ³æ³•æ˜¯å¦ç¬¦åˆå½“å‰é¢†åŸŸçš„ç ”ç©¶çƒ­ç‚¹å’Œè¶‹åŠ¿ï¼Ÿ
2. **æŠ€æœ¯æ ˆå¥‘åˆåº¦**: æ‰€æ¶‰åŠçš„æŠ€æœ¯å’Œæ–¹æ³•æ˜¯å¦ä¸é¢†åŸŸä¸»æµæŠ€æœ¯ç›¸ç¬¦ï¼Ÿ
3. **é—®é¢˜é‡è¦æ€§**: æƒ³æ³•è§£å†³çš„é—®é¢˜æ˜¯å¦æ˜¯é¢†åŸŸå†…çš„é‡è¦é—®é¢˜ï¼Ÿ
4. **å­¦æœ¯ä»·å€¼**: æƒ³æ³•æ˜¯å¦å…·æœ‰é‡è¦çš„å­¦æœ¯æ„ä¹‰å’Œå½±å“æ½œåŠ›ï¼Ÿ

è¯·ä»¥JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼š
{{
  "relevance_score": 8.2,  // ç›¸å…³æ€§è¯„åˆ† (1-10åˆ†ï¼Œ10åˆ†æœ€ç›¸å…³)
  "analysis": {{
    "topic_match": "ç ”ç©¶ä¸»é¢˜åŒ¹é…åº¦åˆ†æ",
    "tech_alignment": "æŠ€æœ¯æ ˆå¥‘åˆåº¦åˆ†æ", 
    "problem_importance": "é—®é¢˜é‡è¦æ€§åˆ†æ",
    "academic_value": "å­¦æœ¯ä»·å€¼åˆ†æ"
  }},
  "strengths": ["ç›¸å…³æ€§ä¼˜åŠ¿1", "ç›¸å…³æ€§ä¼˜åŠ¿2"],
  "concerns": ["å¯èƒ½çš„ç›¸å…³æ€§é—®é¢˜1", "å¯èƒ½çš„ç›¸å…³æ€§é—®é¢˜2"],
  "recommendations": ["æ”¹è¿›å»ºè®®1", "æ”¹è¿›å»ºè®®2"]
}}
"""

        try:
            response_data = await self.llm.generate(
                model_name=self.config.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=15000,
                agent_name=self.name,
                task_type="feasibility_assessment"
            )
            response = response_data.get("content", "")
            
            # è§£æLLMå“åº”
            import json
            import re
            try:
                if "```json" in response:
                    json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
                    if json_match:
                        json_content = json_match.group(1).strip()
                    else:
                        json_start = response.find("```json") + 7
                        json_content = response[json_start:].strip()
                        if json_content.endswith("```"):
                            json_content = json_content[:-3].strip()
                else:
                    json_content = response.strip()
                
                result = json.loads(json_content)
                final_score = float(result.get("relevance_score", 6.0))
                
                print(f"    âœ… ç›¸å…³æ€§è¯„åˆ†: {final_score:.2f}/10.0")
                return final_score
                
            except json.JSONDecodeError:
                print(f"    âš ï¸ LLMç›¸å…³æ€§è¯„ä¼°å“åº”è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†")
                return 6.0
                
        except Exception as e:
            print(f"    âŒ LLMç›¸å…³æ€§è¯„ä¼°å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†")
            return 6.0
    
    def _collect_graph_context(self, graph: SemanticOpportunityGraph) -> str:
        """æ”¶é›†å›¾è°±çš„é¢†åŸŸèƒŒæ™¯ä¿¡æ¯ã€‚"""
        context_lines = []
        
        # ç»Ÿè®¡èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ
        methods = find_nodes_by_type(graph, GraphNodeType.METHOD)
        tasks = find_nodes_by_type(graph, GraphNodeType.TASK)
        datasets = find_nodes_by_type(graph, GraphNodeType.DATASET)
        metrics = find_nodes_by_type(graph, GraphNodeType.METRIC)
        
        context_lines.append(f"ç ”ç©¶é¢†åŸŸç»Ÿè®¡: {len(methods)}ä¸ªæ–¹æ³•, {len(tasks)}ä¸ªä»»åŠ¡, {len(datasets)}ä¸ªæ•°æ®é›†, {len(metrics)}ä¸ªæŒ‡æ ‡")
        
        # ä¸»è¦æ–¹æ³•
        if methods:
            top_methods = []
            for method_id in methods[:5]:
                if method_id in graph.nodes():
                    method_name = graph.nodes[method_id].get('name', method_id)
                    salience = graph.nodes[method_id].get('salience', 0.0)
                    top_methods.append(f"{method_name}(é‡è¦æ€§:{salience:.2f})")
            context_lines.append(f"ä¸»è¦æ–¹æ³•: {', '.join(top_methods)}")
        
        # ä¸»è¦ä»»åŠ¡
        if tasks:
            top_tasks = []
            for task_id in tasks[:5]:
                if task_id in graph.nodes():
                    task_name = graph.nodes[task_id].get('name', task_id)
                    salience = graph.nodes[task_id].get('salience', 0.0)
                    top_tasks.append(f"{task_name}(é‡è¦æ€§:{salience:.2f})")
            context_lines.append(f"ä¸»è¦ä»»åŠ¡: {', '.join(top_tasks)}")
        
        return "\n".join(context_lines) if context_lines else "æ— æ³•è·å–å›¾è°±èƒŒæ™¯ä¿¡æ¯"
    
    async def _evaluate_node_relevance(self, idea: CandidateIdea, graph: SemanticOpportunityGraph) -> float:
        """è¯„ä¼°è§¦å‘èŠ‚ç‚¹çš„å›¾è°±ç›¸å…³æ€§ã€‚"""
        
        if not hasattr(idea, 'source_trigger_nodes') or not idea.source_trigger_nodes:
            return 5.0  # é»˜è®¤ä¸­ç­‰ç›¸å…³æ€§
        
        node_scores = []
        
        for node_id in idea.source_trigger_nodes:
            if node_id in graph.nodes():
                # èŠ‚ç‚¹å­˜åœ¨ï¼Œè¯„ä¼°å…¶é‡è¦æ€§
                node_data = graph.nodes[node_id]
                salience = node_data.get('salience', 0.5)
                node_type = node_data.get('type', 'Unknown')
                
                # æ ¹æ®èŠ‚ç‚¹ç±»å‹è°ƒæ•´æƒé‡
                type_weights = {
                    'Method': 1.0,
                    'Task': 0.9,
                    'Dataset': 0.8,
                    'Metric': 0.7,
                    'Paper': 0.6,
                    'Domain': 0.8
                }
                
                type_weight = type_weights.get(node_type, 0.5)
                node_score = salience * type_weight * 10.0
                node_scores.append(node_score)
            else:
                # èŠ‚ç‚¹ä¸å­˜åœ¨ï¼Œé™ä½ç›¸å…³æ€§
                node_scores.append(2.0)
        
        if node_scores:
            return sum(node_scores) / len(node_scores)
        else:
            return 5.0
    
    async def _evaluate_domain_relevance(self, idea: CandidateIdea, graph: SemanticOpportunityGraph) -> float:
        """è¯„ä¼°é¢†åŸŸåŒ¹é…åº¦ã€‚"""
        
        # ä»å›¾è°±ä¸­æå–ä¸»å¯¼é¢†åŸŸ
        domain_nodes = find_nodes_by_type(graph, GraphNodeType.DOMAIN)
        
        if not domain_nodes:
            return 7.0  # å¦‚æœæ²¡æœ‰æ˜ç¡®é¢†åŸŸèŠ‚ç‚¹ï¼Œç»™äºˆè¾ƒé«˜ç›¸å…³æ€§
        
        # ä»æƒ³æ³•ä¸­æå–é¢†åŸŸä¿¡æ¯
        idea_text = idea.title + " " + idea.core_hypothesis
        idea_domains = self._extract_domains_from_text(idea_text)
        
        # è®¡ç®—é¢†åŸŸé‡å åº¦
        graph_domains = []
        for domain_id in domain_nodes:
            domain_name = graph.nodes[domain_id].get('name', domain_id)
            graph_domains.append(domain_name.lower())
        
        if not idea_domains or not graph_domains:
            return 6.0
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        overlap_count = 0
        for idea_domain in idea_domains:
            for graph_domain in graph_domains:
                if idea_domain in graph_domain or graph_domain in idea_domain:
                    overlap_count += 1
                    break
        
        # è®¡ç®—åŒ¹é…åº¦åˆ†æ•°
        if len(idea_domains) > 0:
            match_ratio = overlap_count / len(idea_domains)
            return 4.0 + match_ratio * 6.0  # 4-10åˆ†èŒƒå›´
        else:
            return 6.0
    
    def _extract_domains_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–é¢†åŸŸå…³é”®è¯ã€‚"""
        import re
        
        domain_patterns = [
            r'\b(?:computer vision|natural language|speech|robotics|healthcare|finance|security)\b',
            r'\b(?:machine learning|deep learning|reinforcement learning|transfer learning)\b',
            r'\b(?:neural networks|optimization|data mining|information retrieval)\b',
            r'\b(?:classification|detection|generation|translation|recommendation|prediction)\b'
        ]
        
        domains = []
        text_lower = text.lower()
        
        for pattern in domain_patterns:
            matches = re.findall(pattern, text_lower)
            domains.extend(matches)
        
        return list(set(domains))
    
    async def _evaluate_academic_relevance(self, idea: CandidateIdea) -> float:
        """è¯„ä¼°å­¦æœ¯ä»·å€¼å’Œç ”ç©¶æ„ä¹‰ã€‚"""
        
        # ç®€åŒ–å®ç°ï¼šåŸºäºæƒ³æ³•ç»“æ„çš„å®Œæ•´æ€§è¯„ä¼°
        score = 5.0  # åŸºç¡€åˆ†
        
        # å› å­1ï¼šåˆ›æ–°ç‚¹æ•°é‡å’Œè´¨é‡
        innovation_count = len(idea.initial_innovation_points)
        if innovation_count >= 3:
            score += 1.5
        elif innovation_count >= 2:
            score += 1.0
        elif innovation_count >= 1:
            score += 0.5
        
        # å› å­2ï¼šé¢„æœŸè´¡çŒ®çš„å¤šæ ·æ€§
        contribution_count = len(idea.expected_contribution)
        if contribution_count >= 2:
            score += 1.0
        elif contribution_count >= 1:
            score += 0.5
        
        # å› å­3ï¼šå®éªŒè®¾è®¡çš„å®Œæ•´æ€§
        experiment_count = len(idea.preliminary_experiments)
        if experiment_count >= 2:
            score += 1.0
        elif experiment_count >= 1:
            score += 0.5
        
        # å› å­4ï¼šæ ¸å¿ƒå‡è®¾çš„æ¸…æ™°åº¦
        hypothesis_length = len(idea.core_hypothesis.strip())
        if hypothesis_length >= 100:
            score += 1.0
        elif hypothesis_length >= 50:
            score += 0.5
        
        return min(10.0, score)

    async def _analyze_required_assets(self, idea: CandidateIdea, graph: SemanticOpportunityGraph) -> Dict[str, Any]:
        """åˆ†ææ‰€éœ€èµ„äº§çš„å¯å¾—æ€§ã€‚
        
        è¾“å…¥:
            - idea: å€™é€‰æƒ³æ³•ã€‚
            - graph: è¯­ä¹‰æœºä¼šå›¾è°±ã€‚
            
        è¾“å‡º:
            - Dict: èµ„äº§åˆ†æç»“æœï¼ŒåŒ…å«å¯å¾—èµ„äº§åˆ—è¡¨ã€ç¼ºå¤±èµ„äº§ã€æ›¿ä»£æ–¹æ¡ˆã€‚
            
        å®ç°æ€è·¯:
            1) ä»idea.required_assetså’Œå›¾è°±ä¸­è¯†åˆ«æ‰€éœ€çš„æ•°æ®é›†ã€ç®—åŠ›ã€ä»£ç ç­‰èµ„æºã€‚
            2) æ£€æŸ¥èµ„æºåœ¨å›¾è°±ä¸­çš„å¯å¾—æ€§æ ‡è®°ï¼ŒæŸ¥è¯¢å¤–éƒ¨æ•°æ®åº“ç¡®è®¤å…¬å¼€æ€§ã€‚
            3) ä¸ºç¼ºå¤±èµ„æºæå‡ºæ›¿ä»£æ–¹æ¡ˆï¼šåˆæˆæ•°æ®ã€ä»£ç†ä»»åŠ¡ã€å¼€æºå®ç°ç­‰ã€‚
        """
        print(f"    ğŸ“‹ åˆ†ææ‰€éœ€èµ„äº§çš„å¯å¾—æ€§")
        
        # æ”¶é›†æ‰€æœ‰æ‰€éœ€èµ„äº§
        all_required_assets = []
        
        # æ¥æº1ï¼šæƒ³æ³•ä¸­æ˜ç¡®åˆ—å‡ºçš„èµ„äº§
        if hasattr(idea, 'required_assets') and idea.required_assets:
            all_required_assets.extend(idea.required_assets)
        
        # æ¥æº2ï¼šä»æƒ³æ³•å†…å®¹æ¨æ–­çš„éšå«èµ„äº§
        inferred_assets = await self._infer_assets_from_idea(idea)
        all_required_assets.extend(inferred_assets)
        
        # åˆ†ææ¯ä¸ªèµ„äº§çš„å¯å¾—æ€§
        available_assets = []
        missing_assets = []
        
        for asset in all_required_assets:
            availability_analysis = await self._analyze_single_asset(asset, graph)
            
            if availability_analysis['available']:
                available_assets.append(availability_analysis)
            else:
                missing_assets.append(availability_analysis)
        
        # ä¸ºç¼ºå¤±èµ„äº§ç”Ÿæˆæ›¿ä»£æ–¹æ¡ˆ
        alternatives = await self._generate_asset_alternatives(missing_assets)
        
        # è®¡ç®—æ•´ä½“å¯å¾—æ€§åˆ†æ•°
        overall_availability = await self._calculate_asset_availability_score(available_assets, missing_assets, alternatives)
        
        print(f"    âœ… èµ„äº§åˆ†æå®Œæˆ: {len(available_assets)} å¯å¾—, {len(missing_assets)} ç¼ºå¤±")
        
        return {
            "available_assets": available_assets,
            "missing_assets": missing_assets,
            "alternatives": alternatives,
            "overall_availability_score": overall_availability,
            "total_assets_analyzed": len(all_required_assets)
        }
    
    async def _infer_assets_from_idea(self, idea: CandidateIdea) -> List[Dict[str, Any]]:
        """ä»æƒ³æ³•å†…å®¹æ¨æ–­éšå«çš„æ‰€éœ€èµ„äº§ã€‚"""
        
        inferred_assets = []
        
        # ä»å®éªŒè®¾è®¡ä¸­æ¨æ–­æ•°æ®é›†éœ€æ±‚
        for exp in idea.preliminary_experiments:
            dataset = exp.get('dataset')
            if dataset and dataset != 'to_be_determined':
                inferred_assets.append({
                    'type': 'Dataset',
                    'id': dataset,
                    'availability': 'to_be_verified',
                    'source': 'experiment_design'
                })
        
        # ä»åˆ›æ–°ç‚¹ä¸­æ¨æ–­æ–¹æ³•å’ŒæŠ€æœ¯éœ€æ±‚
        for point in idea.initial_innovation_points:
            # ç®€åŒ–æ¨æ–­ï¼šå¯»æ‰¾å¸¸è§çš„æŠ€æœ¯æœ¯è¯­
            tech_terms = self._extract_technology_requirements(point)
            for term in tech_terms:
                inferred_assets.append({
                    'type': 'Technology',
                    'id': term,
                    'availability': 'to_be_verified',
                    'source': 'innovation_points'
                })
        
        # åŸºäºæƒ³æ³•å¤æ‚åº¦æ¨æ–­è®¡ç®—èµ„æºéœ€æ±‚
        complexity_score = self._estimate_computational_complexity(idea)
        if complexity_score > 7:
            inferred_assets.append({
                'type': 'ComputeResource',
                'id': 'high_performance_gpu',
                'availability': 'institutional_dependent',
                'source': 'complexity_analysis'
            })
        elif complexity_score > 4:
            inferred_assets.append({
                'type': 'ComputeResource',
                'id': 'standard_gpu',
                'availability': 'widely_available',
                'source': 'complexity_analysis'
            })
        
        return inferred_assets
    
    def _extract_technology_requirements(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–æŠ€æœ¯éœ€æ±‚ã€‚"""
        import re
        
        tech_patterns = [
            r'\b(?:transformer|attention|lstm|cnn|bert|gpt|resnet|vgg|alexnet)\b',
            r'\b(?:pytorch|tensorflow|keras|huggingface|openai|anthropic)\b',
            r'\b(?:cuda|gpu|distributed|parallel|cloud)\b'
        ]
        
        technologies = []
        text_lower = text.lower()
        
        for pattern in tech_patterns:
            matches = re.findall(pattern, text_lower)
            technologies.extend(matches)
        
        return list(set(technologies))
    
    def _estimate_computational_complexity(self, idea: CandidateIdea) -> float:
        """ä¼°ç®—æƒ³æ³•çš„è®¡ç®—å¤æ‚åº¦ã€‚"""
        
        complexity_score = 3.0  # åŸºç¡€åˆ†
        
        # å› å­1ï¼šæ–¹æ³•å¤æ‚åº¦
        complex_methods = ['transformer', 'attention', 'neural', 'deep', 'reinforcement', 'meta']
        idea_text = (idea.title + " " + idea.core_hypothesis + " " + 
                    " ".join(idea.initial_innovation_points)).lower()
        
        for method in complex_methods:
            if method in idea_text:
                complexity_score += 1.0
        
        # å› å­2ï¼šæ•°æ®è§„æ¨¡
        large_data_indicators = ['large-scale', 'massive', 'big data', 'billion', 'million']
        for indicator in large_data_indicators:
            if indicator in idea_text:
                complexity_score += 1.5
        
        # å› å­3ï¼šå¤šæ¨¡æ€æˆ–å¤šä»»åŠ¡
        multi_indicators = ['multi-modal', 'multi-task', 'cross-domain', 'transfer']
        for indicator in multi_indicators:
            if indicator in idea_text:
                complexity_score += 1.0
        
        return min(10.0, complexity_score)
    
    async def _analyze_single_asset(self, asset: Dict[str, Any], graph: SemanticOpportunityGraph) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªèµ„äº§çš„å¯å¾—æ€§ã€‚"""
        
        asset_type = asset.get('type', 'Unknown')
        asset_id = asset.get('id', 'unknown')
        stated_availability = asset.get('availability', 'unknown')
        
        analysis = {
            'type': asset_type,
            'id': asset_id,
            'stated_availability': stated_availability,
            'available': False,
            'confidence': 0.5,
            'notes': []
        }
        
        # æ£€æŸ¥å›¾è°±ä¸­çš„èµ„äº§ä¿¡æ¯
        graph_availability = await self._check_asset_in_graph(asset_id, graph)
        if graph_availability:
            analysis['available'] = True
            analysis['confidence'] = 0.8
            analysis['notes'].append(f"åœ¨å›¾è°±ä¸­æ‰¾åˆ°ç›¸å…³èŠ‚ç‚¹")
        
        # åŸºäºå£°æ˜çš„å¯å¾—æ€§è¿›è¡Œè¯„ä¼°
        availability_scores = {
            'public': (True, 0.9),
            'open_source': (True, 0.9),
            'widely_available': (True, 0.8),
            'institutional_available': (True, 0.7),
            'commercial': (True, 0.6),
            'restricted': (False, 0.3),
            'proprietary': (False, 0.2),
            'unavailable': (False, 0.1),
            'to_be_determined': (False, 0.4),
            'unknown': (False, 0.5)
        }
        
        if stated_availability in availability_scores:
            available, confidence = availability_scores[stated_availability]
            analysis['available'] = available
            analysis['confidence'] = max(analysis['confidence'], confidence)
        
        # åŸºäºèµ„äº§ç±»å‹çš„é»˜è®¤è¯„ä¼°
        type_defaults = {
            'Dataset': (True, 0.7),  # å¤§å¤šæ•°æ•°æ®é›†ç›¸å¯¹å¯å¾—
            'Method': (True, 0.8),   # æ–¹æ³•é€šå¸¸æœ‰å¼€æºå®ç°
            'Technology': (True, 0.6), # æŠ€æœ¯å¯å¾—æ€§å˜åŒ–è¾ƒå¤§
            'ComputeResource': (True, 0.5), # è®¡ç®—èµ„æºä¾èµ–æœºæ„
            'Tool': (True, 0.8),     # å·¥å…·é€šå¸¸å¼€æº
            'Library': (True, 0.9)   # åº“æ–‡ä»¶é«˜åº¦å¯å¾—
        }
        
        if asset_type in type_defaults and analysis['confidence'] < 0.6:
            default_available, default_confidence = type_defaults[asset_type]
            if not analysis['available']:  # åªåœ¨å½“å‰è¯„ä¼°ä¸ºä¸å¯å¾—æ—¶åº”ç”¨é»˜è®¤å€¼
                analysis['available'] = default_available
                analysis['confidence'] = default_confidence
        
        return analysis
    
    async def _check_asset_in_graph(self, asset_id: str, graph: SemanticOpportunityGraph) -> bool:
        """æ£€æŸ¥èµ„äº§æ˜¯å¦åœ¨å›¾è°±ä¸­å­˜åœ¨ã€‚"""
        
        # ç›´æ¥IDåŒ¹é…
        if asset_id in graph.nodes():
            return True
        
        # åç§°åŒ¹é…
        for node_id, node_data in graph.nodes(data=True):
            node_name = node_data.get('name', '').lower()
            if asset_id.lower() in node_name or node_name in asset_id.lower():
                return True
        
        # åˆ«ååŒ¹é…
        for node_id, node_data in graph.nodes(data=True):
            aliases = node_data.get('aliases', [])
            for alias in aliases:
                if asset_id.lower() in alias.lower() or alias.lower() in asset_id.lower():
                    return True
        
        return False
    
    async def _generate_asset_alternatives(self, missing_assets: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä¸ºç¼ºå¤±èµ„äº§ç”Ÿæˆæ›¿ä»£æ–¹æ¡ˆã€‚"""
        
        alternatives = []
        
        for asset in missing_assets:
            asset_type = asset['type']
            asset_id = asset['id']
            
            if asset_type == 'Dataset':
                # æ•°æ®é›†æ›¿ä»£æ–¹æ¡ˆ
                alternatives.extend([
                    {
                        'for_asset': asset_id,
                        'alternative_type': 'synthetic_dataset',
                        'description': f'ä¸º{asset_id}ç”Ÿæˆåˆæˆæ•°æ®é›†',
                        'feasibility': 0.7,
                        'effort': 'medium'
                    },
                    {
                        'for_asset': asset_id,
                        'alternative_type': 'similar_dataset',
                        'description': f'ä½¿ç”¨ä¸{asset_id}ç›¸ä¼¼çš„å…¬å¼€æ•°æ®é›†',
                        'feasibility': 0.8,
                        'effort': 'low'
                    }
                ])
            
            elif asset_type == 'Method':
                # æ–¹æ³•æ›¿ä»£æ–¹æ¡ˆ
                alternatives.append({
                    'for_asset': asset_id,
                    'alternative_type': 'open_source_implementation',
                    'description': f'å¯»æ‰¾{asset_id}çš„å¼€æºå®ç°',
                    'feasibility': 0.8,
                    'effort': 'low'
                })
            
            elif asset_type == 'ComputeResource':
                # è®¡ç®—èµ„æºæ›¿ä»£æ–¹æ¡ˆ
                alternatives.extend([
                    {
                        'for_asset': asset_id,
                        'alternative_type': 'cloud_platform',
                        'description': f'ä½¿ç”¨äº‘è®¡ç®—å¹³å°æä¾›{asset_id}',
                        'feasibility': 0.9,
                        'effort': 'low'
                    },
                    {
                        'for_asset': asset_id,
                        'alternative_type': 'model_optimization',
                        'description': f'é€šè¿‡æ¨¡å‹ä¼˜åŒ–å‡å°‘å¯¹{asset_id}çš„éœ€æ±‚',
                        'feasibility': 0.6,
                        'effort': 'high'
                    }
                ])
            
            else:
                # é€šç”¨æ›¿ä»£æ–¹æ¡ˆ
                alternatives.append({
                    'for_asset': asset_id,
                    'alternative_type': 'equivalent_alternative',
                    'description': f'å¯»æ‰¾{asset_id}çš„ç­‰ä»·æ›¿ä»£å“',
                    'feasibility': 0.6,
                    'effort': 'medium'
                })
        
        return alternatives
    
    async def _calculate_asset_availability_score(self, available_assets: List[Dict[str, Any]], 
                                                missing_assets: List[Dict[str, Any]], 
                                                alternatives: List[Dict[str, Any]]) -> float:
        """è®¡ç®—æ•´ä½“èµ„äº§å¯å¾—æ€§åˆ†æ•°ã€‚"""
        
        total_assets = len(available_assets) + len(missing_assets)
        
        if total_assets == 0:
            return 8.0  # å¦‚æœæ²¡æœ‰æ˜ç¡®èµ„äº§éœ€æ±‚ï¼Œç»™äºˆè¾ƒé«˜åˆ†æ•°
        
        # åŸºç¡€åˆ†æ•°ï¼šç›´æ¥å¯å¾—çš„èµ„äº§
        available_score = 0.0
        for asset in available_assets:
            confidence = asset.get('confidence', 0.5)
            available_score += confidence
        
        # æ›¿ä»£æ–¹æ¡ˆçš„åˆ†æ•°
        alternative_score = 0.0
        for alternative in alternatives:
            feasibility = alternative.get('feasibility', 0.5)
            effort_multiplier = {'low': 1.0, 'medium': 0.8, 'high': 0.6}.get(alternative.get('effort', 'medium'), 0.7)
            alternative_score += feasibility * effort_multiplier
        
        # ç»¼åˆè®¡ç®—
        total_effective_score = available_score + alternative_score * 0.7  # æ›¿ä»£æ–¹æ¡ˆæƒé‡é™ä½
        max_possible_score = total_assets * 1.0
        
        if max_possible_score > 0:
            normalized_score = (total_effective_score / max_possible_score) * 10.0
        else:
            normalized_score = 8.0
        
        return max(0.0, min(10.0, normalized_score))

    async def _assess_risks_and_complexity(self, idea: CandidateIdea, graph: SemanticOpportunityGraph) -> Dict[str, Any]:
        """è¯„ä¼°å®ç°é£é™©ä¸å¤æ‚åº¦ã€‚
        
        è¾“å…¥:
            - idea: å€™é€‰æƒ³æ³•ã€‚
            - graph: è¯­ä¹‰æœºä¼šå›¾è°±ã€‚
            
        è¾“å‡º:
            - Dict: é£é™©åˆ†æç»“æœï¼ŒåŒ…å«é£é™©ç±»å‹ã€ä¸¥é‡ç¨‹åº¦ã€ç¼“è§£ç­–ç•¥ã€‚
            
        å®ç°æ€è·¯:
            1) æŠ€æœ¯é£é™©ï¼šæ–¹æ³•å¤æ‚åº¦ã€å®ç°éš¾åº¦ã€è°ƒå‚æ•æ„Ÿæ€§ã€‚
            2) æ•°æ®é£é™©ï¼šæ•°æ®è´¨é‡ã€åˆ†å¸ƒåç§»ã€æ ‡æ³¨æˆæœ¬ã€‚
            3) èµ„æºé£é™©ï¼šè®¡ç®—å¼€é”€ã€æ—¶é—´é¢„ç®—ã€äººåŠ›éœ€æ±‚ã€‚
            4) ç»“æœé£é™©ï¼šå¯é‡ç°æ€§ã€æ³›åŒ–èƒ½åŠ›ã€è¯„ä¼°å…¬å¹³æ€§ã€‚
        """
        print(f"    âš ï¸ è¯„ä¼°å®ç°é£é™©ä¸å¤æ‚åº¦")
        
        # ç®€åŒ–å®ç°ï¼šåŸºäºæƒ³æ³•å†…å®¹çš„é£é™©è¯„ä¼°
        overall_risk_score = await self._calculate_simplified_risk_score(idea)
        
        # è¯†åˆ«ä¸»è¦é£é™©ç±»å‹
        risk_types = await self._identify_main_risk_types(idea)
        
        # ç”Ÿæˆé£é™©ç¼“è§£å»ºè®®
        mitigation_strategies = await self._generate_basic_mitigation_strategies(risk_types)
        
        risk_analysis = {
            "overall_risk_score": overall_risk_score,
            "identified_risks": risk_types,
            "mitigation_strategies": mitigation_strategies,
            "risk_level": self._risk_level_from_score(overall_risk_score)
        }
        
        print(f"    âœ… é£é™©è¯„ä¼°å®Œæˆï¼Œæ•´ä½“é£é™©ç­‰çº§: {risk_analysis['risk_level']}")
        
        return risk_analysis
    
    async def _calculate_simplified_risk_score(self, idea: CandidateIdea) -> float:
        """è®¡ç®—ç®€åŒ–çš„é£é™©åˆ†æ•°ã€‚"""
        
        risk_score = 3.0  # åŸºç¡€é£é™©åˆ†æ•°
        
        # å› å­1ï¼šè®¡ç®—å¤æ‚åº¦
        complexity = self._estimate_computational_complexity(idea)
        if complexity > 8:
            risk_score += 2.0
        elif complexity > 6:
            risk_score += 1.0
        
        # å› å­2ï¼šåˆ›æ–°ç¨‹åº¦ï¼ˆåˆ›æ–°è¶Šé«˜ï¼Œé£é™©è¶Šå¤§ï¼‰
        innovation_count = len(idea.initial_innovation_points)
        if innovation_count > 3:
            risk_score += 1.5
        elif innovation_count > 2:
            risk_score += 1.0
        
        # å› å­3ï¼šå®éªŒå¤æ‚åº¦
        experiment_count = len(idea.preliminary_experiments)
        if experiment_count > 3:
            risk_score += 1.0
        
        # å› å­4ï¼šå·²çŸ¥é£é™©
        existing_risks = len(idea.risks) if hasattr(idea, 'risks') and idea.risks else 0
        risk_score += existing_risks * 0.5
        
        return max(0.0, min(10.0, risk_score))
    
    async def _identify_main_risk_types(self, idea: CandidateIdea) -> List[Dict[str, Any]]:
        """è¯†åˆ«ä¸»è¦é£é™©ç±»å‹ã€‚"""
        
        risks = []
        idea_text = (idea.title + " " + idea.core_hypothesis + " " + 
                    " ".join(idea.initial_innovation_points)).lower()
        
        # æŠ€æœ¯é£é™©
        if any(term in idea_text for term in ['complex', 'novel', 'advanced', 'sophisticated']):
            risks.append({
                "type": "technical_complexity",
                "severity": "medium",
                "description": "æŠ€æœ¯å®ç°å¤æ‚åº¦è¾ƒé«˜"
            })
        
        # æ•°æ®é£é™©
        if any(term in idea_text for term in ['data', 'dataset', 'training']):
            risks.append({
                "type": "data_dependency",
                "severity": "medium", 
                "description": "å¯¹æ•°æ®è´¨é‡å’Œå¯å¾—æ€§æœ‰è¾ƒé«˜ä¾èµ–"
            })
        
        # èµ„æºé£é™©
        complexity = self._estimate_computational_complexity(idea)
        if complexity > 6:
            risks.append({
                "type": "resource_intensive",
                "severity": "medium",
                "description": "è®¡ç®—èµ„æºéœ€æ±‚è¾ƒé«˜"
            })
        
        # ç»“æœé£é™©
        if len(idea.preliminary_experiments) < 2:
            risks.append({
                "type": "limited_validation",
                "severity": "low",
                "description": "éªŒè¯å®éªŒç›¸å¯¹æœ‰é™"
            })
        
        return risks
    
    async def _generate_basic_mitigation_strategies(self, risk_types: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç”ŸæˆåŸºç¡€çš„é£é™©ç¼“è§£ç­–ç•¥ã€‚"""
        
        strategies = []
        
        for risk in risk_types:
            risk_type = risk["type"]
            
            if risk_type == "technical_complexity":
                strategies.append({
                    "target_risk": risk_type,
                    "strategy": "åˆ†é˜¶æ®µå®ç°",
                    "description": "å°†å¤æ‚æŠ€æœ¯åˆ†è§£ä¸ºå¯ç®¡ç†çš„å­æ¨¡å—",
                    "priority": "high"
                })
            
            elif risk_type == "data_dependency":
                strategies.append({
                    "target_risk": risk_type,
                    "strategy": "æ•°æ®å¤‡é€‰æ–¹æ¡ˆ",
                    "description": "å‡†å¤‡å¤šä¸ªæ•°æ®æºå’Œåˆæˆæ•°æ®æ–¹æ¡ˆ",
                    "priority": "medium"
                })
            
            elif risk_type == "resource_intensive":
                strategies.append({
                    "target_risk": risk_type,
                    "strategy": "èµ„æºä¼˜åŒ–",
                    "description": "é‡‡ç”¨æ¨¡å‹å‹ç¼©å’Œäº‘è®¡ç®—èµ„æº",
                    "priority": "medium"
                })
            
            elif risk_type == "limited_validation":
                strategies.append({
                    "target_risk": risk_type,
                    "strategy": "æ‰©å±•éªŒè¯",
                    "description": "å¢åŠ æ›´å¤šè¯„ä¼°ç»´åº¦å’Œæµ‹è¯•åœºæ™¯",
                    "priority": "low"
                })
        
        return strategies
    
    def _risk_level_from_score(self, score: float) -> str:
        """æ ¹æ®åˆ†æ•°ç¡®å®šé£é™©ç­‰çº§ã€‚"""
        if score >= 7:
            return "é«˜é£é™©"
        elif score >= 4:
            return "ä¸­ç­‰é£é™©"
        else:
            return "ä½é£é™©"

    async def _verify_graph_consistency(self, idea: CandidateIdea, graph: SemanticOpportunityGraph) -> Dict[str, Any]:
        """æ£€æŸ¥æƒ³æ³•ä¸å›¾è°±çš„ä¸€è‡´æ€§ã€‚
        
        è¾“å…¥:
            - idea: å€™é€‰æƒ³æ³•ã€‚
            - graph: è¯­ä¹‰æœºä¼šå›¾è°±ã€‚
            
        è¾“å‡º:
            - Dict: å›¾è°±æ£€æŸ¥ç»“æœï¼ŒåŒ…å«éªŒè¯è¾¹æ•°ã€å†²çªæ£€æµ‹ã€ä¸€è‡´æ€§è¯„åˆ†ã€‚
            
        å®ç°æ€è·¯:
            1) æ£€æŸ¥æƒ³æ³•å‡è®¾æ˜¯å¦ä¸å›¾è°±ä¸­çš„å†²çªè¾¹ï¼ˆcontradicts/critiquesï¼‰ä¸€è‡´ã€‚
            2) éªŒè¯æƒ³æ³•æ¶‰åŠçš„æ–¹æ³•-ä»»åŠ¡-æ•°æ®-æŒ‡æ ‡ç»„åˆåœ¨å›¾è°±ä¸­çš„æ”¯æŒåº¦ã€‚
            3) åˆ©ç”¨NetworkXçš„å›¾åˆ†æç®—æ³•æ£€æµ‹æ½œåœ¨çš„é€»è¾‘ä¸ä¸€è‡´ã€‚
        """
        print(f"    ğŸ” æ£€æŸ¥æƒ³æ³•ä¸å›¾è°±çš„ä¸€è‡´æ€§")
        
        # æ£€æŸ¥è§¦å‘èŠ‚ç‚¹çš„å­˜åœ¨æ€§å’Œè¿æ¥æ€§
        node_verification = await self._verify_trigger_nodes(idea, graph)
        
        # æ£€æŸ¥å†²çªè¾¹
        conflict_analysis = await self._detect_graph_conflicts(idea, graph)
        
        # éªŒè¯æ–¹æ³•-ä»»åŠ¡ç»„åˆçš„æ”¯æŒåº¦
        combination_support = await self._verify_method_task_combinations(idea, graph)
        
        # è®¡ç®—æ•´ä½“ä¸€è‡´æ€§åˆ†æ•°
        consistency_score = await self._calculate_consistency_score(node_verification, conflict_analysis, combination_support)
        
        graph_checks = {
            "node_verification": node_verification,
            "conflict_analysis": conflict_analysis,
            "combination_support": combination_support,
            "consistency_score": consistency_score,
            "is_consistent": consistency_score >= 6.0
        }
        
        print(f"    âœ… å›¾è°±ä¸€è‡´æ€§æ£€æŸ¥å®Œæˆï¼Œä¸€è‡´æ€§åˆ†æ•°: {consistency_score:.2f}/10.0")
        
        return graph_checks
    
    async def _verify_trigger_nodes(self, idea: CandidateIdea, graph: SemanticOpportunityGraph) -> Dict[str, Any]:
        """éªŒè¯è§¦å‘èŠ‚ç‚¹åœ¨å›¾è°±ä¸­çš„å­˜åœ¨æ€§ã€‚"""
        
        if not hasattr(idea, 'source_trigger_nodes') or not idea.source_trigger_nodes:
            return {
                "nodes_checked": 0,
                "nodes_found": 0,
                "missing_nodes": [],
                "verification_score": 7.0  # å¦‚æœæ²¡æœ‰æ˜ç¡®èŠ‚ç‚¹ï¼Œç»™äºˆä¸­ç­‰åˆ†æ•°
            }
        
        nodes_found = 0
        missing_nodes = []
        
        for node_id in idea.source_trigger_nodes:
            if node_id in graph.nodes():
                nodes_found += 1
            else:
                missing_nodes.append(node_id)
        
        total_nodes = len(idea.source_trigger_nodes)
        verification_score = (nodes_found / total_nodes) * 10.0 if total_nodes > 0 else 7.0
        
        return {
            "nodes_checked": total_nodes,
            "nodes_found": nodes_found,
            "missing_nodes": missing_nodes,
            "verification_score": verification_score
        }
    
    async def _detect_graph_conflicts(self, idea: CandidateIdea, graph: SemanticOpportunityGraph) -> Dict[str, Any]:
        """æ£€æµ‹ä¸å›¾è°±ä¸­å†²çªè¾¹çš„çŸ›ç›¾ã€‚"""
        
        conflicts = []
        conflict_score = 0.0
        
        # æ£€æŸ¥å›¾è°±ä¸­çš„æ‰€æœ‰å†²çªè¾¹
        for src, dst, edge_data in graph.edges(data=True):
            relation = edge_data.get('relation', '')
            
            if relation in ['contradicts', 'critiques', 'conflicts_with']:
                # æ£€æŸ¥æƒ³æ³•æ˜¯å¦æ¶‰åŠè¿™äº›å†²çªçš„æ¦‚å¿µ
                src_name = graph.nodes[src].get('name', src)
                dst_name = graph.nodes[dst].get('name', dst)
                
                idea_text = (idea.title + " " + idea.core_hypothesis).lower()
                
                if (src_name.lower() in idea_text and dst_name.lower() in idea_text):
                    conflicts.append({
                        "source": src_name,
                        "target": dst_name,
                        "relation": relation,
                        "confidence": edge_data.get('confidence', 0.5)
                    })
                    conflict_score += edge_data.get('confidence', 0.5)
        
        # è®¡ç®—å†²çªå½±å“åˆ†æ•°ï¼ˆå†²çªè¶Šå¤šï¼Œåˆ†æ•°è¶Šä½ï¼‰
        if len(conflicts) == 0:
            final_score = 9.0
        elif conflict_score < 0.5:
            final_score = 7.0
        elif conflict_score < 1.0:
            final_score = 5.0
        else:
            final_score = 3.0
        
        return {
            "conflicts_detected": conflicts,
            "conflict_count": len(conflicts),
            "total_conflict_confidence": conflict_score,
            "conflict_score": final_score
        }
    
    async def _verify_method_task_combinations(self, idea: CandidateIdea, graph: SemanticOpportunityGraph) -> Dict[str, Any]:
        """éªŒè¯æ–¹æ³•-ä»»åŠ¡ç»„åˆåœ¨å›¾è°±ä¸­çš„æ”¯æŒåº¦ã€‚"""
        
        # ä»æƒ³æ³•ä¸­æå–æ–¹æ³•å’Œä»»åŠ¡ä¿¡æ¯
        extracted_methods = self._extract_methods_from_idea(idea)
        extracted_tasks = self._extract_tasks_from_idea(idea)
        
        if not extracted_methods or not extracted_tasks:
            return {
                "combinations_checked": 0,
                "supported_combinations": 0,
                "support_score": 7.0
            }
        
        supported_combinations = 0
        total_combinations = 0
        
        # æ£€æŸ¥æ¯ä¸ªæ–¹æ³•-ä»»åŠ¡ç»„åˆåœ¨å›¾è°±ä¸­çš„æ”¯æŒåº¦
        for method in extracted_methods:
            for task in extracted_tasks:
                total_combinations += 1
                
                # åœ¨å›¾è°±ä¸­å¯»æ‰¾æ”¯æŒè¿™ä¸ªç»„åˆçš„è¯æ®
                if await self._check_method_task_support(method, task, graph):
                    supported_combinations += 1
        
        support_ratio = supported_combinations / total_combinations if total_combinations > 0 else 0.7
        support_score = support_ratio * 10.0
        
        return {
            "combinations_checked": total_combinations,
            "supported_combinations": supported_combinations,
            "support_ratio": support_ratio,
            "support_score": support_score,
            "extracted_methods": extracted_methods,
            "extracted_tasks": extracted_tasks
        }
    
    def _extract_methods_from_idea(self, idea: CandidateIdea) -> List[str]:
        """ä»æƒ³æ³•ä¸­æå–æ–¹æ³•åç§°ã€‚"""
        
        methods = []
        idea_text = idea.title + " " + idea.core_hypothesis + " " + " ".join(idea.initial_innovation_points)
        
        # å¸¸è§æ–¹æ³•å…³é”®è¯
        method_keywords = [
            'transformer', 'attention', 'lstm', 'cnn', 'bert', 'gpt',
            'neural network', 'deep learning', 'machine learning',
            'reinforcement learning', 'transfer learning', 'meta learning'
        ]
        
        for keyword in method_keywords:
            if keyword.lower() in idea_text.lower():
                methods.append(keyword)
        
        return list(set(methods))
    
    def _extract_tasks_from_idea(self, idea: CandidateIdea) -> List[str]:
        """ä»æƒ³æ³•ä¸­æå–ä»»åŠ¡åç§°ã€‚"""
        
        tasks = []
        idea_text = idea.title + " " + idea.core_hypothesis + " " + " ".join(idea.initial_innovation_points)
        
        # å¸¸è§ä»»åŠ¡å…³é”®è¯
        task_keywords = [
            'classification', 'detection', 'generation', 'translation',
            'prediction', 'recommendation', 'optimization', 'clustering',
            'regression', 'segmentation', 'recognition', 'synthesis'
        ]
        
        for keyword in task_keywords:
            if keyword.lower() in idea_text.lower():
                tasks.append(keyword)
        
        return list(set(tasks))
    
    async def _check_method_task_support(self, method: str, task: str, graph: SemanticOpportunityGraph) -> bool:
        """æ£€æŸ¥å›¾è°±ä¸­æ˜¯å¦æ”¯æŒç‰¹å®šçš„æ–¹æ³•-ä»»åŠ¡ç»„åˆã€‚"""
        
        # å¯»æ‰¾æ–¹æ³•èŠ‚ç‚¹
        method_nodes = []
        for node_id, node_data in graph.nodes(data=True):
            node_name = node_data.get('name', '').lower()
            if method.lower() in node_name or node_name in method.lower():
                if node_data.get('type') == 'Method':
                    method_nodes.append(node_id)
        
        # å¯»æ‰¾ä»»åŠ¡èŠ‚ç‚¹
        task_nodes = []
        for node_id, node_data in graph.nodes(data=True):
            node_name = node_data.get('name', '').lower()
            if task.lower() in node_name or node_name in task.lower():
                if node_data.get('type') == 'Task':
                    task_nodes.append(node_id)
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨è¿æ¥
        for method_node in method_nodes:
            for task_node in task_nodes:
                if graph.has_edge(method_node, task_node) or graph.has_edge(task_node, method_node):
                    return True
        
        # å¦‚æœæ²¡æœ‰ç›´æ¥è¿æ¥ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰é—´æ¥æ”¯æŒ
        # ç®€åŒ–å®ç°ï¼šå¦‚æœä¸¤ä¸ªèŠ‚ç‚¹éƒ½å­˜åœ¨ï¼Œè®¤ä¸ºæœ‰ä¸€å®šæ”¯æŒ
        return len(method_nodes) > 0 and len(task_nodes) > 0
    
    async def _calculate_consistency_score(self, node_verification: Dict[str, Any], 
                                         conflict_analysis: Dict[str, Any], 
                                         combination_support: Dict[str, Any]) -> float:
        """è®¡ç®—æ•´ä½“ä¸€è‡´æ€§åˆ†æ•°ã€‚"""
        
        # æƒé‡åˆ†é…
        node_weight = 0.4
        conflict_weight = 0.3
        support_weight = 0.3
        
        node_score = node_verification.get("verification_score", 7.0)
        conflict_score = conflict_analysis.get("conflict_score", 7.0)
        support_score = combination_support.get("support_score", 7.0)
        
        total_score = (node_score * node_weight + 
                      conflict_score * conflict_weight + 
                      support_score * support_weight)
        
        return max(0.0, min(10.0, total_score))

    async def _synthesize_feasibility_critique(self, idea: CandidateIdea, relevance_score: float, 
                                             asset_analysis: Dict[str, Any], risk_analysis: Dict[str, Any], 
                                             graph_checks: Dict[str, Any]) -> FeasibilityCritique:
        """ç»¼åˆå¯è¡Œæ€§è¯„å®¡ç»“æœã€‚
        
        è¾“å…¥:
            - idea: å€™é€‰æƒ³æ³•ã€‚
            - relevance_score: ç›¸å…³æ€§å¾—åˆ†ã€‚
            - asset_analysis: èµ„äº§åˆ†æç»“æœã€‚
            - risk_analysis: é£é™©åˆ†æç»“æœã€‚
            - graph_checks: å›¾è°±æ£€æŸ¥ç»“æœã€‚
            
        è¾“å‡º:
            - FeasibilityCritique: å®Œæ•´çš„å¯è¡Œæ€§è¯„å®¡ç»“æœã€‚
            
        å®ç°æ€è·¯:
            1) æ ¹æ®å„ç»´åº¦å¾—åˆ†è®¡ç®—ç»¼åˆå¯è¡Œæ€§åˆ†æ•°ã€‚
            2) æ•´ç†æ‰€éœ€èµ„äº§æ¸…å•ä¸æ½œåœ¨é£é™©åˆ—è¡¨ã€‚
            3) ç”Ÿæˆå¯è¡Œæ€§è¯„ä¼°æ€»ç»“ä¸æ”¹è¿›å»ºè®®ã€‚
        """
        print(f"    ğŸ¯ ç»¼åˆå¯è¡Œæ€§è¯„å®¡ç»“æœ")
        
        # æ­¥éª¤1ï¼šè®¡ç®—ç»¼åˆå¯è¡Œæ€§åˆ†æ•°
        feasibility_score = await self._calculate_weighted_feasibility_score(
            relevance_score, asset_analysis, risk_analysis, graph_checks
        )
        
        # æ­¥éª¤2ï¼šæ•´ç†è¯¦ç»†çš„å¯è¡Œæ€§åˆ†æ
        dimension_scores = {
            "relevance": relevance_score,
            "asset_availability": asset_analysis.get("overall_availability_score", 7.0),
            "risk_assessment": 10.0 - risk_analysis.get("overall_risk_score", 3.0),  # é£é™©è¶Šä½å¯è¡Œæ€§è¶Šé«˜
            "graph_consistency": graph_checks.get("consistency_score", 7.0)
        }
        
        # æ­¥éª¤3ï¼šæ•´ç†æ‰€éœ€èµ„äº§å’Œæ½œåœ¨é£é™©
        required_assets = await self._compile_required_assets(asset_analysis)
        potential_risks = await self._compile_potential_risks(risk_analysis)
        
        # æ­¥éª¤4ï¼šç”Ÿæˆæ”¹è¿›å»ºè®®
        improvement_suggestions = await self._generate_improvement_suggestions(
            dimension_scores, asset_analysis, risk_analysis, graph_checks
        )
        
        # åˆ›å»ºFeasibilityCritiqueå¯¹è±¡
        critique = FeasibilityCritique(
            idea_id=idea.id,
            feasibility_score=feasibility_score,
            required_assets=required_assets,
            potential_risks=potential_risks,
            graph_checks={
                "approach": "multi_dimensional_analysis",
                "dimensions": list(self.feasibility_dimensions),
                "weights": self._get_feasibility_weights(),
                "timestamp": datetime.now().isoformat()
            }
        )
        
        print(f"    âœ… å¯è¡Œæ€§è¯„åˆ†: {feasibility_score:.2f}/10.0")
        print(f"    ğŸ“‹ è¯†åˆ« {len(required_assets)} é¡¹èµ„äº§éœ€æ±‚")
        print(f"    âš ï¸ å‘ç° {len(potential_risks)} é¡¹æ½œåœ¨é£é™©")
        
        return critique
    
    async def _calculate_weighted_feasibility_score(self, relevance_score: float, 
                                                  asset_analysis: Dict[str, Any], 
                                                  risk_analysis: Dict[str, Any], 
                                                  graph_checks: Dict[str, Any]) -> float:
        """è®¡ç®—åŠ æƒå¯è¡Œæ€§åˆ†æ•°ã€‚"""
        
        # ç»´åº¦æƒé‡é…ç½®
        weights = self._get_feasibility_weights()
        
        # æ”¶é›†å„ç»´åº¦åˆ†æ•°
        scores = {
            "relevance": relevance_score,
            "asset_availability": asset_analysis.get("overall_availability_score", 7.0),
            "risk_assessment": 10.0 - risk_analysis.get("overall_risk_score", 3.0),  # é£é™©åˆ†è½¬å¯è¡Œæ€§åˆ†
            "graph_consistency": graph_checks.get("consistency_score", 7.0)
        }
        
        # åŠ æƒè®¡ç®—
        total_score = 0.0
        total_weight = 0.0
        
        for dimension, weight in weights.items():
            if dimension in scores:
                score = scores[dimension]
                total_score += score * weight
                total_weight += weight
        
        # å½’ä¸€åŒ–
        if total_weight > 0:
            final_score = total_score / total_weight
        else:
            final_score = 6.0  # é»˜è®¤ä¸­ç­‰å¯è¡Œæ€§
        
        return max(0.0, min(10.0, final_score))
    
    def _get_feasibility_weights(self) -> Dict[str, float]:
        """è·å–å¯è¡Œæ€§ç»´åº¦æƒé‡é…ç½®ã€‚"""
        return {
            "relevance": 0.25,          # ç›¸å…³æ€§
            "asset_availability": 0.35, # èµ„äº§å¯å¾—æ€§æœ€é‡è¦
            "risk_assessment": 0.25,    # é£é™©è¯„ä¼°
            "graph_consistency": 0.15   # å›¾è°±ä¸€è‡´æ€§
        }
    
    async def _compile_required_assets(self, asset_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ•´ç†æ‰€éœ€èµ„äº§æ¸…å•ã€‚"""
        
        required_assets = []
        
        # å¯å¾—èµ„äº§
        available_assets = asset_analysis.get("available_assets", [])
        for asset in available_assets:
            required_assets.append({
                "type": asset.get("type", "Unknown"),
                "id": asset.get("id", "unknown"),
                "status": "available",
                "confidence": asset.get("confidence", 0.8),
                "notes": asset.get("notes", [])
            })
        
        # ç¼ºå¤±èµ„äº§
        missing_assets = asset_analysis.get("missing_assets", [])
        for asset in missing_assets:
            required_assets.append({
                "type": asset.get("type", "Unknown"),
                "id": asset.get("id", "unknown"),
                "status": "missing",
                "confidence": asset.get("confidence", 0.3),
                "notes": ["éœ€è¦å¯»æ‰¾æ›¿ä»£æ–¹æ¡ˆ"]
            })
        
        # æ·»åŠ æ›¿ä»£æ–¹æ¡ˆä¿¡æ¯
        alternatives = asset_analysis.get("alternatives", [])
        alternative_map = {}
        for alt in alternatives:
            asset_id = alt.get("for_asset", "unknown")
            if asset_id not in alternative_map:
                alternative_map[asset_id] = []
            alternative_map[asset_id].append({
                "type": alt.get("alternative_type", "unknown"),
                "description": alt.get("description", ""),
                "feasibility": alt.get("feasibility", 0.5)
            })
        
        # ä¸ºç¼ºå¤±èµ„äº§æ·»åŠ æ›¿ä»£æ–¹æ¡ˆ
        for asset in required_assets:
            if asset["status"] == "missing" and asset["id"] in alternative_map:
                asset["alternatives"] = alternative_map[asset["id"]]
        
        return required_assets
    
    async def _compile_potential_risks(self, risk_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ•´ç†æ½œåœ¨é£é™©æ¸…å•ã€‚"""
        
        potential_risks = []
        
        # ä»é£é™©åˆ†æä¸­æå–é£é™©
        identified_risks = risk_analysis.get("identified_risks", [])
        for risk in identified_risks:
            potential_risks.append({
                "type": risk.get("type", "unknown"),
                "severity": risk.get("severity", "medium"),
                "description": risk.get("description", ""),
                "likelihood": "medium"  # ç®€åŒ–è®¾ç½®
            })
        
        # æ·»åŠ ç¼“è§£ç­–ç•¥
        mitigation_strategies = risk_analysis.get("mitigation_strategies", [])
        strategy_map = {}
        for strategy in mitigation_strategies:
            target_risk = strategy.get("target_risk", "unknown")
            if target_risk not in strategy_map:
                strategy_map[target_risk] = []
            strategy_map[target_risk].append({
                "strategy": strategy.get("strategy", ""),
                "description": strategy.get("description", ""),
                "priority": strategy.get("priority", "medium")
            })
        
        # ä¸ºé£é™©æ·»åŠ ç¼“è§£ç­–ç•¥
        for risk in potential_risks:
            risk_type = risk["type"]
            if risk_type in strategy_map:
                risk["mitigation_strategies"] = strategy_map[risk_type]
        
        return potential_risks
    
    async def _generate_improvement_suggestions(self, dimension_scores: Dict[str, float], 
                                              asset_analysis: Dict[str, Any], 
                                              risk_analysis: Dict[str, Any], 
                                              graph_checks: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®ã€‚"""
        
        suggestions = []
        
        # åŸºäºç»´åº¦åˆ†æ•°çš„å»ºè®®
        for dimension, score in dimension_scores.items():
            if score < 6.0:
                if dimension == "relevance":
                    suggestions.append("å»ºè®®åŠ å¼ºæƒ³æ³•ä¸é¢†åŸŸä¸»æµç ”ç©¶çš„å…³è”æ€§ï¼Œæ˜ç¡®å­¦æœ¯ä»·å€¼å®šä½")
                elif dimension == "asset_availability":
                    suggestions.append("å»ºè®®ç¡®è®¤å…³é”®èµ„äº§çš„å¯å¾—æ€§ï¼Œåˆ¶å®šå¤‡é€‰èµ„æºè·å–æ–¹æ¡ˆ")
                elif dimension == "risk_assessment":
                    suggestions.append("å»ºè®®è¯¦ç»†åˆ†æå®ç°é£é™©ï¼Œåˆ¶å®šç›¸åº”çš„é£é™©ç¼“è§£ç­–ç•¥")
                elif dimension == "graph_consistency":
                    suggestions.append("å»ºè®®æ£€æŸ¥æƒ³æ³•ä¸ç°æœ‰çŸ¥è¯†çš„ä¸€è‡´æ€§ï¼Œé¿å…é€»è¾‘å†²çª")
        
        # åŸºäºèµ„äº§åˆ†æçš„å»ºè®®
        missing_count = len(asset_analysis.get("missing_assets", []))
        if missing_count > 2:
            suggestions.append(f"å»ºè®®ä¼˜å…ˆè§£å†³ {missing_count} é¡¹ç¼ºå¤±èµ„äº§ï¼Œæˆ–å¯»æ‰¾å¯è¡Œçš„æ›¿ä»£æ–¹æ¡ˆ")
        
        # åŸºäºé£é™©åˆ†æçš„å»ºè®®
        high_risk_count = len([r for r in risk_analysis.get("identified_risks", []) if r.get("severity") == "high"])
        if high_risk_count > 0:
            suggestions.append(f"å»ºè®®é‡ç‚¹å…³æ³¨ {high_risk_count} é¡¹é«˜é£é™©å› ç´ ï¼Œåˆ¶å®šè¯¦ç»†çš„åº”å¯¹é¢„æ¡ˆ")
        
        # åŸºäºå›¾è°±æ£€æŸ¥çš„å»ºè®®
        if not graph_checks.get("is_consistent", True):
            conflict_count = graph_checks.get("conflict_analysis", {}).get("conflict_count", 0)
            if conflict_count > 0:
                suggestions.append(f"å»ºè®®è§£å†³ä¸å›¾è°±çŸ¥è¯†çš„ {conflict_count} é¡¹å†²çªï¼Œæˆ–é‡æ–°è¯„ä¼°æƒ³æ³•çš„å¯è¡Œæ€§")
        
        # é€šç”¨æ”¹è¿›å»ºè®®
        if not suggestions:
            suggestions.append("å»ºè®®è¿›ä¸€æ­¥ç»†åŒ–å®ç°æ–¹æ¡ˆï¼ŒåŠ å¼ºå®éªŒè®¾è®¡çš„å®Œæ•´æ€§")
        
        return suggestions


class IdeaRefinerAgent(BaseIdeaAgent):
    """ç¬¬å››é˜¶æ®µï¼šå¼•å¯¼å¼ç²¾ç‚¼ä¸è¿­ä»£å†³ç­–ã€‚

    æ ¸å¿ƒèŒè´£:
        - æ±‡æ€»æ–°é¢–æ€§/å¯è¡Œæ€§æ‰¹åˆ¤ï¼Œç”Ÿæˆå¯æ‰§è¡Œ `RefinementPrompt`ï¼Œå¹¶å†³å®šæ˜¯å¦ç»§ç»­è¿­ä»£ã€‚

    æ³¨æ„äº‹é¡¹:
        - æŒ‡ä»¤éœ€å…·ä½“å¯æ‰§è¡Œå¹¶é™„éªŒæ”¶æ ‡å‡†(ç›®æ ‡åˆ†é˜ˆå€¼/è¾¹ç•Œæ¡ä»¶)ã€‚
    """

    def __init__(self, name: str, llm_factory: LLMFactory, db: AcademicPaperDatabase, config: Optional[AgentConfig] = None):
        super().__init__(name, llm_factory, db, config)
        self.default_thresholds = {
            "novelty_threshold": 8.0,
            "feasibility_threshold": 7.0,
            "combined_threshold": 15.0
        }

    async def make_refinement_prompt(self, idea: CandidateIdea, novelty: NoveltyCritique, feasibility: FeasibilityCritique) -> RefinementPrompt:
        """ç”Ÿæˆç²¾ç‚¼æŒ‡ä»¤ã€‚

        è¾“å…¥:
            - idea: å½“å‰æƒ³æ³•ç‰ˆæœ¬ã€‚
            - novelty: æ–°é¢–æ€§æ‰¹åˆ¤ç»“æœã€‚
            - feasibility: å¯è¡Œæ€§æ‰¹åˆ¤ç»“æœã€‚

        è¾“å‡º:
            - RefinementPrompt: å†³ç­–(decision)ä¸å…·ä½“ä¿®æ”¹è¯´æ˜(instructions)ã€éªŒæ”¶æ ‡å‡†(acceptance_criteria)ã€‚

        å®ç°æ­¥éª¤å»ºè®®:
            1) è¯†åˆ«"åƒè°/ç¼ºä»€ä¹ˆ/éš¾åœ¨å“ª"ï¼Œä»¥å·®å¼‚-é£é™©å¯¹é½ç”Ÿæˆä¿®æ”¹è·¯å¾„ã€‚
            2) ç»™å‡ºæ˜ç¡®ä¿ç•™/æ›¿æ¢/è¡¥å……é¡¹ä¸å¯éªŒè¯çš„éªŒæ”¶æ ‡å‡†ã€‚
        """
        # åˆ†æå½“å‰çŠ¶æ€
        decision = await self._analyze_refinement_decision(idea, novelty, feasibility)
        
        # ç”Ÿæˆå…·ä½“æŒ‡ä»¤
        instructions = await self._generate_refinement_instructions(idea, novelty, feasibility, decision)
        
        # è®¾å®šéªŒæ”¶æ ‡å‡†
        acceptance_criteria = self._define_acceptance_criteria(novelty, feasibility, decision)
        
        return RefinementPrompt(
            idea_id=idea.id,
            decision=decision,
            instructions=instructions,
            rationale=await self._generate_rationale(novelty, feasibility, decision),
            acceptance_criteria=acceptance_criteria
        )

    async def make_refinement_decisions_batch(self, idea_critique_pairs: List[Tuple[CandidateIdea, NoveltyCritique, FeasibilityCritique]]) -> List[Dict[str, Any]]:
        """æ‰¹é‡åˆ†æç²¾ç‚¼å†³ç­–ç±»å‹ï¼šæ¥å—/ä¿®è®¢/æ‹†åˆ†/åˆå¹¶/ä¸¢å¼ƒã€‚
        
        è¾“å…¥:
            - idea_critique_pairs: (æƒ³æ³•, æ–°é¢–æ€§è¯„å®¡, å¯è¡Œæ€§è¯„å®¡)çš„å…ƒç»„åˆ—è¡¨ã€‚
            
        è¾“å‡º:
            - List[Dict]: å†³ç­–ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«decisionã€confidenceã€reasoningç­‰ã€‚
            
        å®ç°æ€è·¯:
            ä½¿ç”¨LLMä¸€æ¬¡æ€§åˆ†æå¤šä¸ªæƒ³æ³•çš„åŒæ–¹æ‰¹åˆ¤ï¼Œæ™ºèƒ½åˆ¤æ–­æœ€ä½³å†³ç­–è·¯å¾„ã€‚
        """
        print(f"ğŸ¤” å¼€å§‹æ‰¹é‡ç²¾ç‚¼å†³ç­–åˆ†æï¼š{len(idea_critique_pairs)}ä¸ªæƒ³æ³•")
        
        # æ„å»ºæ‰¹é‡åˆ†æprompt
        ideas_summary = []
        for i, (idea, novelty, feasibility) in enumerate(idea_critique_pairs, 1):
            ideas_summary.append(f"""**æƒ³æ³•{i}**ï¼š
- ID: {idea.id}
- æ ‡é¢˜: {idea.title}
- æ ¸å¿ƒå‡è®¾: {idea.core_hypothesis}
- åˆ›æ–°ç‚¹: {', '.join(idea.initial_innovation_points)}
- å½“å‰ç‰ˆæœ¬: {idea.version}
- æ–°é¢–æ€§è¯„åˆ†: {novelty.novelty_score:.1f}/10.0 (ç›¸ä¼¼å·¥ä½œæ•°é‡: {len(novelty.similar_works)}, å·®å¼‚æ€§ä¸»å¼ : {len(novelty.difference_claims)} æ¡)
- å¯è¡Œæ€§è¯„åˆ†: {feasibility.feasibility_score:.1f}/10.0 (æ‰€éœ€èµ„æº: {len(feasibility.required_assets)} é¡¹, æ½œåœ¨é£é™©: {len(feasibility.potential_risks)} é¡¹)""")
        
        prompt = f"""ä½œä¸ºç ”ç©¶æƒ³æ³•ç²¾ç‚¼ä¸“å®¶å’Œä¼šè®®ä¸»å¸­ï¼Œè¯·å¯¹ä»¥ä¸‹{len(idea_critique_pairs)}ä¸ªæƒ³æ³•çš„è¯„å®¡ç»“æœè¿›è¡Œæ‰¹é‡åˆ†æï¼Œä¸ºæ¯ä¸ªæƒ³æ³•ç»™å‡ºæœ€ä½³å†³ç­–å»ºè®®ã€‚

**å¾…åˆ†ææƒ³æ³•åˆ—è¡¨**ï¼š
{chr(10).join(ideas_summary)}

**å†³ç­–é€‰é¡¹è¯´æ˜**ï¼š
1. **accept**: æƒ³æ³•å·²è¶³å¤Ÿæˆç†Ÿï¼Œå¯ä»¥è¿›å…¥å®æ–½é˜¶æ®µ
2. **revise**: æƒ³æ³•æœ‰æ½œåŠ›ä½†éœ€è¦ç‰¹å®šæ”¹è¿›ï¼ˆå¦‚æ›¿æ¢æ–¹æ³•ã€è°ƒæ•´èŒƒå›´ç­‰ï¼‰
3. **split**: æƒ³æ³•è¿‡äºå¤æ‚ï¼Œåº”æ‹†åˆ†ä¸ºå¤šä¸ªç‹¬ç«‹çš„å­æƒ³æ³•
4. **merge**: æƒ³æ³•è¿‡äºå•è–„ï¼Œéœ€è¦ä¸å…¶ä»–æƒ³æ³•åˆå¹¶
5. **discard**: æƒ³æ³•å­˜åœ¨æ ¹æœ¬æ€§é—®é¢˜ï¼Œæ”¹è¿›æ½œåŠ›æœ‰é™

è¯·åŸºäºæ¯ä¸ªæƒ³æ³•çš„ç ”ç©¶ä»·å€¼ã€æŠ€æœ¯å¯è¡Œæ€§ã€æ”¹è¿›æ½œåŠ›ç­‰è§’åº¦ç»¼åˆåˆ†æï¼Œç»™å‡ºæœ€ä½³å†³ç­–ã€‚

è¾“å‡ºæ ¼å¼ï¼š
```json
{{
    "batch_analysis": "å¯¹æ•´æ‰¹æƒ³æ³•çš„æ€»ä½“åˆ†æå’Œå†³ç­–æ€è·¯",
    "decisions": [
        {{
            "idea_id": "æƒ³æ³•ID",
            "decision": "accept|revise|split|merge|discard",
            "confidence": 0.95,
            "reasoning": "è¯¦ç»†çš„å†³ç­–ç†ç”±ï¼Œè¯´æ˜ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªå†³ç­–",
            "key_factors": ["å½±å“å†³ç­–çš„å…³é”®å› ç´ 1", "å…³é”®å› ç´ 2", "å…³é”®å› ç´ 3"]
        }}
    ]
}}
```"""

        try:
            llm = self.llm
            
            response_data = await llm.generate(
                model_name=self.config.model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=20000,
                temperature=0.3,
                agent_name=self.name,
                task_type="batch_refinement_decisions"
            )
            response = response_data.get('content', '')
            
            # è§£æLLMå“åº”
            response_text = response.strip()
            
            # æå–JSONéƒ¨åˆ†
            import re
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
            if json_match:
                import json
                result = json.loads(json_match.group(1))
                decisions = result.get('decisions', [])
                batch_analysis = result.get('batch_analysis', '')
                
                print(f"âœ… æ‰¹é‡å†³ç­–åˆ†æå®Œæˆï¼š{len(decisions)}ä¸ªå†³ç­–")
                if batch_analysis:
                    print(f"ğŸ“Š æ€»ä½“åˆ†æ: {batch_analysis[:100]}...")
                
                # ç¡®ä¿å†³ç­–æ•°é‡åŒ¹é…
                if len(decisions) != len(idea_critique_pairs):
                    print(f"âš ï¸ å†³ç­–æ•°é‡ä¸åŒ¹é…ï¼Œé¢„æœŸ{len(idea_critique_pairs)}ä¸ªï¼Œå®é™…{len(decisions)}ä¸ªï¼Œä½¿ç”¨å¤‡ç”¨é€»è¾‘")
                    return await self._fallback_batch_decisions(idea_critique_pairs)
                
                return decisions
            else:
                print(f"âš ï¸ LLMæ‰¹é‡å†³ç­–å“åº”è§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨å†³ç­–é€»è¾‘")
                return await self._fallback_batch_decisions(idea_critique_pairs)
                
        except Exception as e:
            print(f"âŒ LLMæ‰¹é‡å†³ç­–åˆ†æå¤±è´¥: {e}")
            return await self._fallback_batch_decisions(idea_critique_pairs)

    async def _fallback_batch_decisions(self, idea_critique_pairs: List[Tuple[CandidateIdea, NoveltyCritique, FeasibilityCritique]]) -> List[Dict[str, Any]]:
        """æ‰¹é‡å†³ç­–çš„å¤‡ç”¨é€»è¾‘ã€‚"""
        print(f"ğŸ”„ æ‰§è¡Œæ‰¹é‡å†³ç­–å¤‡ç”¨é€»è¾‘")
        results = []
        for idea, novelty, feasibility in idea_critique_pairs:
            decision = await self._fallback_decision(idea, novelty, feasibility)
            results.append({
                "idea_id": idea.id,
                "decision": decision,
                "confidence": 0.7,
                "reasoning": f"å¤‡ç”¨å†³ç­–é€»è¾‘ï¼šåŸºäºæ–°é¢–æ€§{novelty.novelty_score:.1f}å’Œå¯è¡Œæ€§{feasibility.feasibility_score:.1f}çš„ç»¼åˆåˆ¤æ–­",
                "key_factors": ["å¤‡ç”¨é€»è¾‘", "åˆ†æ•°è¯„ä¼°", "é˜ˆå€¼æ¯”è¾ƒ"]
            })
        return results

    async def _analyze_refinement_decision(self, idea: CandidateIdea, novelty: NoveltyCritique, feasibility: FeasibilityCritique) -> str:
        """åŸºäºLLMåˆ†æå†³ç­–ç±»å‹ï¼šæ¥å—/ä¿®è®¢/æ‹†åˆ†/åˆå¹¶/ä¸¢å¼ƒã€‚
        
        è¾“å…¥:
            - idea: å½“å‰æƒ³æ³•ã€‚
            - novelty: æ–°é¢–æ€§è¯„å®¡ã€‚
            - feasibility: å¯è¡Œæ€§è¯„å®¡ã€‚
            
        è¾“å‡º:
            - str: å†³ç­–ç±»å‹ (accept | revise | split | merge | discard)ã€‚
            
        å®ç°æ€è·¯:
            ä½¿ç”¨LLMåˆ†æåŒæ–¹æ‰¹åˆ¤ï¼Œæ™ºèƒ½åˆ¤æ–­æœ€ä½³å†³ç­–è·¯å¾„ã€‚
        """
        print(f"    ğŸ¤” åˆ†æç²¾ç‚¼å†³ç­– - æ–°é¢–æ€§: {novelty.novelty_score:.1f}, å¯è¡Œæ€§: {feasibility.feasibility_score:.1f}")
        
        prompt = f"""ä½œä¸ºç ”ç©¶æƒ³æ³•ç²¾ç‚¼ä¸“å®¶å’Œä¼šè®®ä¸»å¸­ï¼Œè¯·åˆ†æä»¥ä¸‹æƒ³æ³•çš„è¯„å®¡ç»“æœï¼Œå¹¶ç»™å‡ºæœ€ä½³å†³ç­–å»ºè®®ã€‚

**å¾…åˆ†ææƒ³æ³•**ï¼š
- ID: {idea.id}
- æ ‡é¢˜: {idea.title}
- æ ¸å¿ƒå‡è®¾: {idea.core_hypothesis}
- åˆ›æ–°ç‚¹: {', '.join(idea.initial_innovation_points)}
- å½“å‰ç‰ˆæœ¬: {idea.version}

**æ–°é¢–æ€§è¯„å®¡ç»“æœ**ï¼š
- è¯„åˆ†: {novelty.novelty_score:.1f}/10.0
- ç›¸ä¼¼å·¥ä½œæ•°é‡: {len(novelty.similar_works)}
- å·®å¼‚æ€§ä¸»å¼ : {len(novelty.difference_claims)} æ¡
- å…³é”®è¯„ä»·: {novelty.similar_works[0].get('summary', 'æš‚æ— ') if novelty.similar_works else 'æš‚æ— ç›¸ä¼¼å·¥ä½œ'}

**å¯è¡Œæ€§è¯„å®¡ç»“æœ**ï¼š
- è¯„åˆ†: {feasibility.feasibility_score:.1f}/10.0
- æ‰€éœ€èµ„æº: {len(feasibility.required_assets)} é¡¹
- æ½œåœ¨é£é™©: {len(feasibility.potential_risks)} é¡¹
- å…³é”®è¯„ä»·: {feasibility.relevance[:200] if feasibility.relevance else 'æš‚æ— '}

**å†³ç­–é€‰é¡¹è¯´æ˜**ï¼š
1. **accept**: æƒ³æ³•å·²è¶³å¤Ÿæˆç†Ÿï¼Œå¯ä»¥è¿›å…¥å®æ–½é˜¶æ®µ
2. **revise**: æƒ³æ³•æœ‰æ½œåŠ›ä½†éœ€è¦ç‰¹å®šæ”¹è¿›ï¼ˆå¦‚æ›¿æ¢æ–¹æ³•ã€è°ƒæ•´èŒƒå›´ç­‰ï¼‰
3. **split**: æƒ³æ³•è¿‡äºå¤æ‚ï¼Œåº”æ‹†åˆ†ä¸ºå¤šä¸ªç‹¬ç«‹çš„å­æƒ³æ³•
4. **merge**: æƒ³æ³•è¿‡äºå•è–„ï¼Œéœ€è¦ä¸å…¶ä»–æƒ³æ³•åˆå¹¶
5. **discard**: æƒ³æ³•å­˜åœ¨æ ¹æœ¬æ€§é—®é¢˜ï¼Œæ”¹è¿›æ½œåŠ›æœ‰é™

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œä»ç ”ç©¶ä»·å€¼ã€æŠ€æœ¯å¯è¡Œæ€§ã€æ”¹è¿›æ½œåŠ›ç­‰è§’åº¦ç»¼åˆåˆ†æï¼Œç»™å‡ºæœ€ä½³å†³ç­–ã€‚

è¾“å‡ºæ ¼å¼ï¼š
```json
{{
    "decision": "accept|revise|split|merge|discard",
    "confidence": 0.95,
    "reasoning": "è¯¦ç»†çš„å†³ç­–ç†ç”±ï¼Œè¯´æ˜ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªå†³ç­–",
    "key_factors": ["å½±å“å†³ç­–çš„å…³é”®å› ç´ 1", "å…³é”®å› ç´ 2", "å…³é”®å› ç´ 3"]
}}
```"""

        try:
            llm = self.llm
            
            response_data = await llm.generate(
                model_name=self.config.model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=15000,
                temperature=0.3,
                agent_name=self.name,
                task_type="refinement_decision"
            )
            response = response_data.get('content', '')
            
            # è§£æLLMå“åº”
            response_text = response.strip()
            
            # æå–JSONéƒ¨åˆ†
            import re
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
            if json_match:
                import json
                decision_result = json.loads(json_match.group(1))
                decision = decision_result.get('decision', 'revise')
                reasoning = decision_result.get('reasoning', '')
                confidence = decision_result.get('confidence', 0.5)
                
                print(f"    ğŸ¯ LLMå†³ç­–: {decision} (ç½®ä¿¡åº¦: {confidence:.2f})")
                if reasoning:
                    print(f"    ğŸ’­ å†³ç­–ç†ç”±: {reasoning[:100]}...")
                
                return decision
            else:
                print(f"    âš ï¸ LLMå“åº”è§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨å†³ç­–é€»è¾‘")
                return await self._fallback_decision(idea, novelty, feasibility)
                
        except Exception as e:
            print(f"    âŒ LLMå†³ç­–åˆ†æå¤±è´¥: {e}")
            return await self._fallback_decision(idea, novelty, feasibility)
    
    async def _fallback_decision(self, idea: CandidateIdea, novelty: NoveltyCritique, feasibility: FeasibilityCritique) -> str:
        """å¤‡ç”¨å†³ç­–é€»è¾‘ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼Œå½“LLMè°ƒç”¨å¤±è´¥æ—¶ä½¿ç”¨ã€‚"""
        combined_score = novelty.novelty_score + feasibility.feasibility_score
        
        if combined_score >= 15.0:
            return "accept"
        elif combined_score < 8.0:
            return "discard"
        elif len(idea.initial_innovation_points) > 4:
            return "split"
        elif len(idea.initial_innovation_points) < 2:
            return "merge"
        else:
            return "revise"
    
    async def _assess_improvement_potential(self, idea: CandidateIdea, novelty: NoveltyCritique, feasibility: FeasibilityCritique) -> float:
        """è¯„ä¼°æƒ³æ³•çš„æ”¹è¿›æ½œåŠ›ã€‚"""
        
        potential_score = 0.0
        
        # æ–°é¢–æ€§æ”¹è¿›æ½œåŠ›
        if novelty.novelty_score < 6.0 and len(novelty.similar_works) > 0:
            # å¦‚æœæœ‰æ˜ç¡®çš„ç›¸ä¼¼å·¥ä½œï¼Œå¯ä»¥é’ˆå¯¹æ€§æ”¹è¿›
            potential_score += 0.3
        
        # å¯è¡Œæ€§æ”¹è¿›æ½œåŠ›  
        if feasibility.feasibility_score < 6.0:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ›¿ä»£æ–¹æ¡ˆ
            alternatives_count = sum(1 for asset in feasibility.required_assets 
                                   if asset.get('alternatives', []))
            if alternatives_count > 0:
                potential_score += 0.4
            
            # æ£€æŸ¥é£é™©æ˜¯å¦å¯ç¼“è§£
            mitigatable_risks = sum(1 for risk in feasibility.potential_risks 
                                  if risk.get('mitigation_strategies', []))
            if mitigatable_risks > 0:
                potential_score += 0.3
        
        # æƒ³æ³•ç»“æ„å®Œæ•´æ€§
        if len(idea.initial_innovation_points) >= 2 and len(idea.preliminary_experiments) >= 1:
            potential_score += 0.2
        
        return min(1.0, potential_score)
    
    async def _should_split_idea(self, idea: CandidateIdea, novelty: NoveltyCritique, feasibility: FeasibilityCritique) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ‹†åˆ†æƒ³æ³•ã€‚"""
        
        # æ‹†åˆ†æ¡ä»¶1ï¼šåˆ›æ–°ç‚¹è¿‡å¤š
        if len(idea.initial_innovation_points) > 4:
            return True
        
        # æ‹†åˆ†æ¡ä»¶2ï¼šå®éªŒè®¾è®¡å¤æ‚åº¦è¿‡é«˜
        if len(idea.preliminary_experiments) > 3:
            return True
        
        # æ‹†åˆ†æ¡ä»¶3ï¼šå¯è¡Œæ€§é£é™©è¿‡äºåˆ†æ•£
        if len(feasibility.potential_risks) > 5:
            risk_types = set(risk.get('type', '') for risk in feasibility.potential_risks)
            if len(risk_types) > 3:  # é£é™©ç±»å‹è¿‡äºå¤šæ ·
                return True
        
        # æ‹†åˆ†æ¡ä»¶4ï¼šæƒ³æ³•æ ‡é¢˜æˆ–å‡è®¾è¡¨æ˜å¤šä¸ªç‹¬ç«‹æ–¹å‘
        idea_text = idea.title + " " + idea.core_hypothesis
        multi_indicators = ['and', 'ä»¥åŠ', 'åŒæ—¶', 'both', 'multiple', 'å¤šä¸ª', 'å¤šç§']
        multi_count = sum(1 for indicator in multi_indicators if indicator in idea_text.lower())
        if multi_count > 2:
            return True
        
        return False
    
    async def _should_merge_idea(self, idea: CandidateIdea, novelty: NoveltyCritique, feasibility: FeasibilityCritique) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆå¹¶æƒ³æ³•ã€‚"""
        
        # åˆå¹¶æ¡ä»¶1ï¼šåˆ›æ–°ç‚¹è¿‡å°‘ä¸”å•è–„
        if len(idea.initial_innovation_points) < 2:
            return True
        
        # åˆå¹¶æ¡ä»¶2ï¼šå®éªŒè®¾è®¡è¿‡äºç®€å•
        if len(idea.preliminary_experiments) < 1:
            return True
        
        # åˆå¹¶æ¡ä»¶3ï¼šæ ¸å¿ƒå‡è®¾è¿‡äºç®€çŸ­
        if len(idea.core_hypothesis.strip()) < 50:
            return True
        
        # åˆå¹¶æ¡ä»¶4ï¼šæ–°é¢–æ€§å’Œå¯è¡Œæ€§éƒ½ä¸­ç­‰ä½†ä¸çªå‡º
        if 4.0 <= novelty.novelty_score <= 6.0 and 4.0 <= feasibility.feasibility_score <= 6.0:
            return True
        
        return False

    async def _generate_refinement_instructions(self, idea: CandidateIdea, novelty: NoveltyCritique, 
                                               feasibility: FeasibilityCritique, decision: str) -> List[str]:
        """åŸºäºLLMç”Ÿæˆå…·ä½“çš„ä¿®æ”¹æŒ‡ä»¤ã€‚
        
        è¾“å…¥:
            - idea: å½“å‰æƒ³æ³•ã€‚
            - novelty: æ–°é¢–æ€§è¯„å®¡ã€‚
            - feasibility: å¯è¡Œæ€§è¯„å®¡ã€‚
            - decision: å†³ç­–ç±»å‹ã€‚
            
        è¾“å‡º:
            - List[str]: å…·ä½“ä¿®æ”¹æŒ‡ä»¤åˆ—è¡¨ã€‚
            
        å®ç°æ€è·¯:
            ä½¿ç”¨LLMæ ¹æ®è¯„å®¡ç»“æœç”Ÿæˆå…·ä½“å¯æ‰§è¡Œçš„ä¿®æ”¹æŒ‡ä»¤ã€‚
        """
        print(f"    ğŸ“ ç”Ÿæˆ {decision} ç±»å‹çš„ç²¾ç‚¼æŒ‡ä»¤")
        
        if decision in ["accept", "discard"]:
            if decision == "accept":
                return ["æƒ³æ³•å·²è¾¾åˆ°éªŒæ”¶æ ‡å‡†ï¼Œæ— éœ€è¿›ä¸€æ­¥ä¿®æ”¹"]
            else:
                return ["æƒ³æ³•è´¨é‡ä¸è¶³ä¸”æ— æ˜æ˜¾æ”¹è¿›æ½œåŠ›ï¼Œå»ºè®®ä¸¢å¼ƒ"]
        
        # æ„å»ºè¯¦ç»†çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆæ·»åŠ ç±»å‹æ£€æŸ¥ï¼‰
        similar_works_summary = []
        for work in novelty.similar_works[:3]:  # åªå–å‰3ä¸ªæœ€ç›¸ä¼¼çš„å·¥ä½œ
            if isinstance(work, dict):
                similar_works_summary.append(f"- {work.get('title', 'Unknown')}: {work.get('summary', 'No summary')}")
            else:
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                similar_works_summary.append(f"- {str(work)}")
        
        risks_summary = []
        for risk in feasibility.potential_risks[:3]:  # åªå–å‰3ä¸ªä¸»è¦é£é™©
            if isinstance(risk, dict):
                risks_summary.append(f"- {risk.get('description', 'Unknown risk')}")
            else:
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                risks_summary.append(f"- {str(risk)}")
        
        assets_summary = []
        for asset in feasibility.required_assets[:3]:  # åªå–å‰3ä¸ªä¸»è¦èµ„æº
            if isinstance(asset, dict):
                assets_summary.append(f"- {asset.get('name', 'Unknown asset')}: {asset.get('availability', 'Unknown')}")
            else:
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                assets_summary.append(f"- {str(asset)}")

        prompt = f"""ä½œä¸ºç ”ç©¶æƒ³æ³•ç²¾ç‚¼ä¸“å®¶ï¼Œè¯·æ ¹æ®è¯„å®¡ç»“æœä¸ºä»¥ä¸‹æƒ³æ³•ç”Ÿæˆå…·ä½“å¯æ‰§è¡Œçš„ä¿®æ”¹æŒ‡ä»¤ã€‚

**å¾…ç²¾ç‚¼æƒ³æ³•**ï¼š
- ID: {idea.id}
- æ ‡é¢˜: {idea.title}
- æ ¸å¿ƒå‡è®¾: {idea.core_hypothesis}
- åˆ›æ–°ç‚¹: {idea.initial_innovation_points}
- ç‰ˆæœ¬: {idea.version}

**å†³ç­–ç±»å‹**: {decision}

**æ–°é¢–æ€§è¯„å®¡è¯¦æƒ…**ï¼š
- è¯„åˆ†: {novelty.novelty_score:.1f}/10.0
- ä¸»è¦ç›¸ä¼¼å·¥ä½œ:
{chr(10).join(similar_works_summary) if similar_works_summary else '- æ— ç›¸ä¼¼å·¥ä½œ'}
- å·®å¼‚æ€§ä¸»å¼ æ•°é‡: {len(novelty.difference_claims)}

**å¯è¡Œæ€§è¯„å®¡è¯¦æƒ…**ï¼š
- è¯„åˆ†: {feasibility.feasibility_score:.1f}/10.0
- ä¸»è¦é£é™©:
{chr(10).join(risks_summary) if risks_summary else '- æ— æ˜æ˜¾é£é™©'}
- å…³é”®èµ„æº:
{chr(10).join(assets_summary) if assets_summary else '- æ— ç‰¹æ®Šèµ„æºéœ€æ±‚'}

**ä»»åŠ¡è¦æ±‚**ï¼š
æ ¹æ®å†³ç­–ç±»å‹"{decision}"ï¼Œç”Ÿæˆ3-5æ¡å…·ä½“å¯æ‰§è¡Œçš„ä¿®æ”¹æŒ‡ä»¤ã€‚æ¯æ¡æŒ‡ä»¤åº”è¯¥ï¼š
1. å…·ä½“æ˜ç¡®ï¼Œé¿å…æ¨¡ç³Šè¡¨è¿°
2. å¯ç›´æ¥æ“ä½œï¼ŒæŒ‡æ˜éœ€è¦ä¿®æ”¹çš„å…·ä½“éƒ¨åˆ†ï¼ˆæ ‡é¢˜/å‡è®¾/åˆ›æ–°ç‚¹/å®éªŒè®¾è®¡ç­‰ï¼‰
3. æä¾›æ”¹è¿›æ–¹å‘å’Œé¢„æœŸæ•ˆæœ
4. è€ƒè™‘æ–°é¢–æ€§å’Œå¯è¡Œæ€§çš„å¹³è¡¡

**æŒ‡ä»¤ç±»å‹è¯´æ˜**ï¼š
- revise: é’ˆå¯¹æ€§æ”¹è¿›ï¼Œä¿æŒæƒ³æ³•ä¸»ä½“ç»“æ„ä¸å˜
- split: å°†å¤æ‚æƒ³æ³•æ‹†åˆ†ä¸ºå¤šä¸ªç‹¬ç«‹ä¸”èšç„¦çš„å­æƒ³æ³•
- merge: ä¸å…¶ä»–æƒ³æ³•åˆå¹¶ï¼Œæˆ–å†…éƒ¨æ•´åˆå¢å¼ºå†…å®¹æ·±åº¦

è¾“å‡ºæ ¼å¼ï¼š
```json
{{
    "instructions": [
        "å…·ä½“æŒ‡ä»¤1ï¼šè¯¦ç»†æè¿°éœ€è¦åšä»€ä¹ˆæ”¹åŠ¨",
        "å…·ä½“æŒ‡ä»¤2ï¼šè¯¦ç»†æè¿°éœ€è¦åšä»€ä¹ˆæ”¹åŠ¨",
        "å…·ä½“æŒ‡ä»¤3ï¼šè¯¦ç»†æè¿°éœ€è¦åšä»€ä¹ˆæ”¹åŠ¨"
    ],
    "rationale": "ç”Ÿæˆè¿™äº›æŒ‡ä»¤çš„æ€»ä½“æ€è·¯å’Œé¢„æœŸæ”¹è¿›æ•ˆæœ"
}}
```"""

        try:
            llm = self.llm
            
            response_data = await llm.generate(
                model_name=self.config.model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=15000,
                temperature=0.4,
                agent_name=self.name,
                task_type="refinement_instructions"
            )
            response = response_data.get('content', '')
            
            # è§£æLLMå“åº”
            response_text = response.strip()
            
            # æå–JSONéƒ¨åˆ†
            import re
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
            if json_match:
                import json
                result = json.loads(json_match.group(1))
                instructions = result.get('instructions', [])
                rationale = result.get('rationale', '')
                
                print(f"    âœ… ç”Ÿæˆ {len(instructions)} æ¡å…·ä½“æŒ‡ä»¤")
                if rationale:
                    print(f"    ğŸ’¡ æŒ‡ä»¤æ€è·¯: {rationale[:100]}...")
                
                return instructions
            else:
                print(f"    âš ï¸ LLMæŒ‡ä»¤ç”Ÿæˆè§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
                return await self._fallback_instructions(idea, novelty, feasibility, decision)
                
        except Exception as e:
            print(f"    âŒ LLMæŒ‡ä»¤ç”Ÿæˆå¤±è´¥: {e}")
            return await self._fallback_instructions(idea, novelty, feasibility, decision)

    async def _generate_refinement_instructions_batch(self, batch_data: List[Tuple[CandidateIdea, NoveltyCritique, FeasibilityCritique, str]]) -> List[List[str]]:
        """æ‰¹é‡ç”Ÿæˆç²¾ç‚¼æŒ‡ä»¤ã€‚
        
        è¾“å…¥:
            - batch_data: [(æƒ³æ³•, æ–°é¢–æ€§è¯„å®¡, å¯è¡Œæ€§è¯„å®¡, å†³ç­–ç±»å‹)] çš„åˆ—è¡¨ã€‚
            
        è¾“å‡º:
            - List[List[str]]: æ¯ä¸ªæƒ³æ³•å¯¹åº”çš„æŒ‡ä»¤åˆ—è¡¨ã€‚
            
        å®ç°æ€è·¯:
            ä½¿ç”¨LLMä¸€æ¬¡æ€§ä¸ºå¤šä¸ªæƒ³æ³•ç”Ÿæˆç²¾ç‚¼æŒ‡ä»¤ã€‚
        """
        print(f"ğŸ“ å¼€å§‹æ‰¹é‡ç”Ÿæˆç²¾ç‚¼æŒ‡ä»¤ï¼š{len(batch_data)}ä¸ªæƒ³æ³•")
        
        # å‡†å¤‡æ‰¹é‡prompt
        ideas_summary = []
        for i, (idea, novelty, feasibility, decision) in enumerate(batch_data, 1):
            # æ„å»ºç›¸ä¼¼å·¥ä½œå’Œé£é™©æ‘˜è¦ï¼ˆæ·»åŠ ç±»å‹æ£€æŸ¥ï¼‰
            similar_works_summary = []
            for work in novelty.similar_works[:3]:
                if isinstance(work, dict):
                    similar_works_summary.append(f"  - {work.get('title', 'Unknown')}")
                else:
                    # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                    similar_works_summary.append(f"  - {str(work)}")
            
            risks_summary = []
            for risk in feasibility.potential_risks[:3]:
                if isinstance(risk, dict):
                    risks_summary.append(f"  - {risk.get('description', 'Unknown risk')}")
                else:
                    # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                    risks_summary.append(f"  - {str(risk)}")
            
            assets_summary = []
            for asset in feasibility.required_assets[:3]:
                if isinstance(asset, dict):
                    assets_summary.append(f"  - {asset.get('name', 'Unknown asset')}")
                else:
                    # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                    assets_summary.append(f"  - {str(asset)}")

            ideas_summary.append(f"""**æƒ³æ³•{i}**ï¼š
- ID: {idea.id}
- æ ‡é¢˜: {idea.title}
- æ ¸å¿ƒå‡è®¾: {idea.core_hypothesis}
- åˆ›æ–°ç‚¹: {idea.initial_innovation_points}
- ç‰ˆæœ¬: {idea.version}
- å†³ç­–ç±»å‹: {decision}
- æ–°é¢–æ€§è¯„åˆ†: {novelty.novelty_score:.1f}/10.0
- å¯è¡Œæ€§è¯„åˆ†: {feasibility.feasibility_score:.1f}/10.0
- ä¸»è¦ç›¸ä¼¼å·¥ä½œ:
{chr(10).join(similar_works_summary) if similar_works_summary else '  - æ— ç›¸ä¼¼å·¥ä½œ'}
- ä¸»è¦é£é™©:
{chr(10).join(risks_summary) if risks_summary else '  - æ— æ˜æ˜¾é£é™©'}
- å…³é”®èµ„æº:
{chr(10).join(assets_summary) if assets_summary else '  - æ— ç‰¹æ®Šèµ„æºéœ€æ±‚'}""")

        prompt = f"""ä½œä¸ºç ”ç©¶æƒ³æ³•ç²¾ç‚¼ä¸“å®¶ï¼Œè¯·æ ¹æ®è¯„å®¡ç»“æœä¸ºä»¥ä¸‹{len(batch_data)}ä¸ªæƒ³æ³•æ‰¹é‡ç”Ÿæˆå…·ä½“å¯æ‰§è¡Œçš„ä¿®æ”¹æŒ‡ä»¤ã€‚

**å¾…ç²¾ç‚¼æƒ³æ³•åˆ—è¡¨**ï¼š
{chr(10).join(ideas_summary)}

**ä»»åŠ¡è¦æ±‚**ï¼š
ä¸ºæ¯ä¸ªæƒ³æ³•æ ¹æ®å…¶å†³ç­–ç±»å‹ç”Ÿæˆ3-5æ¡å…·ä½“å¯æ‰§è¡Œçš„ä¿®æ”¹æŒ‡ä»¤ã€‚æ¯æ¡æŒ‡ä»¤åº”è¯¥ï¼š
1. å…·ä½“æ˜ç¡®ï¼Œé¿å…æ¨¡ç³Šè¡¨è¿°
2. å¯ç›´æ¥æ“ä½œï¼ŒæŒ‡æ˜éœ€è¦ä¿®æ”¹çš„å…·ä½“éƒ¨åˆ†ï¼ˆæ ‡é¢˜/å‡è®¾/åˆ›æ–°ç‚¹/å®éªŒè®¾è®¡ç­‰ï¼‰
3. æä¾›æ”¹è¿›æ–¹å‘å’Œé¢„æœŸæ•ˆæœ
4. è€ƒè™‘æ–°é¢–æ€§å’Œå¯è¡Œæ€§çš„å¹³è¡¡

**æŒ‡ä»¤ç±»å‹è¯´æ˜**ï¼š
- accept: æƒ³æ³•å·²è¾¾åˆ°éªŒæ”¶æ ‡å‡†ï¼Œæ— éœ€è¿›ä¸€æ­¥ä¿®æ”¹
- discard: æƒ³æ³•è´¨é‡ä¸è¶³ä¸”æ— æ˜æ˜¾æ”¹è¿›æ½œåŠ›ï¼Œå»ºè®®ä¸¢å¼ƒ
- revise: é’ˆå¯¹æ€§æ”¹è¿›ï¼Œä¿æŒæƒ³æ³•ä¸»ä½“ç»“æ„ä¸å˜
- split: å°†å¤æ‚æƒ³æ³•æ‹†åˆ†ä¸ºå¤šä¸ªç‹¬ç«‹ä¸”èšç„¦çš„å­æƒ³æ³•
- merge: ä¸å…¶ä»–æƒ³æ³•åˆå¹¶ï¼Œæˆ–å†…éƒ¨æ•´åˆå¢å¼ºå†…å®¹æ·±åº¦

è¾“å‡ºæ ¼å¼ï¼š
```json
{{
    "batch_analysis": "å¯¹æ•´æ‰¹æƒ³æ³•çš„æ€»ä½“æŒ‡ä»¤ç”Ÿæˆæ€è·¯",
    "instructions_list": [
        {{
            "idea_id": "æƒ³æ³•1çš„ID", 
            "instructions": [
                "å…·ä½“æŒ‡ä»¤1ï¼šè¯¦ç»†æè¿°éœ€è¦åšä»€ä¹ˆæ”¹åŠ¨",
                "å…·ä½“æŒ‡ä»¤2ï¼šè¯¦ç»†æè¿°éœ€è¦åšä»€ä¹ˆæ”¹åŠ¨",
                "å…·ä½“æŒ‡ä»¤3ï¼šè¯¦ç»†æè¿°éœ€è¦åšä»€ä¹ˆæ”¹åŠ¨"
            ],
            "rationale": "ç”Ÿæˆè¿™äº›æŒ‡ä»¤çš„æ€è·¯"
        }}
    ]
}}
```"""

        try:
            llm = self.llm
            
            response_data = await llm.generate(
                model_name=self.config.model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=25000,
                temperature=0.4,
                agent_name=self.name,
                task_type="batch_refinement_instructions"
            )
            response = response_data.get('content', '')
            
            # è§£æLLMå“åº”
            response_text = response.strip()
            
            # æå–JSONéƒ¨åˆ†
            import re
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
            if json_match:
                import json
                result = json.loads(json_match.group(1))
                instructions_list = result.get('instructions_list', [])
                batch_analysis = result.get('batch_analysis', '')
                
                print(f"âœ… æ‰¹é‡æŒ‡ä»¤ç”Ÿæˆå®Œæˆï¼š{len(instructions_list)}ä¸ªæƒ³æ³•çš„æŒ‡ä»¤")
                if batch_analysis:
                    print(f"ğŸ’¡ æ‰¹é‡åˆ†æ: {batch_analysis[:100]}...")
                
                # ç¡®ä¿æŒ‡ä»¤æ•°é‡åŒ¹é…å¹¶æå–æŒ‡ä»¤
                final_instructions = []
                for i, (idea, _, _, decision) in enumerate(batch_data):
                    if i < len(instructions_list):
                        instruction_data = instructions_list[i]
                        # æ·»åŠ ç±»å‹æ£€æŸ¥ï¼Œç¡®ä¿ instruction_data æ˜¯å­—å…¸
                        if isinstance(instruction_data, dict):
                            instructions = instruction_data.get('instructions', [])
                        elif isinstance(instruction_data, list):
                            instructions = instruction_data  # å¦‚æœå·²ç»æ˜¯åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
                        else:
                            instructions = []  # å…¶ä»–æƒ…å†µä½¿ç”¨ç©ºåˆ—è¡¨
                        
                        # å¯¹äºacceptå’Œdiscardç±»å‹ï¼Œæä¾›é»˜è®¤æŒ‡ä»¤
                        if decision == "accept" and not instructions:
                            instructions = ["æƒ³æ³•å·²è¾¾åˆ°éªŒæ”¶æ ‡å‡†ï¼Œæ— éœ€è¿›ä¸€æ­¥ä¿®æ”¹"]
                        elif decision == "discard" and not instructions:
                            instructions = ["æƒ³æ³•è´¨é‡ä¸è¶³ä¸”æ— æ˜æ˜¾æ”¹è¿›æ½œåŠ›ï¼Œå»ºè®®ä¸¢å¼ƒ"]
                        
                        final_instructions.append(instructions)
                        print(f"    âœ… æƒ³æ³• {idea.id}: {len(instructions)} æ¡æŒ‡ä»¤")
                    else:
                        print(f"    âš ï¸ æƒ³æ³• {idea.id} ç¼ºå°‘æŒ‡ä»¤ï¼Œä½¿ç”¨å¤‡ç”¨é€»è¾‘")
                        fallback_instructions = await self._fallback_instructions(idea, batch_data[i][1], batch_data[i][2], decision)
                        final_instructions.append(fallback_instructions)
                
                return final_instructions
            else:
                print(f"âš ï¸ LLMæ‰¹é‡æŒ‡ä»¤å“åº”è§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
                return await self._fallback_instructions_batch(batch_data)
                
        except Exception as e:
            print(f"âŒ LLMæ‰¹é‡æŒ‡ä»¤ç”Ÿæˆå¤±è´¥: {e}")
            return await self._fallback_instructions_batch(batch_data)

    async def _fallback_instructions_batch(self, batch_data: List[Tuple[CandidateIdea, NoveltyCritique, FeasibilityCritique, str]]) -> List[List[str]]:
        """æ‰¹é‡æŒ‡ä»¤ç”Ÿæˆçš„å¤‡ç”¨é€»è¾‘ã€‚"""
        print(f"ğŸ”„ æ‰§è¡Œæ‰¹é‡æŒ‡ä»¤å¤‡ç”¨é€»è¾‘")
        results = []
        for idea, novelty, feasibility, decision in batch_data:
            instructions = await self._fallback_instructions(idea, novelty, feasibility, decision)
            results.append(instructions)
        return results

    async def make_refinement_prompts_batch(self, decisions: List[Dict[str, Any]], 
                                          idea_critique_pairs: List[Tuple[CandidateIdea, NoveltyCritique, FeasibilityCritique]]) -> List[RefinementPrompt]:
        """æ‰¹é‡ç”Ÿæˆç²¾ç‚¼æŒ‡ä»¤ã€‚
        
        è¾“å…¥:
            - decisions: æ‰¹é‡å†³ç­–ç»“æœåˆ—è¡¨ã€‚
            - idea_critique_pairs: (æƒ³æ³•, æ–°é¢–æ€§è¯„å®¡, å¯è¡Œæ€§è¯„å®¡)çš„å…ƒç»„åˆ—è¡¨ã€‚
            
        è¾“å‡º:
            - List[RefinementPrompt]: ç²¾ç‚¼æŒ‡ä»¤åˆ—è¡¨ã€‚
            
        å®ç°æ€è·¯:
            åŸºäºæ‰¹é‡å†³ç­–ç»“æœï¼Œä½¿ç”¨æ‰¹é‡LLMè°ƒç”¨ä¸ºæ‰€æœ‰æƒ³æ³•ç”Ÿæˆå¯¹åº”çš„ç²¾ç‚¼æŒ‡ä»¤ã€‚
        """
        print(f"ğŸ“ å¼€å§‹æ‰¹é‡ç”Ÿæˆç²¾ç‚¼æŒ‡ä»¤ï¼š{len(decisions)}ä¸ªå†³ç­–ï¼ˆçœŸæ­£çš„æ‰¹é‡æ¨¡å¼ï¼‰")
        
        try:
            # å‡†å¤‡æ‰¹é‡æ•°æ®ï¼š[(æƒ³æ³•, æ–°é¢–æ€§è¯„å®¡, å¯è¡Œæ€§è¯„å®¡, å†³ç­–ç±»å‹)]
            batch_data = []
            for decision_info, (idea, novelty, feasibility) in zip(decisions, idea_critique_pairs):
                # æ·»åŠ ç±»å‹æ£€æŸ¥
                if isinstance(decision_info, dict):
                    decision = decision_info.get('decision', 'revise')
                elif isinstance(decision_info, str):
                    decision = decision_info  # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                else:
                    decision = 'revise'  # é»˜è®¤å€¼
                batch_data.append((idea, novelty, feasibility, decision))
            
            # ğŸš€ æ‰¹é‡ç”Ÿæˆæ‰€æœ‰æƒ³æ³•çš„æŒ‡ä»¤ï¼ˆä¸€æ¬¡LLMè°ƒç”¨ï¼‰
            print(f"    ğŸ¤– å¯åŠ¨æ‰¹é‡æŒ‡ä»¤ç”ŸæˆLLMè°ƒç”¨...")
            all_instructions = await self._generate_refinement_instructions_batch(batch_data)
            
            # æ„å»ºæœ€ç»ˆçš„RefinementPromptå¯¹è±¡åˆ—è¡¨
            refinement_prompts = []
            for i, (decision_info, (idea, novelty, feasibility)) in enumerate(zip(decisions, idea_critique_pairs)):
                # æ·»åŠ ç±»å‹æ£€æŸ¥
                if isinstance(decision_info, dict):
                    decision = decision_info.get('decision', 'revise')
                    reasoning = decision_info.get('reasoning', '')
                elif isinstance(decision_info, str):
                    decision = decision_info  # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                    reasoning = ''
                else:
                    decision = 'revise'  # é»˜è®¤å€¼
                    reasoning = ''
                
                try:
                    # è·å–æ‰¹é‡ç”Ÿæˆçš„æŒ‡ä»¤
                    instructions = all_instructions[i] if i < len(all_instructions) else []
                    
                    # è®¾å®šéªŒæ”¶æ ‡å‡†
                    acceptance_criteria = self._define_acceptance_criteria(novelty, feasibility, decision)
                    
                    # ç”Ÿæˆç†ç”±ï¼ˆç›´æ¥ä½¿ç”¨å†³ç­–ç†ç”±ï¼‰
                    rationale = reasoning if reasoning else await self._generate_rationale(novelty, feasibility, decision)
                    
                    refinement_prompt = RefinementPrompt(
                        idea_id=idea.id,
                        decision=decision,
                        instructions=instructions,
                        rationale=rationale,
                        acceptance_criteria=acceptance_criteria
                    )
                    
                    refinement_prompts.append(refinement_prompt)
                    print(f"    âœ… æƒ³æ³• {idea.id}: {decision} æŒ‡ä»¤ç»„è£…å®Œæˆï¼ˆ{len(instructions)}æ¡æŒ‡ä»¤ï¼‰")
                    
                except Exception as e:
                    print(f"    âŒ æƒ³æ³• {idea.id} æŒ‡ä»¤ç»„è£…å¤±è´¥: {e}")
                    # åˆ›å»ºé»˜è®¤çš„ç²¾ç‚¼æŒ‡ä»¤
                    fallback_instructions = await self._fallback_instructions(idea, novelty, feasibility, decision)
                    refinement_prompt = RefinementPrompt(
                        idea_id=idea.id,
                        decision=decision,
                        instructions=fallback_instructions,
                        rationale=f"æŒ‡ä»¤ç»„è£…å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨é€»è¾‘: {str(e)}",
                        acceptance_criteria=self._define_acceptance_criteria(novelty, feasibility, decision)
                    )
                    refinement_prompts.append(refinement_prompt)
            
            print(f"âœ… æ‰¹é‡ç²¾ç‚¼æŒ‡ä»¤ç”Ÿæˆå®Œæˆï¼š{len(refinement_prompts)}ä¸ªæŒ‡ä»¤ï¼ˆæ‰¹é‡LLMæ¨¡å¼ï¼‰")
            return refinement_prompts
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡æŒ‡ä»¤ç”Ÿæˆæµç¨‹å¤±è´¥: {e}ï¼Œå›é€€åˆ°é€ä¸ªç”Ÿæˆæ¨¡å¼")
            # å®Œå…¨å¤±è´¥æ—¶å›é€€åˆ°åŸæ¥çš„é€ä¸ªç”Ÿæˆæ¨¡å¼
            return await self._fallback_prompts_batch_generation(decisions, idea_critique_pairs)
   
    async def _fallback_prompts_batch_generation(self, decisions: List[Dict[str, Any]], 
                                            idea_critique_pairs: List[Tuple[CandidateIdea, NoveltyCritique, FeasibilityCritique]]) -> List[RefinementPrompt]:
        """æ‰¹é‡æŒ‡ä»¤ç”Ÿæˆçš„å®Œå…¨å¤‡ç”¨æ–¹æ¡ˆï¼šå›é€€åˆ°é€ä¸ªç”Ÿæˆæ¨¡å¼ã€‚"""
        print(f"ğŸ”„ æ‰§è¡Œå®Œå…¨å¤‡ç”¨æ–¹æ¡ˆï¼šé€ä¸ªç”ŸæˆæŒ‡ä»¤æ¨¡å¼")
        
        refinement_prompts = []
        for i, (decision_info, (idea, novelty, feasibility)) in enumerate(zip(decisions, idea_critique_pairs)):
            # æ·»åŠ ç±»å‹æ£€æŸ¥ï¼Œç¡®ä¿ decision_info æ˜¯å­—å…¸
            if isinstance(decision_info, dict):
                decision = decision_info.get('decision', 'revise')
                reasoning = decision_info.get('reasoning', '')
            elif isinstance(decision_info, str):
                decision = decision_info  # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                reasoning = ''
            else:
                decision = 'revise'  # é»˜è®¤å€¼
                reasoning = ''
            
            try:
                # é€ä¸ªç”Ÿæˆå…·ä½“æŒ‡ä»¤
                instructions = await self._generate_refinement_instructions(idea, novelty, feasibility, decision)
                acceptance_criteria = self._define_acceptance_criteria(novelty, feasibility, decision)
                rationale = reasoning if reasoning else await self._generate_rationale(novelty, feasibility, decision)
                
                refinement_prompt = RefinementPrompt(
                    idea_id=idea.id,
                    decision=decision,
                    instructions=instructions,
                    rationale=rationale,
                    acceptance_criteria=acceptance_criteria
                )
                refinement_prompts.append(refinement_prompt)
                
            except Exception as e:
                print(f"    âŒ æƒ³æ³• {idea.id} å¤‡ç”¨æŒ‡ä»¤ç”Ÿæˆå¤±è´¥: {e}")
                fallback_instructions = await self._fallback_instructions(idea, novelty, feasibility, decision)
                refinement_prompt = RefinementPrompt(
                    idea_id=idea.id,
                    decision=decision,
                    instructions=fallback_instructions,
                    rationale=f"å¤‡ç”¨æŒ‡ä»¤ç”Ÿæˆå¤±è´¥: {str(e)}",
                    acceptance_criteria=self._define_acceptance_criteria(novelty, feasibility, decision)
                )
                refinement_prompts.append(refinement_prompt)
        
        return refinement_prompts
    
    async def _fallback_instructions(self, idea: CandidateIdea, novelty: NoveltyCritique, 
                                   feasibility: FeasibilityCritique, decision: str) -> List[str]:
        """å¤‡ç”¨æŒ‡ä»¤ç”Ÿæˆé€»è¾‘ï¼Œå½“LLMè°ƒç”¨å¤±è´¥æ—¶ä½¿ç”¨ã€‚"""
        instructions = []
        
        if decision == "revise":
            if novelty.novelty_score < 7.0:
                instructions.append("å¢å¼ºæƒ³æ³•çš„æ–°é¢–æ€§ï¼šåœ¨æ ‡é¢˜ä¸­çªå‡ºç‹¬ç‰¹çš„æŠ€æœ¯è·¯å¾„æˆ–åº”ç”¨åœºæ™¯")
                instructions.append("å¼ºåŒ–ä¸ç°æœ‰å·¥ä½œçš„å·®å¼‚æ€§ï¼šæ˜ç¡®è¯´æ˜æ ¸å¿ƒæ–¹æ³•çš„åˆ›æ–°ä¹‹å¤„")
            
            if feasibility.feasibility_score < 7.0:
                instructions.append("é™ä½å®ç°éš¾åº¦ï¼šç®€åŒ–æŠ€æœ¯æ–¹æ¡ˆæˆ–æä¾›å¯è¡Œçš„æ›¿ä»£æ–¹æ¡ˆ")
                instructions.append("è¡¥å……å®éªŒè®¾è®¡ï¼šæ·»åŠ å…·ä½“çš„éªŒè¯æ–¹æ³•å’Œè¯„ä»·æŒ‡æ ‡")
        
        elif decision == "split":
            instructions.append("æ‹†åˆ†ä¸ºå¤šä¸ªç‹¬ç«‹æƒ³æ³•ï¼šå°†æ ¸å¿ƒå‡è®¾åˆ†è§£ä¸º2-3ä¸ªå¯ç‹¬ç«‹ç ”ç©¶çš„å­é—®é¢˜")
            instructions.append("ç®€åŒ–å•ä¸ªæƒ³æ³•çš„å¤æ‚åº¦ï¼šæ¯ä¸ªå­æƒ³æ³•ä¸“æ³¨ä¸€ä¸ªæ ¸å¿ƒåˆ›æ–°ç‚¹")
        
        elif decision == "merge":
            instructions.append("å¢å¼ºæƒ³æ³•æ·±åº¦ï¼šè¡¥å……æ›´å¤šåˆ›æ–°ç‚¹å’ŒæŠ€æœ¯ç»†èŠ‚")
            instructions.append("æ‹“å±•åº”ç”¨èŒƒå›´ï¼šè€ƒè™‘ä¸ç›¸å…³é¢†åŸŸçš„ç»“åˆåº”ç”¨")
        
        return instructions if instructions else ["è¿›è¡Œå¸¸è§„ä¼˜åŒ–ï¼šå®Œå–„æƒ³æ³•çš„è¡¨è¿°å’ŒæŠ€æœ¯æè¿°"]
    
    async def _generate_novelty_instructions(self, idea: CandidateIdea, novelty: NoveltyCritique) -> List[str]:
        """ç”Ÿæˆæ–°é¢–æ€§æ”¹è¿›æŒ‡ä»¤ã€‚"""
        
        instructions = []
        
        if novelty.novelty_score < self.default_thresholds["novelty_threshold"]:
            # ç­–ç•¥1ï¼šå¼ºåŒ–ä¸æœ€ç›¸ä¼¼å·¥ä½œçš„å·®å¼‚
            if novelty.similar_works:
                most_similar = max(novelty.similar_works, key=lambda x: x.get('max_similarity', 0))
                similar_title = most_similar.get('title', 'unknown work')
                
                instructions.append(
                    f"å¼ºåŒ–ä¸ã€Š{similar_title}ã€‹çš„å·®å¼‚æ€§ï¼šåœ¨æ ‡é¢˜å’Œæ ¸å¿ƒå‡è®¾ä¸­æ˜ç¡®çªå‡ºæœ¬æƒ³æ³•çš„ç‹¬ç‰¹ä»·å€¼ä¸»å¼ "
                )
            
            # ç­–ç•¥2ï¼šåŸºäºå·®å¼‚æ€§ä¸»å¼ çš„å…·ä½“æ”¹è¿›
            if novelty.difference_claims:
                top_claims = novelty.difference_claims[:2]  # å–å‰2ä¸ªæœ€é‡è¦çš„å·®å¼‚ç‚¹
                for i, claim in enumerate(top_claims):
                    instructions.append(
                        f"æ ¹æ®å·®å¼‚ç‚¹{i+1}ï¼ˆ{claim}ï¼‰ï¼Œåœ¨åˆ›æ–°ç‚¹ä¸­è¡¥å……ç›¸åº”çš„æŠ€æœ¯ç»†èŠ‚å’Œå®ç°æœºåˆ¶"
                    )
            
            # ç­–ç•¥3ï¼šè¡¥å……åˆ›æ–°æœºåˆ¶
            if len(idea.initial_innovation_points) < 3:
                instructions.append(
                    "è¡¥å……è‡³å°‘1ä¸ªæ–°çš„åˆ›æ–°ç‚¹ï¼Œé‡ç‚¹æè¿°ä¸ç°æœ‰æ–¹æ³•çš„æŠ€æœ¯å·®å¼‚å’Œä¼˜åŠ¿"
                )
            
            # ç­–ç•¥4ï¼šå¢å¼ºæ–¹æ³•æ–°é¢–æ€§
            facet_scores = novelty.facet_scores
            if facet_scores.get('methodological', 7) < 7:
                instructions.append(
                    "æ”¹è¿›æ–¹æ³•æ–°é¢–æ€§ï¼šå¼•å…¥æ–°çš„æŠ€æœ¯ç»„ä»¶æˆ–åˆ›æ–°çš„ç»„åˆæ–¹å¼ï¼Œé¿å…ç›´æ¥å¤ç”¨ç°æœ‰æ–¹æ³•"
                )
            
            # ç­–ç•¥5ï¼šå¢å¼ºåº”ç”¨æ–°é¢–æ€§
            if facet_scores.get('application', 7) < 7:
                instructions.append(
                    "æ‰©å±•åº”ç”¨æ–°é¢–æ€§ï¼šæ˜ç¡®æŒ‡å‡ºåœ¨æ–°åº”ç”¨åœºæ™¯ä¸‹çš„ç‹¬ç‰¹æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆ"
                )
        
        return instructions
    
    async def _generate_feasibility_instructions(self, idea: CandidateIdea, feasibility: FeasibilityCritique) -> List[str]:
        """ç”Ÿæˆå¯è¡Œæ€§æ”¹è¿›æŒ‡ä»¤ã€‚"""
        
        instructions = []
        
        if feasibility.feasibility_score < self.default_thresholds["feasibility_threshold"]:
            # ç­–ç•¥1ï¼šè§£å†³èµ„äº§å¯å¾—æ€§é—®é¢˜
            missing_assets = [asset for asset in feasibility.required_assets 
                            if asset.get('status') == 'missing']
            
            for asset in missing_assets[:2]:  # å¤„ç†å‰2ä¸ªæœ€å…³é”®çš„ç¼ºå¤±èµ„äº§
                asset_type = asset.get('type', 'Unknown')
                asset_id = asset.get('id', 'unknown')
                
                alternatives = asset.get('alternatives', [])
                if alternatives:
                    best_alt = max(alternatives, key=lambda x: x.get('feasibility', 0))
                    instructions.append(
                        f"æ›¿æ¢ç¼ºå¤±çš„{asset_type} '{asset_id}'ä¸ºæ›¿ä»£æ–¹æ¡ˆï¼š{best_alt.get('description', 'å¯»æ‰¾å¯å¾—çš„æ›¿ä»£èµ„æº')}"
                    )
                else:
                    instructions.append(
                        f"ä¸ºç¼ºå¤±çš„{asset_type} '{asset_id}'å¯»æ‰¾å¼€æºå®ç°æˆ–å…¬å¼€æ•°æ®é›†æ›¿ä»£"
                    )
            
            # ç­–ç•¥2ï¼šç¼“è§£ä¸»è¦é£é™©
            high_priority_risks = [risk for risk in feasibility.potential_risks 
                                 if risk.get('severity') in ['high', 'medium']]
            
            for risk in high_priority_risks[:2]:  # å¤„ç†å‰2ä¸ªé«˜ä¼˜å…ˆçº§é£é™©
                risk_type = risk.get('type', 'unknown')
                mitigation_strategies = risk.get('mitigation_strategies', [])
                
                if mitigation_strategies:
                    strategy = mitigation_strategies[0]  # é‡‡ç”¨ç¬¬ä¸€ä¸ªç­–ç•¥
                    instructions.append(
                        f"ç¼“è§£{risk_type}é£é™©ï¼š{strategy.get('description', 'åˆ¶å®šè¯¦ç»†çš„é£é™©åº”å¯¹è®¡åˆ’')}"
                    )
                else:
                    instructions.append(
                        f"åˆ¶å®š{risk_type}é£é™©çš„å…·ä½“ç¼“è§£ç­–ç•¥ï¼Œé™ä½å®ç°éš¾åº¦"
                    )
            
            # ç­–ç•¥3ï¼šæ”¹å–„ç»´åº¦åˆ†æ•°
            dimension_scores = feasibility.dimension_scores
            
            if dimension_scores.get('relevance', 7) < 6:
                instructions.append(
                    "å¢å¼ºæƒ³æ³•çš„é¢†åŸŸç›¸å…³æ€§ï¼šæ˜ç¡®ä¸å½“å‰ç ”ç©¶çƒ­ç‚¹çš„è”ç³»ï¼Œå¼ºåŒ–å­¦æœ¯ä»·å€¼"
                )
            
            if dimension_scores.get('asset_availability', 7) < 6:
                instructions.append(
                    "æ”¹å–„èµ„äº§å¯å¾—æ€§ï¼šä¼˜å…ˆä½¿ç”¨å…¬å¼€æ•°æ®é›†å’Œå¼€æºå·¥å…·ï¼Œé¿å…ä¾èµ–ä¸“æœ‰èµ„æº"
                )
            
            if dimension_scores.get('graph_consistency', 7) < 6:
                instructions.append(
                    "è§£å†³ä¸ç°æœ‰çŸ¥è¯†çš„ä¸€è‡´æ€§é—®é¢˜ï¼šé‡æ–°è¯„ä¼°æ–¹æ³•-ä»»åŠ¡ç»„åˆçš„åˆç†æ€§"
                )
            
            # ç­–ç•¥4ï¼šç®€åŒ–å¤æ‚åº¦
            if len(feasibility.potential_risks) > 4:
                instructions.append(
                    "ç®€åŒ–å®ç°å¤æ‚åº¦ï¼šå°†å¤æ‚çš„æŠ€æœ¯æ–¹æ¡ˆåˆ†è§£ä¸ºæ›´å®¹æ˜“å®ç°çš„å­æ­¥éª¤"
                )
        
        return instructions
    
    async def _generate_split_instructions(self, idea: CandidateIdea, novelty: NoveltyCritique, feasibility: FeasibilityCritique) -> List[str]:
        """ç”Ÿæˆæ‹†åˆ†æŒ‡ä»¤ã€‚"""
        
        instructions = []
        
        # æ‹†åˆ†ç­–ç•¥1ï¼šæŒ‰åˆ›æ–°ç‚¹æ‹†åˆ†
        if len(idea.initial_innovation_points) > 3:
            instructions.append(
                f"å°† {len(idea.initial_innovation_points)} ä¸ªåˆ›æ–°ç‚¹æ‹†åˆ†ä¸º2-3ä¸ªç‹¬ç«‹çš„å­æƒ³æ³•ï¼Œæ¯ä¸ªæƒ³æ³•ä¸“æ³¨äº1-2ä¸ªæ ¸å¿ƒåˆ›æ–°"
            )
        
        # æ‹†åˆ†ç­–ç•¥2ï¼šæŒ‰å®éªŒç»´åº¦æ‹†åˆ†
        if len(idea.preliminary_experiments) > 2:
            instructions.append(
                f"å°† {len(idea.preliminary_experiments)} ä¸ªå®éªŒæ‹†åˆ†ä¸ºä¸åŒçš„ç ”ç©¶é˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µå½¢æˆç‹¬ç«‹çš„éªŒè¯æƒ³æ³•"
            )
        
        # æ‹†åˆ†ç­–ç•¥3ï¼šæŒ‰é£é™©ç±»å‹æ‹†åˆ†
        if len(feasibility.potential_risks) > 4:
            risk_types = set(risk.get('type', '') for risk in feasibility.potential_risks)
            if len(risk_types) > 2:
                instructions.append(
                    f"æŒ‰é£é™©ç±»å‹æ‹†åˆ†ï¼šå°†æ¶‰åŠ {len(risk_types)} ç§ä¸åŒé£é™©çš„æƒ³æ³•åˆ†è§£ä¸ºé£é™©æ›´é›†ä¸­çš„å­æ–¹æ¡ˆ"
                )
        
        # æ‹†åˆ†ç­–ç•¥4ï¼šæŒ‰åº”ç”¨åœºæ™¯æ‹†åˆ†
        idea_text = idea.title + " " + idea.core_hypothesis
        if any(indicator in idea_text.lower() for indicator in ['multi', 'å¤š', 'various', 'å„ç§']):
            instructions.append(
                "æŒ‰åº”ç”¨åœºæ™¯æ‹†åˆ†ï¼šå°†å¤šåœºæ™¯åº”ç”¨çš„æƒ³æ³•æ‹†åˆ†ä¸ºä¸“æ³¨äºå•ä¸€åœºæ™¯çš„å…·ä½“å®ç°"
            )
        
        return instructions
    
    async def _generate_merge_instructions(self, idea: CandidateIdea, novelty: NoveltyCritique, feasibility: FeasibilityCritique) -> List[str]:
        """ç”Ÿæˆåˆå¹¶æŒ‡ä»¤ã€‚"""
        
        instructions = []
        
        # åˆå¹¶ç­–ç•¥1ï¼šè¡¥å……åˆ›æ–°ç‚¹
        if len(idea.initial_innovation_points) < 2:
            instructions.append(
                "è¡¥å……åˆ›æ–°ç‚¹ï¼šä¸ç›¸å…³çš„æŠ€æœ¯åˆ›æ–°æˆ–åº”ç”¨æ‰©å±•è¿›è¡Œåˆå¹¶ï¼Œå½¢æˆæ›´ä¸°å¯Œçš„ç ”ç©¶å†…å®¹"
            )
        
        # åˆå¹¶ç­–ç•¥2ï¼šæ‰©å±•å®éªŒè®¾è®¡
        if len(idea.preliminary_experiments) < 2:
            instructions.append(
                "æ‰©å±•å®éªŒéªŒè¯ï¼šä¸ç›¸å…³çš„è¯„ä¼°ç»´åº¦æˆ–æ•°æ®é›†éªŒè¯è¿›è¡Œåˆå¹¶ï¼Œå½¢æˆæ›´å…¨é¢çš„éªŒè¯æ–¹æ¡ˆ"
            )
        
        # åˆå¹¶ç­–ç•¥3ï¼šä¸°å¯Œæ ¸å¿ƒå‡è®¾
        if len(idea.core_hypothesis.strip()) < 50:
            instructions.append(
                "ä¸°å¯Œæ ¸å¿ƒå‡è®¾ï¼šç»“åˆç›¸å…³çš„ç†è®ºåŸºç¡€æˆ–æŠ€æœ¯æœºåˆ¶ï¼Œå½¢æˆæ›´æ·±å…¥çš„ç ”ç©¶å‡è®¾"
            )
        
        # åˆå¹¶ç­–ç•¥4ï¼šæå‡æ•´ä½“ä»·å€¼
        combined_score = novelty.novelty_score + feasibility.feasibility_score
        if combined_score < 12:
            instructions.append(
                f"æå‡æ•´ä½“ä»·å€¼ï¼šä¸äº’è¡¥çš„ç ”ç©¶æ–¹å‘åˆå¹¶ï¼Œç›®æ ‡æ˜¯å°†ç»¼åˆåˆ†ä» {combined_score:.1f} æå‡åˆ° >12"
            )
        
        return instructions

        
    def _define_acceptance_criteria(self, novelty: NoveltyCritique, feasibility: FeasibilityCritique, decision: str) -> List[str]:
        """å®šä¹‰éªŒæ”¶æ ‡å‡†ã€‚
        
        è¾“å…¥:
            - novelty: æ–°é¢–æ€§è¯„å®¡ã€‚
            - feasibility: å¯è¡Œæ€§è¯„å®¡ã€‚
            - decision: å†³ç­–ç±»å‹ã€‚
            
        è¾“å‡º:
            - List[str]: éªŒæ”¶æ ‡å‡†åˆ—è¡¨ã€‚
            
        å®ç°æ€è·¯:
            åŸºäºè¯„å®¡ç»“æœå’Œå†³ç­–ç±»å‹ï¼Œç”Ÿæˆå…·ä½“çš„éªŒæ”¶æ ‡å‡†ã€‚
        """
        criteria = []
        
        # åŸºç¡€è´¨é‡æ ‡å‡†
        criteria.append("æƒ³æ³•è¡¨è¿°æ¸…æ™°å®Œæ•´ï¼Œé€»è¾‘è‡ªæ´½")
        criteria.append("æ ¸å¿ƒå‡è®¾æ˜ç¡®ä¸”å…·æœ‰å¯éªŒè¯æ€§")
        criteria.append("åˆ›æ–°ç‚¹æè¿°å…·ä½“ä¸”ä¸ç°æœ‰å·¥ä½œæœ‰æ˜ç¡®å·®å¼‚")
        
        if decision in ["accept", "revise"]:
            # æ–°é¢–æ€§å…·ä½“æ ‡å‡†
            if novelty.novelty_score < self.default_thresholds["novelty_threshold"]:
                criteria.append("æ–°é¢–æ€§è¯„åˆ†è¾¾åˆ°é˜ˆå€¼è¦æ±‚")
                
                if len(novelty.similar_works) > 0:
                    criteria.append("ä¸æœ€ç›¸ä¼¼å·¥ä½œçš„å·®å¼‚æ€§å·²æ˜ç¡®é˜è¿°å¹¶åœ¨æ ‡é¢˜/å‡è®¾ä¸­ä½“ç°")
                
                # æ·»åŠ ç±»å‹æ£€æŸ¥
                facet_scores = novelty.facet_scores
                if isinstance(facet_scores, dict):
                    if facet_scores.get('methodological', 7) < 7:
                        criteria.append("æ–¹æ³•æ–°é¢–æ€§åˆ†é¢å¾—åˆ† >= 7.0")
                    if facet_scores.get('conceptual', 7) < 7:
                        criteria.append("æ¦‚å¿µæ–°é¢–æ€§åˆ†é¢å¾—åˆ† >= 7.0")
            
            # å¯è¡Œæ€§å…·ä½“æ ‡å‡†
            if feasibility.feasibility_score < self.default_thresholds["feasibility_threshold"]:
                # èµ„äº§å¯å¾—æ€§æ ‡å‡† - æ·»åŠ ç±»å‹æ£€æŸ¥
                missing_assets = []
                for asset in feasibility.required_assets:
                    if isinstance(asset, dict) and asset.get('status') == 'missing':
                        missing_assets.append(asset)
                if missing_assets:
                    criteria.append("æ‰€æœ‰æ ‡è®°ä¸º'ç¼ºå¤±'çš„å…³é”®èµ„äº§å·²è¢«æ›¿æ¢æˆ–æä¾›æ›¿ä»£æ–¹æ¡ˆ")
                
                # é£é™©ç¼“è§£æ ‡å‡† - æ·»åŠ ç±»å‹æ£€æŸ¥
                high_risks = []
                for risk in feasibility.potential_risks:
                    if isinstance(risk, dict) and risk.get('severity') == 'high':
                        high_risks.append(risk)
                if high_risks:
                    criteria.append("æ‰€æœ‰é«˜é£é™©é¡¹å·²åˆ¶å®šå…·ä½“çš„ç¼“è§£ç­–ç•¥")
                
                # ç»´åº¦æ”¹è¿›æ ‡å‡† - æ·»åŠ ç±»å‹æ£€æŸ¥
                dimension_scores = feasibility.dimension_scores
                if isinstance(dimension_scores, dict):
                    if dimension_scores.get('asset_availability', 7) < 6:
                        criteria.append("èµ„äº§å¯å¾—æ€§ç»´åº¦å¾—åˆ† >= 6.0")
                    if dimension_scores.get('relevance', 7) < 6:
                        criteria.append("é¢†åŸŸç›¸å…³æ€§ç»´åº¦å¾—åˆ† >= 6.0")
        
        elif decision == "split":
            criteria.append("å·²æˆåŠŸæ‹†åˆ†ä¸º2-3ä¸ªç‹¬ç«‹çš„èšç„¦å­æƒ³æ³•")
            criteria.append("æ¯ä¸ªå­æƒ³æ³•éƒ½æœ‰æ˜ç¡®çš„ç ”ç©¶è¾¹ç•Œå’Œæ ¸å¿ƒé—®é¢˜")
            criteria.append("æ‹†åˆ†åçš„æƒ³æ³•å¤æ‚åº¦é€‚ä¸­ä¸”å„è‡ªå…·æœ‰ç ”ç©¶ä»·å€¼")
        
        elif decision == "merge":
            criteria.append("å·²ä¸å…¶ä»–ç›¸å…³æƒ³æ³•æˆåŠŸåˆå¹¶æˆ–å†…å®¹æ·±åº¦æ˜¾è‘—å¢å¼º")
            criteria.append("åˆå¹¶åçš„æƒ³æ³•å†…å®¹æ›´åŠ ä¸°å¯Œä¸”é€»è¾‘è¿è´¯")
            criteria.append("åˆ›æ–°ç‚¹æ›´åŠ çªå‡ºä¸”ç ”ç©¶ä»·å€¼æ˜æ˜¾æå‡")
        
        elif decision == "discard":
            criteria.append("å·²ç¡®è®¤æƒ³æ³•æ— æ³•é€šè¿‡åˆç†ä¿®æ”¹è¾¾åˆ°è´¨é‡è¦æ±‚")
        
        return criteria

    async def _generate_rationale(self, novelty: NoveltyCritique, feasibility: FeasibilityCritique, decision: str) -> str:
        """åŸºäºLLMç”Ÿæˆå†³ç­–ç†ç”±ã€‚
        
        è¾“å…¥:
            - novelty: æ–°é¢–æ€§è¯„å®¡ã€‚
            - feasibility: å¯è¡Œæ€§è¯„å®¡ã€‚
            - decision: å†³ç­–ç±»å‹ã€‚
            
        è¾“å‡º:
            - str: å†³ç­–ç†ç”±è¯´æ˜ã€‚
        """
        combined_score = novelty.novelty_score + feasibility.feasibility_score
        
        prompt = f"""ä½œä¸ºç ”ç©¶æƒ³æ³•è¯„å®¡ä¸“å®¶ï¼Œè¯·ä¸ºä»¥ä¸‹å†³ç­–ç”Ÿæˆç®€æ´æœ‰åŠ›çš„ç†ç”±è¯´æ˜ã€‚

**è¯„å®¡æ‘˜è¦**ï¼š
- æ–°é¢–æ€§è¯„åˆ†: {novelty.novelty_score:.1f}/10.0
- å¯è¡Œæ€§è¯„åˆ†: {feasibility.feasibility_score:.1f}/10.0
- ç»¼åˆè¯„åˆ†: {combined_score:.1f}/20.0
- ç›¸ä¼¼å·¥ä½œæ•°é‡: {len(novelty.similar_works)}
- é£é™©ç‚¹æ•°é‡: {len(feasibility.potential_risks)}

**åšå‡ºçš„å†³ç­–**: {decision}

è¯·ç”¨1-2å¥è¯è§£é‡Šè¿™ä¸ªå†³ç­–çš„åˆç†æ€§ï¼Œé‡ç‚¹è¯´æ˜ï¼š
1. å†³ç­–çš„ä¸»è¦ä¾æ®ï¼ˆåˆ†æ•°æ°´å¹³ã€å…³é”®é—®é¢˜ã€æ”¹è¿›æ½œåŠ›ç­‰ï¼‰
2. é¢„æœŸçš„åç»­è¡ŒåŠ¨æ–¹å‘

è¦æ±‚ï¼š
- è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¿å…å†—ä½™
- çªå‡ºå…³é”®æ•°æ®å’Œäº‹å®ä¾æ®
- ä½“ç°ä¸“ä¸šåˆ¤æ–­çš„é€»è¾‘æ€§

ç›´æ¥è¾“å‡ºç†ç”±è¯´æ˜æ–‡æœ¬ï¼Œæ— éœ€æ ¼å¼åŒ–ã€‚"""

        try:
            llm = self.llm
            
            response_data = await llm.generate(
                model_name=self.config.model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=15000,
                temperature=0.2,
                agent_name=self.name,
                task_type="refinement_rationale"
            )
            response = response_data.get('content', '')
            
            rationale = response.strip()
            return rationale if rationale else self._fallback_rationale(novelty, feasibility, decision)
            
        except Exception as e:
            print(f"    âš ï¸ LLMç†ç”±ç”Ÿæˆå¤±è´¥: {e}")
            return self._fallback_rationale(novelty, feasibility, decision)
    
    def _fallback_rationale(self, novelty: NoveltyCritique, feasibility: FeasibilityCritique, decision: str) -> str:
        """å¤‡ç”¨å†³ç­–ç†ç”±ç”Ÿæˆï¼Œå½“LLMè°ƒç”¨å¤±è´¥æ—¶ä½¿ç”¨ã€‚"""
        combined_score = novelty.novelty_score + feasibility.feasibility_score
        
        if decision == "accept":
            return f"æƒ³æ³•è¾¾åˆ°éªŒæ”¶æ ‡å‡†ï¼šæ–°é¢–æ€§{novelty.novelty_score:.1f}/10.0ï¼Œå¯è¡Œæ€§{feasibility.feasibility_score:.1f}/10.0ï¼Œç»¼åˆè¡¨ç°ä¼˜ç§€"
        elif decision == "revise":
            return f"æƒ³æ³•æœ‰æ½œåŠ›ä½†éœ€æ”¹è¿›ï¼šæ–°é¢–æ€§{novelty.novelty_score:.1f}ï¼Œå¯è¡Œæ€§{feasibility.feasibility_score:.1f}ï¼Œé€šè¿‡é’ˆå¯¹æ€§ä¿®æ”¹å¯æå‡è´¨é‡"
        elif decision == "split":
            return f"æƒ³æ³•è¿‡äºå¤æ‚ï¼ˆç»¼åˆåˆ†{combined_score:.1f}ï¼‰ï¼Œå»ºè®®æ‹†åˆ†ä¸ºå¤šä¸ªèšç„¦çš„å­é—®é¢˜"
        elif decision == "merge":
            return f"æƒ³æ³•è¾ƒä¸ºå•è–„ï¼ˆç»¼åˆåˆ†{combined_score:.1f}ï¼‰ï¼Œå»ºè®®ä¸å…¶ä»–æƒ³æ³•åˆå¹¶æˆ–å¢å¼ºå†…å®¹æ·±åº¦"
        elif decision == "discard":
            return f"æƒ³æ³•è´¨é‡ä¸è¶³ï¼ˆç»¼åˆåˆ†{combined_score:.1f}ï¼‰ä¸”æ”¹è¿›æ½œåŠ›æœ‰é™ï¼Œå»ºè®®ä¸¢å¼ƒ"
        else:
            return f"åŸºäºè¯„å®¡ç»“æœåšå‡º{decision}å†³ç­–ï¼Œç»¼åˆè¯„åˆ†{combined_score:.1f}/20.0"


# =========================
# åè°ƒå™¨ï¼ˆéª¨æ¶ï¼‰
# =========================


class IdeaGenCoordinator:
    """Idea Generation å­ç³»ç»Ÿçš„æ€»åè°ƒå™¨ã€‚

    ç›®æ ‡:
        - ç®¡ç† 5 ä¸ªæ™ºèƒ½ä½“çš„è°ƒç”¨é¡ºåºä¸å¹¶å‘ï¼Œæä¾›ç«¯åˆ°ç«¯çš„æƒ³æ³•ç”Ÿæˆä¸è¿­ä»£æµç¨‹ã€‚

    è¾“å…¥:
        - llm_factory: ç»Ÿä¸€LLMå·¥å‚ã€‚
        - db: å­¦æœ¯æ•°æ®åº“ã€‚
        - config: è¿è¡Œæ—¶é…ç½®ï¼ˆå¹¶å‘ã€é˜ˆå€¼ã€æœ€å¤§è¿­ä»£è½®æ¬¡ç­‰ï¼‰ã€‚

    è¾“å‡º:
        - ç»“æ„åŒ–å­—å…¸ï¼ŒåŒ…æ‹¬æœºä¼šå›¾è°±ã€å€™é€‰æƒ³æ³•ã€å„è½®è¯„å®¡ä¸ç²¾ç‚¼è®°å½•ã€æœ€ç»ˆæƒ³æ³•é›†ã€‚

    æ³¨æ„äº‹é¡¹:
        - å¹¶å‘ä¸Šé™é»˜è®¤=6ï¼›å¯é€šè¿‡ config è¦†ç›–ã€‚
        - å„é˜¶æ®µçš„äº§å‡ºéœ€åŒ…å« `provenance` ä»¥ä¾¿è¿½æº¯ã€‚
    """

    def __init__(self, llm_factory: LLMFactory, db: AcademicPaperDatabase, config: Optional[Dict[str, Any]] = None):
        self.llm_factory = llm_factory
        self.db = db
        self.config = config or {}
        
        # é…ç½®å‚æ•°è§£æ
        self.concurrency: int = int(self.config.get("idea_concurrency", 6))
        self.max_rounds: int = int(self.config.get("max_rounds", 3))
        self.novelty_threshold: float = float(self.config.get("novelty_threshold", 8.0))
        self.feasibility_threshold: float = float(self.config.get("feasibility_threshold", 7.0))
        self.max_initial_ideas: int = int(self.config.get("max_initial_ideas", 6))
        
        # ç»„è£…æ™ºèƒ½ä½“ï¼ˆå¯åœ¨å¤–éƒ¨ä»¥ä¾èµ–æ³¨å…¥æ–¹å¼è¦†ç›–ï¼‰
        # åˆ›å»ºå„æ™ºèƒ½ä½“çš„é…ç½®
        miner_config = AgentConfig(
            model_name=self.config.get("miner_model", ModelType.GEMINI.value),
            temperature=0.7,
            max_tokens=15000,
            role_description="æœºä¼šå›¾è°±æ„å»ºä¸“å®¶",
            system_message="ä½ æ˜¯æœºä¼šå›¾è°±æ„å»ºä¸“å®¶ï¼Œæ“…é•¿ä»å­¦æœ¯æ–‡çŒ®ä¸­æŠ½å–å®ä½“å…³ç³»å¹¶è¯†åˆ«ç ”ç©¶æœºä¼šã€‚"
        )
        
        generator_config = AgentConfig(
            model_name=self.config.get("generator_model", ModelType.GEMINI.value),
            temperature=0.8,
            max_tokens=15000,
            role_description="ç ”ç©¶æƒ³æ³•ç”Ÿæˆä¸“å®¶",
            system_message="ä½ æ˜¯ç ”ç©¶æƒ³æ³•ç”Ÿæˆä¸“å®¶ï¼Œæ“…é•¿åŸºäºæœºä¼šå›¾è°±ç”Ÿæˆåˆ›æ–°çš„ç ”ç©¶æƒ³æ³•ã€‚"
        )
        
        novelty_critic_config = AgentConfig(
            model_name=self.config.get("novelty_critic_model", ModelType.GEMINI.value),
            temperature=0.3,
            max_tokens=15000,
            role_description="æ–°é¢–æ€§è¯„å®¡ä¸“å®¶",
            system_message="ä½ æ˜¯æ–°é¢–æ€§è¯„å®¡ä¸“å®¶ï¼Œæ“…é•¿è¯„ä¼°ç ”ç©¶æƒ³æ³•çš„åˆ›æ–°æ€§å’Œç‹¬ç‰¹æ€§ã€‚"
        )
        
        feasibility_critic_config = AgentConfig(
            model_name=self.config.get("feasibility_critic_model", ModelType.GEMINI.value),
            temperature=0.3,
            max_tokens=15000,
            role_description="å¯è¡Œæ€§è¯„å®¡ä¸“å®¶",
            system_message="ä½ æ˜¯å¯è¡Œæ€§è¯„å®¡ä¸“å®¶ï¼Œæ“…é•¿è¯„ä¼°ç ”ç©¶æƒ³æ³•çš„æŠ€æœ¯å¯è¡Œæ€§å’Œå®ç°éš¾åº¦ã€‚"
        )
        
        refiner_config = AgentConfig(
            model_name=self.config.get("refiner_model", ModelType.GEMINI.value),
            temperature=0.5,
            max_tokens=15000,
            role_description="æƒ³æ³•ç²¾ç‚¼ä¸“å®¶",
            system_message="ä½ æ˜¯æƒ³æ³•ç²¾ç‚¼ä¸“å®¶ï¼Œæ“…é•¿æ ¹æ®è¯„å®¡æ„è§ä¼˜åŒ–å’Œæ”¹è¿›ç ”ç©¶æƒ³æ³•ã€‚"
        )
        
        self.miner = IdeaMinerAgent("IdeaMiner", llm_factory, db, miner_config)
        self.generator = IdeaGeneratorAgent("IdeaGenerator", llm_factory, db, generator_config)
        self.novelty_critic = NoveltyCriticAgent("NoveltyCritic", llm_factory, db, novelty_critic_config)
        self.feasibility_critic = FeasibilityCriticAgent("FeasibilityCritic", llm_factory, db, feasibility_critic_config)
        self.refiner = IdeaRefinerAgent("IdeaRefiner", llm_factory, db, refiner_config)
        
        print(f"ğŸ—ï¸ åè°ƒå™¨åˆå§‹åŒ–å®Œæˆ - å¹¶å‘åº¦: {self.concurrency}, æœ€å¤§è½®æ¬¡: {self.max_rounds}")

    async def run_pipeline(self, final_result: Dict[str, Any], enriched_outline: Dict[str, Any]) -> Dict[str, Any]:
        """ç«¯åˆ°ç«¯æ‰§è¡Œï¼šå›¾è°±â†’ç”Ÿæˆâ†’è¯„å®¡â†’ç²¾ç‚¼â†’æ”¶æ•›ã€‚

        è¾“å…¥:
            - final_result: ä¸Šæ¸¸ Survey Gen çš„æ•´åˆç»“æœã€‚
            - enriched_outline: ä¸Šæ¸¸ä¸°å¯Œå¤§çº²ã€‚

        è¾“å‡º:
            - Dict: ç»“æ„åŒ–äº§å‡ºï¼ŒåŒ…æ‹¬ graph/candidates/iterations/final_ideas ç­‰ã€‚

        å®ç°æ­¥éª¤å»ºè®®:
            1) è°ƒç”¨ miner æ„å»º SemanticOpportunityGraphã€‚
            2) è°ƒç”¨ generator ç”Ÿæˆåˆå§‹å€™é€‰æƒ³æ³•ï¼ˆâ‰¤å¹¶å‘ä¸Šé™ï¼‰ã€‚
            3) å¯¹æ¯ä¸ªæƒ³æ³•å¹¶è¡Œæ‰§è¡Œ debate_loopï¼ˆæ–°é¢–æ€§/å¯è¡Œæ€§â†’ç²¾ç‚¼â†’é‡ç”Ÿï¼‰ã€‚
            4) æ±‡æ€»å„æƒ³æ³•çš„ç‰ˆæœ¬é“¾ä¸ç»ˆæ­¢çŠ¶æ€ï¼Œç”Ÿæˆæœ€ç»ˆæŠ¥è¡¨ã€‚
        """
        print("ğŸ” ç¬¬ä¸€é˜¶æ®µï¼šæ„å»ºè¯­ä¹‰æœºä¼šå›¾è°±")
        start_time = datetime.now()
        
        try:
            # é˜¶æ®µ1ï¼šæ„å»ºæœºä¼šå›¾è°±
            graph = await self.miner.build_opportunity_graph(final_result, enriched_outline)
            stage1_time = (datetime.now() - start_time).total_seconds()
            print(f"   âœ… å›¾è°±æ„å»ºå®Œæˆ - {graph.number_of_nodes()}èŠ‚ç‚¹, {graph.number_of_edges()}è¾¹, è€—æ—¶{stage1_time:.1f}ç§’")
        except Exception as e:
            print(f"   âŒ å›¾è°±æ„å»ºå¤±è´¥: {e}")
            raise RuntimeError(f"å›¾è°±æ„å»ºé˜¶æ®µå¤±è´¥: {e}") from e
        
        print("ğŸ’¡ ç¬¬äºŒé˜¶æ®µï¼šç”Ÿæˆå€™é€‰æƒ³æ³•")
        stage2_start = datetime.now()
        
        try:
            # é˜¶æ®µ2ï¼šç”Ÿæˆåˆå§‹å€™é€‰æƒ³æ³•
            generation_result = await self.generator.generate_candidates(graph, max_ideas=self.max_initial_ideas)
            initial_candidates = generation_result['all_candidates']
            stage2_time = (datetime.now() - stage2_start).total_seconds()
            print(f"   âœ… å€™é€‰æƒ³æ³•ç”Ÿæˆå®Œæˆ - {len(initial_candidates)}ä¸ªæƒ³æ³•, è€—æ—¶{stage2_time:.1f}ç§’")
            
            if not initial_candidates:
                print("   âš ï¸ æœªç”Ÿæˆä»»ä½•å€™é€‰æƒ³æ³•ï¼Œæµç¨‹æå‰ç»“æŸ")
                return {
                    "opportunity_graph": {
                        "node_count": graph.number_of_nodes(),
                        "edge_count": graph.number_of_edges(),
                        "gaps_count": len(graph.graph.get('gaps', [])),
                        "provenance": graph.graph.get('provenance', {})
                    },
                    "initial_candidates": {"count": 0, "ideas": []},
                    "final_ideas": {"accepted": {"count": 0, "ideas": [], "avg_scores": {}}, 
                                  "discarded": {"count": 0, "ideas": [], "reasons": []},
                                  "failed": {"count": 0, "errors": []}},
                    "statistics": {"total_candidates": 0, "success_rate": 0.0},
                    "timestamp": datetime.now().isoformat(),
                    "status": "no_candidates_generated"
                }
        except Exception as e:
            print(f"   âŒ å€™é€‰æƒ³æ³•ç”Ÿæˆå¤±è´¥: {e}")
            raise RuntimeError(f"æƒ³æ³•ç”Ÿæˆé˜¶æ®µå¤±è´¥: {e}") from e
        
        print(f"ğŸ›ï¸ ç¬¬ä¸‰é˜¶æ®µï¼šè¾©è®ºç«æŠ€åœºï¼ˆ{len(initial_candidates)}ä¸ªæƒ³æ³•ï¼Œæ‰¹é‡è¯„å®¡æ¨¡å¼ï¼‰")
        stage3_start = datetime.now()
        
        # é˜¶æ®µ3ï¼šæ‰¹é‡è¯„å®¡æ¨¡å¼çš„è¾©è®ºå¾ªç¯
        debate_results = await self._batch_debate_arena(generation_result, graph)
        
        # å¤„ç†å¼‚å¸¸ç»“æœ
        processed_results = []
        for i, result in enumerate(debate_results):
            if isinstance(result, Exception):
                processed_results.append({
                    "status": "failed",
                    "final_idea": initial_candidates[i],
                    "history": [],
                    "error": str(result)
                })
            else:
                processed_results.append(result)
        
        stage3_time = (datetime.now() - stage3_start).total_seconds()
        print(f"   âœ… è¾©è®ºç«æŠ€åœºå®Œæˆï¼Œè€—æ—¶{stage3_time:.1f}ç§’")
        
        # é˜¶æ®µ4ï¼šæ•´ç†æœ€ç»ˆç»“æœ
        print("ğŸ“Š ç¬¬å››é˜¶æ®µï¼šæ•´ç†æœ€ç»ˆç»“æœ")
        final_results = await self._synthesize_final_results(graph, initial_candidates, processed_results)
        
        total_time = (datetime.now() - start_time).total_seconds()
        final_results["execution_details"] = {
            "stage_times": {
                "graph_construction": stage1_time,
                "idea_generation": stage2_time,
                "debate_arena": stage3_time,
                "result_synthesis": (datetime.now() - start_time).total_seconds() - stage1_time - stage2_time - stage3_time
            },
            "total_time": total_time
        }
        
        print(f"âœ… æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼Œæ€»è€—æ—¶{total_time:.1f}ç§’")
        return final_results

    async def debate_loop(
        self,
        idea: CandidateIdea,
        graph: SemanticOpportunityGraph,
        max_rounds: Optional[int] = None,
        novelty_threshold: Optional[float] = None,
        feasibility_threshold: Optional[float] = None,
    ) -> Dict[str, Any]:
        """è¾©è®ºç«æŠ€åœºï¼šé’ˆå¯¹å•ä¸ªæƒ³æ³•çš„è¿­ä»£é—­ç¯ã€‚

        è¾“å…¥:
            - idea: åˆå§‹å€™é€‰æƒ³æ³•ã€‚
            - graph: è¯­ä¹‰æœºä¼šå›¾è°±ã€‚
            - max_rounds: æœ€å¤§è¿­ä»£è½®æ•°ï¼ˆé»˜è®¤3ï¼‰ã€‚
            - novelty_threshold/feasibility_threshold: æ¥å—é˜ˆå€¼ã€‚

        è¾“å‡º:
            - Dict: {final_idea, history: [{novelty, feasibility, refinement, version}, ...], status}

        å®ç°æ­¥éª¤å»ºè®®:
            1) åŒæ­¥/å¹¶è¡Œè·å– NoveltyCritique ä¸ FeasibilityCritiqueã€‚
            2) äº¤ç”± IdeaRefiner ç”Ÿæˆ RefinementPrompt å¹¶å†³ç­–ï¼ˆrevise/split/merge/discard/acceptï¼‰ã€‚
            3) è‹¥ reviseï¼Œåˆ™å›åˆ° IdeaGenerator ç”Ÿæˆæ–°ç‰ˆæœ¬ idea(version+1) å¹¶ç»§ç»­ï¼›è¾¾æˆé˜ˆå€¼æˆ–ä¸Šé™åˆ™ç»ˆæ­¢ã€‚
        """
        # ä½¿ç”¨é…ç½®å‚æ•°çš„é»˜è®¤å€¼
        max_rounds = max_rounds or self.max_rounds
        novelty_threshold = novelty_threshold or self.novelty_threshold
        feasibility_threshold = feasibility_threshold or self.feasibility_threshold
        
        current_idea = idea
        history = []
        
        for round_num in range(max_rounds):
            print(f"  ğŸ”„ æƒ³æ³• {idea.id} ç¬¬ {round_num + 1} è½®è¯„å®¡ï¼ˆç‰ˆæœ¬ {current_idea.version}ï¼‰")
            
            try:
                # å¹¶è¡Œæ‰§è¡Œæ–°é¢–æ€§ä¸å¯è¡Œæ€§è¯„å®¡
                novelty_task = self.novelty_critic.assess_novelty(current_idea)
                feasibility_task = self.feasibility_critic.assess_feasibility(current_idea, graph)
                
                novelty_critique, feasibility_critique = await asyncio.gather(novelty_task, feasibility_task)
                
                print(f"    ğŸ“ˆ è¯„å®¡å®Œæˆ - æ–°é¢–æ€§: {novelty_critique.novelty_score:.1f}, å¯è¡Œæ€§: {feasibility_critique.feasibility_score:.1f}")
                
            except Exception as e:
                print(f"    âŒ è¯„å®¡å¤±è´¥: {e}")
                return {
                    "status": "failed",
                    "final_idea": current_idea,
                    "history": history,
                    "error": f"è¯„å®¡é˜¶æ®µå¤±è´¥: {str(e)}"
                }
            
            # ç”Ÿæˆç²¾ç‚¼æŒ‡ä»¤
            refinement_prompt = await self.refiner.make_refinement_prompt(
                current_idea, novelty_critique, feasibility_critique
            )
            
            # è®°å½•æœ¬è½®å†å²
            round_record = {
                "round": round_num + 1,
                "idea_version": current_idea.version,
                "novelty": novelty_critique,
                "feasibility": feasibility_critique,
                "refinement": refinement_prompt,
                "timestamp": datetime.now().isoformat()
            }
            history.append(round_record)
            
            # æ ¹æ®å†³ç­–ç¡®å®šä¸‹ä¸€æ­¥
            decision = refinement_prompt.decision
            print(f"    ğŸ¤” å†³ç­–ç»“æœ: {decision}")
            
            if decision == "accept":
                print(f"    âœ… æƒ³æ³• {idea.id} è¢«æ¥å—ï¼")
                return {
                    "status": "accepted",
                    "final_idea": current_idea,
                    "history": history,
                    "final_scores": {
                        "novelty": novelty_critique.novelty_score,
                        "feasibility": feasibility_critique.feasibility_score,
                        "combined": novelty_critique.novelty_score + feasibility_critique.feasibility_score
                    },
                    "acceptance_round": round_num + 1
                }
            elif decision == "discard":
                print(f"    âŒ æƒ³æ³• {idea.id} è¢«ä¸¢å¼ƒ: {refinement_prompt.rationale}")
                return {
                    "status": "discarded",
                    "final_idea": current_idea,
                    "history": history,
                    "discard_reason": refinement_prompt.rationale,
                    "discard_round": round_num + 1
                }
            elif decision == "revise":
                # ç”Ÿæˆæ–°ç‰ˆæœ¬
                try:
                    current_idea = await self.generator.refine_idea(current_idea, refinement_prompt, graph)
                    print(f"    âœ¨ ç”Ÿæˆæƒ³æ³• {idea.id} çš„ç¬¬ {current_idea.version} ç‰ˆæœ¬")
                except Exception as e:
                    return {
                        "status": "failed",
                        "final_idea": current_idea,
                        "history": history,
                        "error": str(e)
                    }
            elif decision == "split":
                # å¤„ç†æ‹†åˆ†å†³ç­–
                print(f"    âœ‚ï¸ æƒ³æ³• {idea.id} éœ€è¦æ‹†åˆ†: {refinement_prompt.rationale}")
                return {
                    "status": "split_required",
                    "final_idea": current_idea,
                    "history": history,
                    "split_instructions": refinement_prompt.instructions,
                    "rationale": refinement_prompt.rationale,
                    "split_round": round_num + 1
                }
            elif decision == "merge":
                # å¤„ç†åˆå¹¶å†³ç­–
                print(f"    ğŸ”— æƒ³æ³• {idea.id} éœ€è¦åˆå¹¶: {refinement_prompt.rationale}")
                return {
                    "status": "merge_required",
                    "final_idea": current_idea,
                    "history": history,
                    "merge_instructions": refinement_prompt.instructions,
                    "rationale": refinement_prompt.rationale,
                    "merge_round": round_num + 1
                }
            else:
                # å¤„ç†æœªçŸ¥å†³ç­–ç±»å‹
                print(f"    âš ï¸ æœªæ”¯æŒçš„å†³ç­–ç±»å‹: {decision}")
                return {
                    "status": "unsupported_decision",
                    "final_idea": current_idea,
                    "history": history,
                    "decision": decision,
                    "rationale": refinement_prompt.rationale
                }
        
        # è¾¾åˆ°æœ€å¤§è½®æ•°
        print(f"    â° æƒ³æ³• {idea.id} è¾¾åˆ°æœ€å¤§è½®æ•° {max_rounds}ï¼Œè¿­ä»£ç»“æŸ")
        final_novelty = history[-1]["novelty"].novelty_score if history else 0.0
        final_feasibility = history[-1]["feasibility"].feasibility_score if history else 0.0
        
        return {
            "status": "max_rounds_reached",
            "final_idea": current_idea,
            "history": history,
            "final_scores": {
                "novelty": final_novelty,
                "feasibility": final_feasibility,
                "combined": final_novelty + final_feasibility
            },
            "rounds_completed": len(history)
        }
    
    async def _batch_debate_arena(self, generation_result: Dict[str, Any], 
                                graph: SemanticOpportunityGraph) -> List[Dict[str, Any]]:
        """æ‰¹é‡è¯„å®¡æ¨¡å¼çš„è¾©è®ºç«æŠ€åœºã€‚
        
        å®ç°ç”¨æˆ·æè¿°çš„æ¶æ„ï¼š
        1. Generatorç”Ÿæˆ5æ‰¹ï¼Œæ¯æ‰¹10ä¸ªæƒ³æ³•
        2. æ¯æ‰¹10ä¸ªæƒ³æ³•ä¸€èµ·é€ç»™ä¸¤ç±»è¯„å®¡Agentå¹¶è¡Œè¯„åˆ†
        3. æ¯ä¸ªæƒ³æ³•å†å•ç‹¬ç»è¿‡refinerå¤„ç†
        """
        idea_batches = generation_result.get('batches', [])
        total_ideas = sum(len(batch) for batch in idea_batches)
        all_results = []
        
        print(f"ğŸ›ï¸ ç¬¬ä¸‰é˜¶æ®µï¼šè¾©è®ºç«æŠ€åœºï¼ˆ{total_ideas}ä¸ªæƒ³æ³•ï¼Œ{len(idea_batches)}æ‰¹æ¬¡è¯„å®¡æ¨¡å¼ï¼‰")
        
        # æŒ‰ç…§Generatorçš„åŸå§‹æ‰¹æ¬¡ç»“æ„å¤„ç†
        for batch_num, batch_ideas in enumerate(idea_batches, 1):
            print(f"   ğŸ“¦ ç¬¬{batch_num}æ‰¹è¯„å®¡ï¼š{len(batch_ideas)}ä¸ªæƒ³æ³•ï¼ˆGeneratorç¬¬{batch_num}æ‰¹ï¼‰")
            
            try:
                # çœŸæ­£çš„æ‰¹é‡è¯„å®¡ï¼šä¸€æ¬¡LLMè°ƒç”¨è¯„ä¼°æ•´æ‰¹10ä¸ªæƒ³æ³•
                print(f"      ğŸ” æ‰¹é‡æ–°é¢–æ€§ä¸å¯è¡Œæ€§è¯„å®¡ï¼ˆçœŸæ­£çš„æ‰¹é‡LLMè°ƒç”¨ï¼‰...")
                novelty_task = self.novelty_critic.assess_batch_comprehensive(batch_ideas, graph)
                feasibility_task = self.feasibility_critic.assess_batch_comprehensive(batch_ideas, graph)
                
                novelty_critiques, feasibility_critiques = await asyncio.gather(
                    novelty_task, feasibility_task
                )
                
                print(f"      âœ… æ‰¹é‡è¯„å®¡å®Œæˆ")
                
                # æ£€æŸ¥è¯„å®¡ç»“æœæ•°é‡æ˜¯å¦åŒ¹é…
                if len(novelty_critiques) != len(batch_ideas) or len(feasibility_critiques) != len(batch_ideas):
                    print(f"      âš ï¸ è¯„å®¡ç»“æœæ•°é‡ä¸åŒ¹é…ï¼Œé¢„æœŸ{len(batch_ideas)}ä¸ªï¼Œå®é™…æ–°é¢–æ€§{len(novelty_critiques)}ä¸ªï¼Œå¯è¡Œæ€§{len(feasibility_critiques)}ä¸ª")
                    # ä¸ºæ‰€æœ‰æƒ³æ³•åˆ›å»ºå¤±è´¥ç»“æœ
                    batch_results = [{
                        "status": "failed",
                        "final_idea": idea,
                        "history": [],
                        "error": "æ‰¹é‡è¯„å®¡ç»“æœæ•°é‡ä¸åŒ¹é…"
                    } for idea in batch_ideas]
                else:
                    # æ˜¾ç¤ºæ¯ä¸ªæƒ³æ³•çš„è¯„å®¡åˆ†æ•°
                    for i, idea in enumerate(batch_ideas):
                        novelty = novelty_critiques[i]
                        feasibility = feasibility_critiques[i]
                        print(f"      ğŸ’¡ æƒ³æ³• {idea.id}: æ–°é¢–æ€§{novelty.novelty_score:.1f}, å¯è¡Œæ€§{feasibility.feasibility_score:.1f}")
                    
                    # ğŸš€ æ‰¹é‡å¹¶è¡Œç²¾ç‚¼å¤„ç†ï¼ˆæ›¿ä»£é€ä¸ªå¤„ç†ï¼‰
                    print(f"      ğŸ”„ å¯åŠ¨æ‰¹é‡ç²¾ç‚¼å¤„ç†ï¼ˆ{len(batch_ideas)}ä¸ªæƒ³æ³•ï¼‰...")
                    batch_results = await self._batch_idea_refinement(
                        batch_ideas, novelty_critiques, feasibility_critiques, graph
                    )
                
                all_results.extend(batch_results)
                print(f"      âœ… ç¬¬{batch_num}æ‰¹å¤„ç†å®Œæˆï¼ˆæ‰¹é‡ç²¾ç‚¼æ¨¡å¼ï¼‰")
                
            except Exception as e:
                print(f"      âŒ ç¬¬{batch_num}æ‰¹å¤„ç†å¤±è´¥: {e}")
                # ä¸ºè¿™æ‰¹çš„æ‰€æœ‰æƒ³æ³•åˆ›å»ºå¤±è´¥ç»“æœ
                for idea in batch_ideas:
                    all_results.append({
                        "status": "failed",
                        "final_idea": idea,
                        "history": [],
                        "error": f"æ‰¹é‡è¯„å®¡å¤±è´¥: {str(e)}"
                    })
        
        return all_results
    
    async def _single_idea_refinement(self, idea: CandidateIdea, 
                                    novelty: 'NoveltyCritique', 
                                    feasibility: 'FeasibilityCritique',
                                    graph: SemanticOpportunityGraph) -> Dict[str, Any]:
        """é’ˆå¯¹å•ä¸ªæƒ³æ³•çš„ç²¾ç‚¼å¤„ç†ï¼ˆå·²æœ‰è¯„å®¡ç»“æœï¼‰ã€‚"""
        current_idea = idea
        history = []
        
        # åˆå§‹è¯„å®¡è®°å½•
        round_record = {
            "round": 1,
            "idea_version": current_idea.version,
            "novelty": novelty,
            "feasibility": feasibility,
            "timestamp": datetime.now().isoformat()
        }
        
        try:
            # ç”Ÿæˆç²¾ç‚¼æŒ‡ä»¤
            refinement_prompt = await self.refiner.make_refinement_prompt(
                current_idea, novelty, feasibility
            )
            round_record["refinement"] = refinement_prompt
            history.append(round_record)
            
            decision = refinement_prompt.decision
            print(f"        ğŸ¤” æƒ³æ³• {idea.id} å†³ç­–: {decision}")
            
            if decision == "accept":
                return {
                    "status": "accepted",
                    "final_idea": current_idea,
                    "history": history,
                    "final_scores": {
                        "novelty": novelty.novelty_score,
                        "feasibility": feasibility.feasibility_score,
                        "combined": novelty.novelty_score + feasibility.feasibility_score
                    },
                    "acceptance_round": 1
                }
            elif decision == "discard":
                return {
                    "status": "discarded",
                    "final_idea": current_idea,
                    "history": history,
                    "discard_reason": refinement_prompt.rationale,
                    "discard_round": 1
                }
            elif decision == "revise":
                # å¯¹äºéœ€è¦ä¿®è®¢çš„æƒ³æ³•ï¼Œå¯ä»¥è¿›è¡Œä¸€è½®æ”¹è¿›
                try:
                    current_idea = await self.generator.refine_idea(current_idea, refinement_prompt, graph)
                    return {
                        "status": "revised",
                        "final_idea": current_idea,
                        "history": history,
                        "revision_round": 1
                    }
                except Exception as e:
                    return {
                        "status": "failed",
                        "final_idea": current_idea,
                        "history": history,
                        "error": f"ä¿®è®¢å¤±è´¥: {str(e)}"
                    }
            elif decision == "split":
                return {
                    "status": "split_required",
                    "final_idea": current_idea,
                    "history": history,
                    "split_instructions": refinement_prompt.instructions,
                    "rationale": refinement_prompt.rationale,
                    "split_round": 1
                }
            elif decision == "merge":
                return {
                    "status": "merge_required",
                    "final_idea": current_idea,
                    "history": history,
                    "merge_instructions": refinement_prompt.instructions,
                    "rationale": refinement_prompt.rationale,
                    "merge_round": 1
                }
            else:
                return {
                    "status": "unsupported_decision",
                    "final_idea": current_idea,
                    "history": history,
                    "decision": decision,
                    "rationale": refinement_prompt.rationale
                }
                
        except Exception as e:
            round_record["error"] = str(e)
            history.append(round_record)
            return {
                "status": "failed",
                "final_idea": current_idea,
                "history": history,
                "error": f"ç²¾ç‚¼å¤„ç†å¤±è´¥: {str(e)}"
            }

    async def _batch_idea_refinement(self, batch_ideas: List[CandidateIdea], 
                                   novelty_critiques: List['NoveltyCritique'], 
                                   feasibility_critiques: List['FeasibilityCritique'],
                                   graph: SemanticOpportunityGraph) -> List[Dict[str, Any]]:
        """æ‰¹é‡å¹¶è¡Œå¤„ç†æƒ³æ³•ç²¾ç‚¼ã€‚
        
        è¾“å…¥:
            - batch_ideas: æƒ³æ³•åˆ—è¡¨ã€‚
            - novelty_critiques: æ–°é¢–æ€§è¯„å®¡ç»“æœåˆ—è¡¨ã€‚
            - feasibility_critiques: å¯è¡Œæ€§è¯„å®¡ç»“æœåˆ—è¡¨ã€‚
            - graph: è¯­ä¹‰æœºä¼šå›¾è°±ã€‚
            
        è¾“å‡º:
            - List[Dict]: ç²¾ç‚¼ç»“æœåˆ—è¡¨ã€‚
            
        å®ç°æ€è·¯:
            1. æ‰¹é‡åˆ†æç²¾ç‚¼å†³ç­–
            2. æ‰¹é‡ç”Ÿæˆç²¾ç‚¼æŒ‡ä»¤
            3. å¹¶è¡Œå¤„ç†æƒ³æ³•ç²¾ç‚¼
        """
        print(f"ğŸ”„ å¼€å§‹æ‰¹é‡æƒ³æ³•ç²¾ç‚¼ï¼š{len(batch_ideas)}ä¸ªæƒ³æ³•")
        
        try:
            # æ­¥éª¤1ï¼šå‡†å¤‡æ‰¹é‡æ•°æ®
            idea_critique_pairs = list(zip(batch_ideas, novelty_critiques, feasibility_critiques))
            
            # æ­¥éª¤2ï¼šæ‰¹é‡åˆ†æç²¾ç‚¼å†³ç­–ï¼ˆä¸€æ¬¡LLMè°ƒç”¨ï¼‰
            print(f"    ğŸ¤” æ‰¹é‡åˆ†æç²¾ç‚¼å†³ç­–...")
            decisions = await self.refiner.make_refinement_decisions_batch(idea_critique_pairs)
            
            # æ­¥éª¤3ï¼šæ‰¹é‡ç”Ÿæˆç²¾ç‚¼æŒ‡ä»¤ï¼ˆå¹¶è¡Œå¤„ç†ï¼‰
            print(f"    ğŸ“ æ‰¹é‡ç”Ÿæˆç²¾ç‚¼æŒ‡ä»¤...")
            refinement_prompts = await self.refiner.make_refinement_prompts_batch(decisions, idea_critique_pairs)
            
            # æ­¥éª¤4ï¼šå¹¶è¡Œå¤„ç†æƒ³æ³•ç²¾ç‚¼
            print(f"    ğŸš€ å¯åŠ¨å¹¶è¡Œç²¾ç‚¼å¤„ç†...")
            import asyncio
            
            # åˆ›å»ºå¹¶å‘ä»»åŠ¡
            refinement_tasks = []
            for i, (idea, decision_info, refinement_prompt) in enumerate(zip(batch_ideas, decisions, refinement_prompts)):
                task = asyncio.create_task(
                    self._process_single_refinement(
                        idea, decision_info, refinement_prompt, 
                        novelty_critiques[i], feasibility_critiques[i], graph
                    )
                )
                refinement_tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            refinement_results = await asyncio.gather(*refinement_tasks, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            batch_results = []
            for i, result in enumerate(refinement_results):
                if isinstance(result, Exception):
                    print(f"    âŒ æƒ³æ³• {batch_ideas[i].id} ç²¾ç‚¼å¤±è´¥: {str(result)}")
                    batch_results.append({
                        "status": "failed",
                        "final_idea": batch_ideas[i],
                        "history": [],
                        "error": f"å¹¶è¡Œç²¾ç‚¼å¤±è´¥: {str(result)}"
                    })
                else:
                    batch_results.append(result)
                    print(f"    âœ… æƒ³æ³• {batch_ideas[i].id} ç²¾ç‚¼å®Œæˆ: {result.get('status', 'unknown')}")
            
            print(f"âœ… æ‰¹é‡ç²¾ç‚¼å®Œæˆï¼š{len(batch_results)}ä¸ªç»“æœ")
            return batch_results
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡ç²¾ç‚¼å¤„ç†å¤±è´¥: {e}")
            # è¿”å›æ‰€æœ‰æƒ³æ³•çš„å¤±è´¥ç»“æœ
            return [{
                "status": "failed",
                "final_idea": idea,
                "history": [],
                "error": f"æ‰¹é‡ç²¾ç‚¼å¤±è´¥: {str(e)}"
            } for idea in batch_ideas]

    async def _process_single_refinement(self, idea: CandidateIdea, 
                                       decision_info: Dict[str, Any],
                                       refinement_prompt: 'RefinementPrompt',
                                       novelty: 'NoveltyCritique', 
                                       feasibility: 'FeasibilityCritique',
                                       graph: SemanticOpportunityGraph) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæƒ³æ³•çš„ç²¾ç‚¼ï¼ˆåœ¨æ‰¹é‡å¤„ç†çš„å¹¶è¡Œä»»åŠ¡ä¸­ï¼‰ã€‚"""
        current_idea = idea
        history = []
        # æ·»åŠ ç±»å‹æ£€æŸ¥ï¼Œç¡®ä¿ decision_info æ˜¯å­—å…¸
        if isinstance(decision_info, dict):
            decision = decision_info.get('decision', 'revise')
        elif isinstance(decision_info, str):
            decision = decision_info  # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
        else:
            decision = 'revise'  # é»˜è®¤å€¼
        
        # åˆå§‹è¯„å®¡è®°å½•
        round_record = {
            "round": 1,
            "idea_version": current_idea.version,
            "novelty": novelty,
            "feasibility": feasibility,
            "refinement": refinement_prompt,
            "timestamp": datetime.now().isoformat()
        }
        history.append(round_record)
        
        try:
            if decision == "accept":
                return {
                    "status": "accepted",
                    "final_idea": current_idea,
                    "history": history,
                    "final_scores": {
                        "novelty": novelty.novelty_score,
                        "feasibility": feasibility.feasibility_score,
                        "combined": novelty.novelty_score + feasibility.feasibility_score
                    },
                    "acceptance_round": 1
                }
            elif decision == "discard":
                return {
                    "status": "discarded",
                    "final_idea": current_idea,
                    "history": history,
                    "discard_reason": refinement_prompt.rationale,
                    "discard_round": 1
                }
            elif decision == "revise":
                # å¯¹äºéœ€è¦ä¿®è®¢çš„æƒ³æ³•ï¼Œè¿›è¡Œä¸€è½®æ”¹è¿›
                try:
                    current_idea = await self.generator.refine_idea(current_idea, refinement_prompt, graph)
                    return {
                        "status": "revised",
                        "final_idea": current_idea,
                        "history": history,
                        "revision_round": 1
                    }
                except Exception as e:
                    return {
                        "status": "failed",
                        "final_idea": current_idea,
                        "history": history,
                        "error": f"ä¿®è®¢å¤±è´¥: {str(e)}"
                    }
            elif decision == "split":
                return {
                    "status": "split_required",
                    "final_idea": current_idea,
                    "history": history,
                    "split_instructions": refinement_prompt.instructions,
                    "rationale": refinement_prompt.rationale,
                    "split_round": 1
                }
            elif decision == "merge":
                return {
                    "status": "merge_required",
                    "final_idea": current_idea,
                    "history": history,
                    "merge_instructions": refinement_prompt.instructions,
                    "rationale": refinement_prompt.rationale,
                    "merge_round": 1
                }
            else:
                return {
                    "status": "unsupported_decision",
                    "final_idea": current_idea,
                    "history": history,
                    "decision": decision,
                    "rationale": refinement_prompt.rationale
                }
                
        except Exception as e:
            return {
                "status": "failed",
                "final_idea": current_idea,
                "history": history,
                "error": f"ç²¾ç‚¼å¤„ç†å¤±è´¥: {str(e)}"
            }

    async def _synthesize_final_results(self, graph: SemanticOpportunityGraph, 
                                       initial_candidates: List[CandidateIdea], 
                                       debate_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ•´åˆæœ€ç»ˆç»“æœæŠ¥è¡¨ã€‚
        
        è¾“å…¥:
            - graph: æ„å»ºçš„è¯­ä¹‰æœºä¼šå›¾è°±ã€‚
            - initial_candidates: åˆå§‹å€™é€‰æƒ³æ³•ã€‚
            - debate_results: è¾©è®ºå¾ªç¯ç»“æœã€‚
            
        è¾“å‡º:
            - Dict: å®Œæ•´çš„ç³»ç»Ÿäº§å‡ºæŠ¥è¡¨ã€‚
        """
        # ç»Ÿè®¡å„ç§ç»“æœçŠ¶æ€
        accepted_ideas = [r for r in debate_results if isinstance(r, dict) and r.get("status") == "accepted"]
        discarded_ideas = [r for r in debate_results if isinstance(r, dict) and r.get("status") == "discarded"]
        failed_ideas = [r for r in debate_results if isinstance(r, dict) and r.get("status") == "failed"]
        split_ideas = [r for r in debate_results if isinstance(r, dict) and r.get("status") == "split_required"]
        merge_ideas = [r for r in debate_results if isinstance(r, dict) and r.get("status") == "merge_required"]
        max_rounds_ideas = [r for r in debate_results if isinstance(r, dict) and r.get("status") == "max_rounds_reached"]
        other_ideas = [r for r in debate_results if isinstance(r, dict) and r.get("status") not in 
                      ["accepted", "discarded", "failed", "split_required", "merge_required", "max_rounds_reached"]]
        
        return {
            "opportunity_graph": {
                "node_count": graph.number_of_nodes(),
                "edge_count": graph.number_of_edges(),
                "gaps_count": len(graph.graph.get('gaps', [])),
                "provenance": graph.graph.get('provenance', {})
            },
            "initial_candidates": {
                "count": len(initial_candidates),
                "ideas": [{"id": idea.id, "title": idea.title} for idea in initial_candidates]
            },
            "final_ideas": {
                "accepted": {
                    "count": len(accepted_ideas),
                    "ideas": [r["final_idea"] for r in accepted_ideas],
                    "avg_scores": self._calculate_average_scores(accepted_ideas)
                },
                "discarded": {
                    "count": len(discarded_ideas),
                    "ideas": [r["final_idea"] for r in discarded_ideas],
                    "reasons": [r.get("discard_reason", "") for r in discarded_ideas]
                },
                "split_required": {
                    "count": len(split_ideas),
                    "ideas": [r["final_idea"] for r in split_ideas],
                    "instructions": [r.get("split_instructions", []) for r in split_ideas]
                },
                "merge_required": {
                    "count": len(merge_ideas),
                    "ideas": [r["final_idea"] for r in merge_ideas],
                    "instructions": [r.get("merge_instructions", []) for r in merge_ideas]
                },
                "max_rounds_reached": {
                    "count": len(max_rounds_ideas),
                    "ideas": [r["final_idea"] for r in max_rounds_ideas],
                    "final_scores": [r.get("final_scores", {}) for r in max_rounds_ideas]
                },
                "failed": {
                    "count": len(failed_ideas),
                    "errors": [r.get("error", "") for r in failed_ideas]
                },
                "other": {
                    "count": len(other_ideas),
                    "statuses": [r.get("status", "") for r in other_ideas]
                }
            },
            "iteration_history": [r.get("history", []) for r in debate_results if isinstance(r, dict)],
            "statistics": {
                "total_candidates": len(initial_candidates),
                "total_rounds": sum(len(r.get("history", [])) for r in debate_results if isinstance(r, dict)),
                "success_rate": len(accepted_ideas) / len(initial_candidates) if initial_candidates else 0,
                "avg_rounds_per_idea": sum(len(r.get("history", [])) for r in debate_results if isinstance(r, dict)) / len(debate_results) if debate_results else 0
            },
            "timestamp": datetime.now().isoformat()
        }

    def _calculate_average_scores(self, accepted_ideas: List[Dict[str, Any]]) -> Dict[str, float]:
        """è®¡ç®—å·²æ¥å—æƒ³æ³•çš„å¹³å‡åˆ†æ•°ã€‚"""
        if not accepted_ideas:
            return {"novelty": 0.0, "feasibility": 0.0, "combined": 0.0}
        
        scores = [idea.get("final_scores", {}) for idea in accepted_ideas]
        novelty_scores = [s.get("novelty", 0) for s in scores]
        feasibility_scores = [s.get("feasibility", 0) for s in scores]
        
        return {
            "novelty": sum(novelty_scores) / len(novelty_scores),
            "feasibility": sum(feasibility_scores) / len(feasibility_scores),
            "combined": sum(novelty_scores) / len(novelty_scores) + sum(feasibility_scores) / len(feasibility_scores)
        }
    
    async def handle_split_merge_operations(self, split_ideas: List[Dict[str, Any]], 
                                          merge_ideas: List[Dict[str, Any]], 
                                          graph: SemanticOpportunityGraph) -> Dict[str, Any]:
        """å¤„ç†éœ€è¦æ‹†åˆ†å’Œåˆå¹¶çš„æƒ³æ³•ï¼ˆå¯é€‰çš„åå¤„ç†æ­¥éª¤ï¼‰ã€‚
        
        è¾“å…¥:
            - split_ideas: éœ€è¦æ‹†åˆ†çš„æƒ³æ³•åˆ—è¡¨ã€‚
            - merge_ideas: éœ€è¦åˆå¹¶çš„æƒ³æ³•åˆ—è¡¨ã€‚
            - graph: è¯­ä¹‰æœºä¼šå›¾è°±ã€‚
            
        è¾“å‡º:
            - Dict: æ‹†åˆ†å’Œåˆå¹¶æ“ä½œçš„ç»“æœã€‚
            
        æ³¨æ„:
            è¿™æ˜¯ä¸€ä¸ªå¯é€‰çš„æ‰©å±•åŠŸèƒ½ï¼Œç”¨äºå¤„ç†å¤æ‚çš„æƒ³æ³•é‡æ„æ“ä½œã€‚
            å½“å‰ç‰ˆæœ¬åªæä¾›æ¡†æ¶ï¼Œå…·ä½“å®ç°å¯æ ¹æ®éœ€è¦æ‰©å±•ã€‚
        """
        print(f"ğŸ”„ åå¤„ç†é˜¶æ®µï¼šå¤„ç† {len(split_ideas)} ä¸ªæ‹†åˆ†è¯·æ±‚å’Œ {len(merge_ideas)} ä¸ªåˆå¹¶è¯·æ±‚")
        
        split_results = []
        merge_results = []
        
        # å¤„ç†æ‹†åˆ†æ“ä½œ
        for split_request in split_ideas:
            idea = split_request["final_idea"]
            instructions = split_request.get("split_instructions", [])
            
            # è¿™é‡Œå¯ä»¥å®ç°å…·ä½“çš„æ‹†åˆ†é€»è¾‘
            # ä¾‹å¦‚ï¼šåŸºäºåˆ›æ–°ç‚¹ã€å®éªŒè®¾è®¡ç­‰ç»´åº¦æ‹†åˆ†æƒ³æ³•
            split_results.append({
                "original_idea_id": idea.id,
                "split_method": "instruction_based",
                "sub_ideas_count": 2,  # é»˜è®¤æ‹†åˆ†ä¸º2ä¸ªå­æƒ³æ³•
                "status": "pending_implementation",
                "instructions": instructions
            })
        
        # å¤„ç†åˆå¹¶æ“ä½œ
        for merge_request in merge_ideas:
            idea = merge_request["final_idea"]
            instructions = merge_request.get("merge_instructions", [])
            
            # è¿™é‡Œå¯ä»¥å®ç°å…·ä½“çš„åˆå¹¶é€»è¾‘
            # ä¾‹å¦‚ï¼šå¯»æ‰¾ç›¸å…³æƒ³æ³•è¿›è¡Œåˆå¹¶
            merge_results.append({
                "original_idea_id": idea.id,
                "merge_method": "instruction_based",
                "merge_candidates": [],  # å¯åˆå¹¶çš„æƒ³æ³•å€™é€‰
                "status": "pending_implementation",
                "instructions": instructions
            })
        
        return {
            "split_operations": {
                "count": len(split_results),
                "results": split_results
            },
            "merge_operations": {
                "count": len(merge_results),
                "results": merge_results
            },
            "status": "framework_ready"  # è¡¨ç¤ºæ¡†æ¶å·²å°±ç»ªï¼Œç­‰å¾…å…·ä½“å®ç°
        }


# =========================
# å¯¹å¤–ä¾¿æ·å…¥å£ï¼ˆéª¨æ¶ï¼‰
# =========================


async def run_idea_generation(
    final_result: Dict[str, Any],
    enriched_outline: Dict[str, Any],
    llm_factory: LLMFactory,
    db: AcademicPaperDatabase,
    config: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """ä¾¿æ·å…¥å£ï¼šä¸€è¡Œè°ƒç”¨å®Œæˆæƒ³æ³•ç”Ÿæˆå…¨æµç¨‹ã€‚

    è¾“å…¥:
        - final_result: Survey Gen äº§å‡ºå­—å…¸ã€‚
        - enriched_outline: ä¸°å¯Œå¤§çº²å­—å…¸ã€‚
        - llm_factory: ç»Ÿä¸€LLMå·¥å‚ã€‚
        - db: å­¦æœ¯æ•°æ®åº“ã€‚
        - config: è¿è¡Œæ—¶é…ç½®ã€‚

    è¾“å‡º:
        - Dict: ç»“æ„åŒ–äº§ç‰©ï¼ˆè§åè°ƒå™¨è¯´æ˜ï¼‰ã€‚

    å®ç°æ­¥éª¤å»ºè®®:
        1) å®ä¾‹åŒ– `IdeaGenCoordinator`ã€‚
        2) è°ƒç”¨ `run_pipeline` å¹¶è¿”å›ç»“æœã€‚
        
    ä½¿ç”¨ç¤ºä¾‹:
        ```python
        # å‡è®¾å·²æœ‰ Survey Gen çš„äº§ç‰©
        result = await run_idea_generation(
            final_result=survey_result,
            enriched_outline=outline_data,
            llm_factory=llm_factory,
            db=database,
            config={"idea_concurrency": 6}
        )
        
        print(f"ç”Ÿæˆäº† {result['final_ideas']['accepted']['count']} ä¸ªè¢«æ¥å—çš„æƒ³æ³•")
        for idea in result['final_ideas']['accepted']['ideas']:
            print(f"- {idea.title}")
        ```
    """
    print("ğŸš€ å¯åŠ¨ Idea Generation å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ")
    
    # åˆ›å»ºåè°ƒå™¨
    coordinator = IdeaGenCoordinator(llm_factory, db, config)
    
    # æ‰§è¡Œå®Œæ•´æµæ°´çº¿
    start_time = datetime.now()
    try:
        result = await coordinator.run_pipeline(final_result, enriched_outline)
        
        duration = (datetime.now() - start_time).total_seconds()
        result["execution_time_seconds"] = duration
        
        print(f"âœ… Idea Generation å®Œæˆï¼Œè€—æ—¶ {duration:.2f} ç§’")
        print(f"ğŸ“Š ç»“æœç»Ÿè®¡ï¼š")
        print(f"   - æœºä¼šå›¾è°±: {result['opportunity_graph']['node_count']} èŠ‚ç‚¹, {result['opportunity_graph']['edge_count']} è¾¹")
        print(f"   - åˆå§‹å€™é€‰: {result['initial_candidates']['count']} ä¸ª")
        print(f"   - æœ€ç»ˆæ¥å—: {result['final_ideas']['accepted']['count']} ä¸ª")
        print(f"   - æˆåŠŸç‡: {result['statistics']['success_rate']:.1%}")
        
        return result
        
    except Exception as e:
        print(f"âŒ Idea Generation æ‰§è¡Œå¤±è´¥: {str(e)}")
        return {
            "status": "failed",
            "error": str(e),
            "execution_time_seconds": (datetime.now() - start_time).total_seconds(),
            "timestamp": datetime.now().isoformat()
        }