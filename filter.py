# -*- coding: utf-8 -*-

# # ä»»åŠ¡ç›®æ ‡
# æœ¬è„šæœ¬æ—¨åœ¨å¤„ç†ç”± MinerU API è§£æç”Ÿæˆçš„ JSON æ–‡ä»¶ã€‚
# ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
# 1. ç§»é™¤ JSON æ–‡ä»¶ä¸­ "REFERENCES" éƒ¨åˆ†åŠå…¶ä¹‹åçš„æ‰€æœ‰å†…å®¹
# 2. æ¸…ç†æ‰€æœ‰æ–‡æœ¬å†…å®¹ä¸­çš„å­¦æœ¯å¼•ç”¨ä¿¡æ¯ï¼ˆå¦‚å¼•ç”¨æ ‡è®°ã€ä½œè€…å¹´ä»½ç­‰ï¼‰
# ä»¥ç”Ÿæˆä¸€ä¸ªä¸åŒ…å«å‚è€ƒæ–‡çŒ®ã€é™„å½•å’Œå¼•ç”¨æ ‡è®°çš„å¹²å‡€ç‰ˆ JSON æ–‡ä»¶ã€‚

# # ä¸Šä¸‹æ–‡
# å‰ç½®è„šæœ¬ `api.py` ä¼šä» MinerU API ä¸‹è½½å¹¶è§£å‹æ¯ä¸ªPDFçš„è§£æç»“æœã€‚
# æ¯ä¸ªPDFçš„ç»“æœéƒ½å­˜å‚¨åœ¨ä¸€ä¸ªåä¸º `[è®ºæ–‡å].pdf_result` çš„ç›®å½•ä¸­ã€‚
# æ­¤è„šæœ¬å°†éå†è¿™äº›ç»“æœç›®å½•ï¼Œå¤„ç†å…¶ä¸­çš„ `*_content_list.json` æ–‡ä»¶ã€‚

# # è¾“å…¥
# - `results_base_dir`: å­˜å‚¨æ‰€æœ‰ `..._result` æ–‡ä»¶å¤¹çš„æ ¹ç›®å½•ã€‚
# - æ¯ä¸ª `..._result` æ–‡ä»¶å¤¹ä¸­éƒ½åŒ…å«ä¸€ä¸ª `*_content_list.json` æ–‡ä»¶ã€‚

# # æ‰§è¡Œæ­¥éª¤
# 1. è®¾ç½® `results_base_dir` å˜é‡ï¼ŒæŒ‡å‘åŒ…å«æ‰€æœ‰ç»“æœæ–‡ä»¶å¤¹çš„ç›®å½•ã€‚
# 2. éå† `results_base_dir` ä¸‹çš„æ‰€æœ‰ `..._result` ç›®å½•ã€‚
# 3. åœ¨æ¯ä¸ªç»“æœç›®å½•ä¸­ï¼ŒæŸ¥æ‰¾ `*_content_list.json` æ–‡ä»¶ã€‚
# 4. è¯»å–å¹¶è§£æè¯¥ JSON æ–‡ä»¶ï¼Œå®ƒæ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ªå­—å…¸çš„åˆ—è¡¨ã€‚
# 5. ç²¾ç¡®åˆ é™¤å‚è€ƒæ–‡çŒ®å†…å®¹ï¼š
#    a) æŸ¥æ‰¾ "REFERENCES" éƒ¨åˆ†çš„èµ·å§‹ä½ç½®ï¼ˆå…·æœ‰ text_level=1 çš„æ–‡æœ¬é¡¹ï¼‰
#    b) æŸ¥æ‰¾ REFERENCES ä¹‹åç¬¬ä¸€ä¸ªå…·æœ‰ text_level çš„æ–‡æœ¬é¡¹ï¼ˆé€šå¸¸æ˜¯ appendixï¼‰
#    c) åˆ é™¤è¿™ä¸¤ä¸ªä½ç½®ä¹‹é—´çš„æ‰€æœ‰å†…å®¹ï¼Œä¿ç•™ appendix ç­‰æœ‰ç”¨éƒ¨åˆ†
# 6. å¯¹å‰©ä½™å†…å®¹ä¸­æ‰€æœ‰ "type"=="text" çš„é¡¹è¿›è¡Œå¼•ç”¨æ¸…ç†ï¼Œå»é™¤å­¦æœ¯å¼•ç”¨æ ‡è®°ã€‚
# 7. ä»ç»“æœç›®å½•çš„åç§°ä¸­æå–è®ºæ–‡åã€‚
# 8. å°†å¤„ç†åçš„æ–°åˆ—è¡¨ä¿å­˜ä¸ºä¸€ä¸ªæ–°çš„ JSON æ–‡ä»¶ï¼Œå‘½åä¸º `[è®ºæ–‡å]_filter.json`ï¼Œå¹¶å­˜å‚¨åœ¨åŸç»“æœç›®å½•ä¸­ã€‚
# 9. å¦‚æœåœ¨æ–‡ä»¶ä¸­æœªæ‰¾åˆ°å‚è€ƒæ–‡çŒ®éƒ¨åˆ†ï¼Œåˆ™æ‰“å°ä¸€æ¡è­¦å‘Šä¿¡æ¯ã€‚

# # è¾“å‡º
# - åœ¨æ¯ä¸ª `..._result` æ–‡ä»¶å¤¹å†…ç”Ÿæˆä¸€ä¸ªæ–°çš„ `[è®ºæ–‡å]_filter.json` æ–‡ä»¶ï¼Œå…¶ä¸­ï¼š
#   1. ç²¾ç¡®åˆ é™¤äº†å‚è€ƒæ–‡çŒ®åˆ—è¡¨éƒ¨åˆ†ï¼Œä½†ä¿ç•™äº† appendix ç­‰æœ‰ç”¨å†…å®¹
#   2. æ‰€æœ‰æ–‡æœ¬å†…å®¹å·²æ¸…ç†å­¦æœ¯å¼•ç”¨æ ‡è®°

import os
import json
import glob
import re
import time  # æ·»åŠ æ—¶é—´æ¨¡å—ç”¨äºè®¡æ—¶

# --- é…ç½®åŒº ---
# è¯·å°†æ­¤è·¯å¾„è®¾ç½®ä¸ºåŒ…å«æ‰€æœ‰ `..._result` æ–‡ä»¶å¤¹çš„æ ¹ç›®å½•
# è¿™ä¸ªè·¯å¾„åº”è¯¥å’Œæ‚¨ api.py è„šæœ¬ä¸­çš„ output_dir ä¸€è‡´
results_base_dir = "D:/Desktop/ZJU/final_test/pdf/result/set1/"
# --- é…ç½®åŒºç»“æŸ ---


def clean_academic_citations(text: str) -> str:
    """
    æ¸…ç†å­¦æœ¯æ–‡æœ¬ä¸­çš„å¼•ç”¨ä¿¡æ¯
    
    è„šæœ¬ç›®æ ‡: å»é™¤æ–‡æœ¬ä¸­çš„å­¦æœ¯å¼•ç”¨ï¼Œæé«˜LLMå¯¹æ ¸å¿ƒå†…å®¹çš„å…³æ³¨åº¦
    ä¸Šä¸‹æ–‡: åœ¨æœç´¢ç›¸å…³ææ–™æ—¶ï¼Œå»é™¤æ— æ„ä¹‰çš„å¼•ç”¨ä¿¡æ¯
    è¾“å…¥: åŒ…å«å­¦æœ¯å¼•ç”¨çš„æ–‡æœ¬
    æ‰§è¡Œæ­¥éª¤:
    1. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å„ç§å¼•ç”¨æ ¼å¼
    2. å»é™¤åŒ¹é…åˆ°çš„å¼•ç”¨å†…å®¹
    3. æ¸…ç†å¤šä½™çš„ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·
    4. è¿”å›æ¸…ç†åçš„æ–‡æœ¬
    è¾“å‡º: å»é™¤å¼•ç”¨åçš„å¹²å‡€æ–‡æœ¬
    
    Args:
        text (str): éœ€è¦æ¸…ç†çš„æ–‡æœ¬
        
    Returns:
        str: å»é™¤å¼•ç”¨åçš„æ–‡æœ¬
        
    Examples:
        >>> text = "LLMs are powerful (Tang et al., 2024). They solve tasks (Liu et al., 2024; Li et al., 2024a)."
        >>> clean_academic_citations(text)
        "LLMs are powerful. They solve tasks."
    """
    if not text or not isinstance(text, str):
        return text
    
    # ä¿å­˜åŸå§‹æ–‡æœ¬é•¿åº¦ç”¨äºç»Ÿè®¡
    original_length = len(text)
    
    # 1. åŒ¹é…æ ‡å‡†å­¦æœ¯å¼•ç”¨æ ¼å¼ï¼šåŒ…æ‹¬åœ†æ‹¬å·å¼•ç”¨å’Œæ–¹æ‹¬å·æ•°å­—å¼•ç”¨
    # æ”¯æŒå¤šç§æ ¼å¼ï¼š
    # - (Tang et al., 2024)
    # - (Liu et al., 2024; Li et al., 2024a; Wang, 2023)
    # - (Smith & Jones, 2023)
    # - (Brown et al., 2022a,b)
    # - [14, 37, 40, 48, 63, 75, 76, 90, 96]
    # - [1-5, 10, 15-20]
    # - [14a, 37b]
    citation_patterns = [
        # åŒ¹é…æ–¹æ‹¬å·æ•°å­—å¼•ç”¨ï¼ˆæœ€å¸¸è§çš„æ ¼å¼ï¼‰
        # [14, 37, 40, 48, 63, 75, 76, 90, 96]
        # [1-5, 10, 15-20]
        # [14a, 37b]
        r'\[\s*\d+[a-z]?(?:\s*[-â€“]\s*\d+[a-z]?)?(?:\s*[,;]\s*\d+[a-z]?(?:\s*[-â€“]\s*\d+[a-z]?)?)*\s*\]',
        
        # åŒ¹é…åŒ…å« "et al." çš„åœ†æ‹¬å·å¼•ç”¨
        r'\([^)]*et al\.[^)]*\d{4}[a-z]?[^)]*\)',
        
        # åŒ¹é…æ ‡å‡†çš„ä½œè€…-å¹´ä»½æ ¼å¼ï¼ŒåŒ…æ‹¬å¤šä½œè€…ç”¨åˆ†å·åˆ†éš”çš„æƒ…å†µ
        r'\([^)]*[A-Z][a-z]+(?:\s+(?:&|and)\s+[A-Z][a-z]+)*\s*,\s*\d{4}[a-z]?[^)]*\)',
        
        # åŒ¹é…åŒ…å«å¤šä¸ªå¼•ç”¨çš„å¤æ‚æ ¼å¼ï¼ˆç”¨åˆ†å·åˆ†éš”ï¼‰
        r'\([^)]*\d{4}[a-z]?(?:\s*[;,]\s*[^)]*\d{4}[a-z]?)*[^)]*\)',
        
        # åŒ¹é…ç®€å•çš„å¹´ä»½å¼•ç”¨
        r'\(\s*\d{4}[a-z]?\s*\)',
        
        # åŒ¹é… ibid., op. cit. ç­‰å­¦æœ¯å¼•ç”¨
        r'\([^)]*(?:ibid\.|op\.\s*cit\.|loc\.\s*cit\.)[^)]*\)',
        
        # åŒ¹é…å•ä¸ªæ–¹æ‹¬å·æ•°å­—å¼•ç”¨ï¼ˆé˜²æ­¢é—æ¼ï¼‰
        r'\[\s*\d+[a-z]?\s*\]',
    ]
    
    # åº”ç”¨æ‰€æœ‰å¼•ç”¨æ¨¡å¼
    for pattern in citation_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    # 2. æ¸…ç†ç”±äºåˆ é™¤å¼•ç”¨å¯¼è‡´çš„å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·
    # åˆ é™¤å¤šä¸ªè¿ç»­ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text)
    
    # åˆ é™¤å¤šä¸ªè¿ç»­çš„é€—å·æˆ–åˆ†å·
    text = re.sub(r'[,;]\s*[,;]+', ',', text)
    
    # åˆ é™¤å¥å·å‰çš„å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+\.', '.', text)
    
    # åˆ é™¤å¥å­å¼€å¤´çš„é€—å·æˆ–åˆ†å·
    text = re.sub(r'^\s*[,;]\s*', '', text)
    text = re.sub(r'\.\s*[,;]\s*', '. ', text)
    
    # åˆ é™¤å¤šä¸ªè¿ç»­çš„å¥å·
    text = re.sub(r'\.{2,}', '.', text)
    
    # ğŸ†• å¤„ç†åˆ é™¤æ–¹æ‹¬å·å¼•ç”¨åçš„ç‰¹æ®Šæƒ…å†µ
    # åˆ é™¤å•è¯é—´çš„å¤šä½™ç©ºæ ¼ï¼ˆå¦‚ "field [14, 37] applications" å˜æˆ "field  applications"ï¼‰
    text = re.sub(r'([a-zA-Z])\s{2,}([a-zA-Z])', r'\1 \2', text)
    
    # åˆ é™¤å¥æœ«å¼•ç”¨åé—ç•™çš„ç©ºæ ¼å’Œæ ‡ç‚¹é—®é¢˜
    # å¦‚ "applications . " æ”¹ä¸º "applications."
    text = re.sub(r'\s+([.!?;,])', r'\1', text)
    
    # åˆ é™¤æ®µè½å¼€å¤´çš„ç©ºæ ¼å’Œå¥‡æ€ªå­—ç¬¦
    text = re.sub(r'^\s*[.;,]\s*', '', text)
    
    # å¤„ç†å¦‚ "within the medical field . They can" è¿™æ ·çš„æƒ…å†µ
    text = re.sub(r'([a-zA-Z])\s+\.\s+([A-Z])', r'\1. \2', text)
    
    # æ¸…ç†é¦–å°¾ç©ºæ ¼
    text = text.strip()
    
    # ç»Ÿè®¡æ¸…ç†æ•ˆæœ
    cleaned_length = len(text)
    if original_length > cleaned_length:
        reduction = original_length - cleaned_length
        reduction_percent = (reduction / original_length) * 100
        # è¿™é‡Œä½¿ç”¨ç®€å•çš„printï¼Œå®é™…ä½¿ç”¨æ—¶å¯ä»¥æ›¿æ¢ä¸ºlogger
        # print(f"ğŸ“ å¼•ç”¨æ¸…ç†: åˆ é™¤ {reduction} å­—ç¬¦ ({reduction_percent:.1f}%)")
    
    return text


def clean_text_content(content_list):
    """
    æ¸…ç†å†…å®¹åˆ—è¡¨ä¸­æ‰€æœ‰textç±»å‹é¡¹çš„å¼•ç”¨ä¿¡æ¯
    
    Args:
        content_list: JSONå†…å®¹åˆ—è¡¨
        
    Returns:
        tuple: (å¤„ç†åçš„å†…å®¹åˆ—è¡¨, å¤„ç†ç»Ÿè®¡ä¿¡æ¯)
    """
    if not content_list:
        return content_list, {"total_items": 0, "text_items": 0, "cleaned_items": 0}
    
    total_items = len(content_list)
    text_items = 0
    cleaned_items = 0
    
    print(f"  - å¼€å§‹æ¸…ç†æ–‡æœ¬å†…å®¹ä¸­çš„å¼•ç”¨ä¿¡æ¯...")
    
    for item in content_list:
        if isinstance(item, dict) and item.get("type") == "text":
            text_items += 1
            original_text = item.get("text", "")
            
            if original_text and isinstance(original_text, str):
                cleaned_text = clean_academic_citations(original_text)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…çš„æ¸…ç†å‘ç”Ÿ
                if len(cleaned_text) < len(original_text):
                    cleaned_items += 1
                    item["text"] = cleaned_text
    
    stats = {
        "total_items": total_items,
        "text_items": text_items,
        "cleaned_items": cleaned_items
    }
    
    print(f"  - å¼•ç”¨æ¸…ç†å®Œæˆ: å…±å¤„ç† {text_items} ä¸ªæ–‡æœ¬é¡¹ï¼Œå…¶ä¸­ {cleaned_items} ä¸ªåŒ…å«å¼•ç”¨å¹¶å·²æ¸…ç†")
    
    return content_list, stats

def find_references_start(content_list):
    """
    åœ¨å†…å®¹åˆ—è¡¨ä¸­æŸ¥æ‰¾ "REFERENCES" éƒ¨åˆ†çš„èµ·å§‹ç´¢å¼•ã€‚
    """
    for i, item in enumerate(content_list):
        # æ£€æŸ¥æ˜¯å¦ä¸ºç¬¦åˆæ¡ä»¶çš„æ–‡æœ¬å—
        if (item.get("type") == "text" and
            item.get("text", "").strip().upper() == "REFERENCES" and
            item.get("text_level") == 1):
            return i
    return -1  # æœªæ‰¾åˆ°


def find_next_text_level_item(content_list, start_index):
    """
    ä»æŒ‡å®šç´¢å¼•å¼€å§‹ï¼ŒæŸ¥æ‰¾ä¸‹ä¸€ä¸ªåŒ…å«text_levelçš„textç±»å‹é¡¹
    
    Args:
        content_list: JSONå†…å®¹åˆ—è¡¨
        start_index: å¼€å§‹æœç´¢çš„ç´¢å¼•ï¼ˆé€šå¸¸æ˜¯REFERENCESçš„ç´¢å¼•ï¼‰
        
    Returns:
        int: ä¸‹ä¸€ä¸ªæœ‰text_levelçš„texté¡¹çš„ç´¢å¼•ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å›-1
    """
    # ä»start_indexçš„ä¸‹ä¸€é¡¹å¼€å§‹æœç´¢
    for i in range(start_index + 1, len(content_list)):
        item = content_list[i]
        # æŸ¥æ‰¾type==textä¸”å­˜åœ¨text_levelçš„é¡¹
        if (item.get("type") == "text" and 
            "text_level" in item and 
            item.get("text_level") is not None):
            return i
    return -1  # æœªæ‰¾åˆ°


def remove_references_content(content_list):
    """
    ç²¾ç¡®åˆ é™¤å‚è€ƒæ–‡çŒ®å†…å®¹ï¼Œä¿ç•™appendixç­‰æœ‰ç”¨éƒ¨åˆ†
    
    æ‰§è¡Œæ­¥éª¤:
    1. æŸ¥æ‰¾REFERENCESéƒ¨åˆ†çš„èµ·å§‹ä½ç½®
    2. æŸ¥æ‰¾REFERENCESä¹‹åç¬¬ä¸€ä¸ªæœ‰text_levelçš„æ–‡æœ¬é¡¹
    3. åˆ é™¤è¿™ä¸¤ä¸ªä½ç½®ä¹‹é—´çš„æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬REFERENCESæœ¬èº«ï¼‰
    4. ä¿ç•™åç»­çš„appendixç­‰å†…å®¹
    
    Args:
        content_list: JSONå†…å®¹åˆ—è¡¨
        
    Returns:
        tuple: (å¤„ç†åçš„å†…å®¹åˆ—è¡¨, åˆ é™¤ç»Ÿè®¡ä¿¡æ¯)
    """
    if not content_list:
        return content_list, {"found_references": False, "removed_count": 0}
    
    # 1. æŸ¥æ‰¾REFERENCESèµ·å§‹ä½ç½®
    ref_start_index = find_references_start(content_list)
    
    if ref_start_index == -1:
        print("  - è­¦å‘Š: æœªåœ¨æœ¬æ–‡ä»¶ä¸­æ‰¾åˆ° 'REFERENCES' éƒ¨åˆ†ï¼Œå°†ä¸è¿›è¡Œå‚è€ƒæ–‡çŒ®åˆ é™¤ã€‚")
        return content_list, {"found_references": False, "removed_count": 0}
    
    print(f"  - åœ¨ç´¢å¼• {ref_start_index} å¤„æ‰¾åˆ° 'REFERENCES' éƒ¨åˆ†")
    
    # 2. æŸ¥æ‰¾REFERENCESä¹‹åä¸‹ä¸€ä¸ªæœ‰text_levelçš„é¡¹
    next_section_index = find_next_text_level_item(content_list, ref_start_index)
    
    if next_section_index == -1:
        # å¦‚æœæ²¡æ‰¾åˆ°ä¸‹ä¸€ä¸ªsectionï¼Œè¯´æ˜referencesåé¢å°±æ²¡æœ‰å…¶ä»–ç« èŠ‚äº†ï¼Œåˆ é™¤åˆ°æœ€å
        print(f"  - æœªæ‰¾åˆ°REFERENCESåçš„ä¸‹ä¸€ä¸ªç« èŠ‚ï¼Œåˆ é™¤ä»ç´¢å¼• {ref_start_index} åˆ°æ–‡ä»¶æœ«å°¾çš„æ‰€æœ‰å†…å®¹")
        filtered_list = content_list[:ref_start_index]
        removed_count = len(content_list) - ref_start_index
    else:
        # æ‰¾åˆ°äº†ä¸‹ä¸€ä¸ªsectionï¼Œåªåˆ é™¤referenceså’Œä¸‹ä¸€ä¸ªsectionä¹‹é—´çš„å†…å®¹
        next_section_text = content_list[next_section_index].get("text", "").strip()
        print(f"  - åœ¨ç´¢å¼• {next_section_index} å¤„æ‰¾åˆ°ä¸‹ä¸€ä¸ªç« èŠ‚: '{next_section_text}'")
        print(f"  - åˆ é™¤ç´¢å¼• {ref_start_index} åˆ° {next_section_index-1} ä¹‹é—´çš„å‚è€ƒæ–‡çŒ®å†…å®¹")
        
        # æ„å»ºæ–°çš„å†…å®¹åˆ—è¡¨ï¼šä¿ç•™referenceså‰çš„å†…å®¹ + ä¿ç•™ä¸‹ä¸€ä¸ªsectionåŠå…¶åçš„å†…å®¹
        filtered_list = content_list[:ref_start_index] + content_list[next_section_index:]
        removed_count = next_section_index - ref_start_index
    
    print(f"  - å‚è€ƒæ–‡çŒ®åˆ é™¤å®Œæˆ: åˆ é™¤äº† {removed_count} ä¸ªæ¡ç›®ï¼Œä¿ç•™ {len(filtered_list)} ä¸ªæ¡ç›®")
    
    return filtered_list, {"found_references": True, "removed_count": removed_count}

def process_result_directory(dir_path):
    """
    å¤„ç†å•ä¸ªç»“æœç›®å½•ï¼šæŸ¥æ‰¾ã€è¯»å–ã€è¿‡æ»¤å¹¶ä¿å­˜æ–°çš„JSONæ–‡ä»¶ã€‚
    """
    print(f"--- æ­£åœ¨å¤„ç†ç›®å½•: {os.path.basename(dir_path)} ---")

    # 1. æŸ¥æ‰¾éœ€è¦å¤„ç†çš„ JSON æ–‡ä»¶
    # æ–°é€»è¾‘ï¼šæŸ¥æ‰¾ä»¥ '_content_list.json' ç»“å°¾çš„JSONæ–‡ä»¶
    all_json_paths = glob.glob(os.path.join(dir_path, '*.json'))
    
    content_list_files = [
        path for path in all_json_paths 
        if os.path.basename(path).endswith('_content_list.json')
    ]

    if len(content_list_files) == 0:
        print("  âœ— é”™è¯¯: åœ¨æ­¤ç›®å½•ä¸­æœªæ‰¾åˆ°ä»¥ '_content_list.json' ç»“å°¾çš„æ–‡ä»¶ã€‚")
        return
    elif len(content_list_files) > 1:
        print(f"  âœ— é”™è¯¯: æ‰¾åˆ°å¤šä¸ªä»¥ '_content_list.json' ç»“å°¾çš„æ–‡ä»¶: {[os.path.basename(p) for p in content_list_files]}ã€‚")
        print("  æ¯ä¸ªç›®å½•åº”è¯¥åªæœ‰ä¸€ä¸ª _content_list.json æ–‡ä»¶ã€‚")
        return
    
    # æ‰¾åˆ°å”¯ä¸€çš„ _content_list.json æ–‡ä»¶
    input_json_path = content_list_files[0]
    print(f"  âœ“ æ‰¾åˆ°_content_list.jsonæ–‡ä»¶: {os.path.basename(input_json_path)}")

    # 2. è¯»å–å’Œè§£æJSON
    try:
        with open(input_json_path, 'r', encoding='utf-8') as f:
            content_data = json.load(f)
    except Exception as e:
        print(f"  âœ— è¯»å–æˆ–è§£æJSONæ–‡ä»¶å¤±è´¥: {e}")
        return

    # 3. ç²¾ç¡®åˆ é™¤å‚è€ƒæ–‡çŒ®å†…å®¹ï¼Œä¿ç•™appendixç­‰æœ‰ç”¨éƒ¨åˆ†
    filtered_data, ref_removal_stats = remove_references_content(content_data)

    # 4. æ¸…ç†æ–‡æœ¬å†…å®¹ä¸­çš„å¼•ç”¨ä¿¡æ¯
    filtered_data, citation_stats = clean_text_content(filtered_data)

    # 5. å‡†å¤‡è¾“å‡º
    # ä»ç›®å½•å '.../paper.pdf_result' ä¸­æå– 'paper.pdf'
    base_name_with_ext = os.path.basename(dir_path).replace('.pdf_result', '.pdf')
    # ç§»é™¤ '.pdf' åç¼€å¾—åˆ° 'paper'
    paper_name = os.path.splitext(base_name_with_ext)[0]
    output_filename = f"{paper_name}_filter.json"
    output_json_path = os.path.join(dir_path, output_filename)

    # 6. ä¿å­˜æ–°çš„JSONæ–‡ä»¶
    try:
        # ä¸ºæ”¯æŒ Windows é•¿è·¯å¾„ï¼Œå¯¹è·¯å¾„è¿›è¡Œå¤„ç†
        final_output_path = output_json_path
        if os.name == 'nt':
            abs_path = os.path.abspath(output_json_path)
            # æ·»åŠ é•¿è·¯å¾„å‰ç¼€ `\\?\`
            if not abs_path.startswith('\\\\?\\'):
                final_output_path = '\\\\?\\' + abs_path
                
        with open(final_output_path, 'w', encoding='utf-8') as f:
            json.dump(filtered_data, f, indent=4, ensure_ascii=False)
        print(f"  âœ“ æˆåŠŸä¿å­˜è¿‡æ»¤åçš„æ–‡ä»¶åˆ°: {os.path.basename(output_json_path)}")
    except Exception as e:
        print(f"  âœ— ä¿å­˜æ–°çš„JSONæ–‡ä»¶å¤±è´¥: {e}")


def main():
    """
    ä¸»å‡½æ•°ï¼Œæ‰§è¡Œæ•´ä¸ªå¤„ç†æµç¨‹ã€‚
    """
    # è®°å½•è„šæœ¬å¼€å§‹æ—¶é—´
    script_start_time = time.time()
    
    # æ£€æŸ¥æ ¹ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.isdir(results_base_dir):
        print(f"é”™è¯¯: æ ¹ç›®å½•ä¸å­˜åœ¨ -> {results_base_dir}")
        
        # å³ä½¿å‡ºé”™ä¹Ÿæ˜¾ç¤ºè¿è¡Œæ—¶é—´
        end_time = time.time()
        total_time = end_time - script_start_time
        print(f"\nè„šæœ¬è¿è¡Œè€—æ—¶{total_time:.2f}ç§’")
        return

    # è·å–results_base_dirä¸‹é¢æ‰€æœ‰å­ç›®å½•
    result_dirs = [d for d in os.listdir(results_base_dir) if os.path.isdir(os.path.join(results_base_dir, d))]

    if not result_dirs:
        print(f"åœ¨ '{results_base_dir}' ä¸­æœªæ‰¾åˆ°ä»»ä½•å­ç›®å½•ã€‚")
        
        # å³ä½¿æ²¡æœ‰æ‰¾åˆ°ç›®å½•ä¹Ÿæ˜¾ç¤ºè¿è¡Œæ—¶é—´
        end_time = time.time()
        total_time = end_time - script_start_time
        print(f"\nè„šæœ¬è¿è¡Œè€—æ—¶{total_time:.2f}ç§’")
        return

    print(f"åœ¨æ ¹ç›®å½•ä¸­æ‰¾åˆ° {len(result_dirs)} ä¸ªç»“æœç›®å½•ã€‚å¼€å§‹å¤„ç†...\n")
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    try:
        for dir_name in result_dirs:
            dir_path = os.path.join(results_base_dir, dir_name)
            process_result_directory(dir_path)
            print("-" * 50)

        # è®°å½•ç»“æŸæ—¶é—´å¹¶è®¡ç®—æ€»è€—æ—¶
        end_time = time.time()
        total_time = end_time - start_time
        
        # æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤ºï¼ˆæ—¶åˆ†ç§’ï¼‰
        hours = int(total_time // 3600)
        minutes = int((total_time % 3600) // 60)
        seconds = total_time % 60
        
        print(f"\n=== æ•°æ®è¿‡æ»¤å®Œæˆ ===")
        print(f"å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))}")
        print(f"ç»“æŸæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))}")
        
        if hours > 0:
            print(f"æ•°æ®è¿‡æ»¤è€—æ—¶{hours}å°æ—¶{minutes}åˆ†é’Ÿ{seconds:.2f}ç§’")
        elif minutes > 0:
            print(f"æ•°æ®è¿‡æ»¤è€—æ—¶{minutes}åˆ†é’Ÿ{seconds:.2f}ç§’")
        else:
            print(f"æ•°æ®è¿‡æ»¤è€—æ—¶{seconds:.2f}ç§’")

        print("\næ‰€æœ‰ç›®å½•å¤„ç†å®Œæ¯•ã€‚")
        
    except KeyboardInterrupt:
        # ç”¨æˆ·ä¸­æ–­å¤„ç†
        end_time = time.time()
        total_time = end_time - start_time
        
        hours = int(total_time // 3600)
        minutes = int((total_time % 3600) // 60)
        seconds = total_time % 60
        
        print(f"\n=== ç”¨æˆ·ä¸­æ–­å¤„ç† ===")
        print(f"å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))}")
        print(f"ä¸­æ–­æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))}")
        
        if hours > 0:
            print(f"å·²è¿è¡Œ{hours}å°æ—¶{minutes}åˆ†é’Ÿ{seconds:.2f}ç§’")
        elif minutes > 0:
            print(f"å·²è¿è¡Œ{minutes}åˆ†é’Ÿ{seconds:.2f}ç§’")
        else:
            print(f"å·²è¿è¡Œ{seconds:.2f}ç§’")
            
    except Exception as e:
        # å…¶ä»–å¼‚å¸¸å¤„ç†
        end_time = time.time()
        total_time = end_time - start_time
        
        hours = int(total_time // 3600)
        minutes = int((total_time % 3600) // 60)
        seconds = total_time % 60
        
        print(f"\n=== å¤„ç†å¼‚å¸¸ç»“æŸ ===")
        print(f"é”™è¯¯ä¿¡æ¯: {e}")
        print(f"å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))}")
        print(f"å¼‚å¸¸æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))}")
        
        if hours > 0:
            print(f"è¿è¡Œè€—æ—¶{hours}å°æ—¶{minutes}åˆ†é’Ÿ{seconds:.2f}ç§’")
        elif minutes > 0:
            print(f"è¿è¡Œè€—æ—¶{minutes}åˆ†é’Ÿ{seconds:.2f}ç§’")
        else:
            print(f"è¿è¡Œè€—æ—¶{seconds:.2f}ç§’")
            
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()